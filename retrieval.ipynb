{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched records:\n",
      "('https://arxiv.org/pdf/2402.13616v2.pdf',)\n",
      "('https://arxiv.org/pdf/2402.15151v1.pdf',)\n",
      "('https://arxiv.org/pdf/2402.03099v1.pdf',)\n",
      "('https://arxiv.org/pdf/2402.07939v3.pdf',)\n",
      "('https://arxiv.org/pdf/2402.13144v1.pdf',)\n",
      "('https://arxiv.org/pdf/2212.09748v2.pdf',)\n",
      "('https://arxiv.org/pdf/2205.05982v1.pdf',)\n",
      "('https://arxiv.org/pdf/2402.14652v1.pdf',)\n",
      "('https://arxiv.org/pdf/2402.04845v1.pdf',)\n",
      "('https://arxiv.org/pdf/2402.08268v1.pdf',)\n"
     ]
    }
   ],
   "source": [
    "with sqlite3.connect(\"papers.db\") as conn:\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"Select arxiv_link from papers\")\n",
    "    \n",
    "    # Fetch all results\n",
    "    all_rows = c.fetchall()\n",
    "    # print(all_rows)\n",
    "    \n",
    "    # Check if any rows were fetched\n",
    "    if all_rows:\n",
    "        print(\"Fetched records:\")\n",
    "        for row in all_rows:\n",
    "            print(row)\n",
    "    else:\n",
    "        print(\"No records found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from get_data import get_paper_list\n",
    "\n",
    "links = get_paper_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/pdf/2402.13616v2.pdf'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create system to save named papers\n",
    "# import requests\n",
    "\n",
    "# URL = \"https://td-cdn.pw/api.php?download=tikdown.org-42500282235.mp4\"\n",
    "# FILE_TO_SAVE_AS = \"myvideo.mp4\" # the name you want to save file as\n",
    "\n",
    "\n",
    "# resp = requests.get(URL) # making requests to server\n",
    "\n",
    "# with open(FILE_TO_SAVE_AS, \"wb\") as f: # opening a file handler to create new file \n",
    "#     f.write(resp.content) # writing content to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for link in links:\n",
    "    loader = PyPDFLoader(link)\n",
    "    docs.extend(loader.load_and_split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG starts here (docs come in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost of embedding? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017815"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Create encoder\n",
    "encoder = tiktoken.get_encoding('cl100k_base')\n",
    "tokens_per_docs = [len(encoder.encode(doc.page_content)) for doc in docs]\n",
    "\n",
    "\n",
    "# Estimated cost = sum of tokens / 1000\n",
    "cost_per_1000_tokens = 0.0001\n",
    "cost = (sum(tokens_per_docs) / 1000) * cost_per_1000_tokens\n",
    "cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import ParentDocumentRetriever\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embed and store the texts\n",
    "# Supplying a persist dicrectory will store the embeddings on disk\n",
    "persist_direcory = 'data/vectordb'\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=persist_direcory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist DB to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now can load and use the db\n",
    "vectordb = Chroma(embedding_function=embeddings, persist_directory=persist_direcory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='pages 358–373.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\\nWelbl, Aidan Clark, et al. 2022. Training compute-\\noptimal large language models. ArXiv preprint .\\nChristian Kohlschütter, Peter Fankhauser, and Wolfgang\\nNejdl. 2010. Boilerplate detection using shallow text\\nfeatures. In Proceedings of the Third International\\nConference on Web Search and Web Data Mining,\\nWSDM 2010, New York, NY, USA, February 4-6,\\n2010 , pages 441–450.\\nJunlong Li, Yiheng Xu, Lei Cui, and Furu Wei. 2021.\\nMarkuplm: Pre-training of text and markup language\\nfor visually-rich document understanding. ArXiv\\npreprint .\\nStephen Merity, Caiming Xiong, James Bradbury, and\\nRichard Socher. 2017. Pointer sentinel mixture mod-\\nels. In Proceedings of ICLR .', metadata={'page': 4, 'source': 'https://arxiv.org/pdf/2402.14652v1.pdf'}),\n",
       " Document(page_content='Pengxiang Jin, Shenglin Zhang, Minghua Ma, Haozhe Li, Yu Kang, Liqun Li, Yudong\\nLiu, Bo Qiao, Chaoyun Zhang, Pu Zhao, et al. Assess and summarize: Improve outage\\nunderstanding with large language models. arXiv preprint arXiv:2305.18084 , 2023.\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu.\\nGPTeval: Nlg evaluation using GPT-4 with better human alignment. arXiv preprint\\narXiv:2303.16634 , 2023.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language\\nmodels to follow instructions with human feedback. Advances in Neural Information\\nProcessing Systems , 35:27730–27744, 2022.\\nBo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang\\nDong, Jue Zhang, Lu Wang, et al. TaskWeaver: A code-first agent framework. arXiv\\npreprint arXiv:2311.17541 , 2023.\\nRudolf Ramler, Thomas Wetzlmaier, and Robert Hoschek. Gui scalability issues of windows\\ndesktop applications and how to find them. In Companion Proceedings for the ISSTA/ECOOP\\n2018 Workshops , pp. 63–67, 2018.\\nYunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-LLM: A trainable agent for\\nrole-playing. arXiv preprint arXiv:2310.10158 , 2023.\\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366 , 2023.\\nSignificant Gravitas. AutoGPT. URL https://github.com/Significant-Gravitas/\\nAutoGPT .\\nWilliam Stallings. The windows operating system. Operating Systems: Internals and Design\\nPrinciples , 2005.\\nYashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power\\nof intelligent LLM agents. arXiv preprint arXiv:2306.03314 , 2023.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\\nNaoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi.\\nGPT-4V (ision) for robotics: Multimodal task planning from human demonstration. arXiv\\npreprint arXiv:2311.12015 , 2023.\\nJunyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and\\nJitao Sang. Mobile-Agent: Autonomous multi-modal mobile device agent with visual\\nperception. arXiv preprint arXiv:2401.16158 , 2024.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan\\nChen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based\\nautonomous agents. arXiv preprint arXiv:2308.11432 , 2023.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V\\nLe, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language\\nmodels. Advances in Neural Information Processing Systems , 35:24824–24837, 2022.\\nChaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao\\nZhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can GPT-4V (ision) serve medical\\napplications? case studies on GPT-4V for multimodal medical diagnosis. arXiv preprint\\narXiv:2310.09909 , 2023a.\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\\nLi Jiang, Xiaoyun Zhang, and Chi Wang. AutoGen: Enabling next-gen LLM applications\\nvia multi-agent conversation framework. arXiv preprint arXiv:2308.08155 , 2023b.\\n18', metadata={'page': 17, 'source': 'https://arxiv.org/pdf/2402.07939v3.pdf'}),\n",
       " Document(page_content='Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,\\nJunzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model\\nbased agents: A survey. arXiv preprint arXiv:2309.07864 , 2023.\\nAn Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang,\\nYiwu Zhong, Julian McAuley, Jianfeng Gao, et al. GPT-4V in wonderland: Large multi-\\nmodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562 ,\\n2023.\\nZhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu.\\nAppAgent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771 ,\\n2023a.\\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\\nLijuan Wang. The dawn of lmms: Preliminary explorations with GPT-4V (ision). arXiv\\npreprint arXiv:2309.17421 , 9(1):1, 2023b.\\nXinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan,\\nWilliam Yang Wang, and Linda Ruth Petzold. GPT-4V (ision) as a generalist evaluator for\\nvision-language tasks. arXiv preprint arXiv:2311.01361 , 2023.\\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. GPT-4V (ision) is a generalist\\nweb agent, if grounded. arXiv preprint arXiv:2401.01614 , 2024.\\nA Requests in WindoesBench and Detailed Evaluations\\nIn Table 5, 6, and 7, we present the complete user requests included in WindowsBench, along\\nwith the detailed results achieved by UFO . These requests span across nine different popular\\nWindows applications, incorporating various commonly used functions. Requests with\\nfollow-up tasks are sequentially numbered for clarity. In the Safeguard column, “-” denotes\\nthat the request is not sensitive and does not require user confirmation. “ ✓” indicates that\\nthe request is sensitive, and UFO successfully activates the safeguard for user confirmation,\\nwhile “ ✗” signifies that the safeguard fails to trigger for a sensitive request. In the success\\ncolumn, “ ✓” denotes that UFO completes the request successfully, while “ ✗” indicates a\\nfailure to fulfill the request.\\n19', metadata={'page': 18, 'source': 'https://arxiv.org/pdf/2402.07939v3.pdf'}),\n",
       " Document(page_content='WORLD MODEL ON MILLION -LENGTH VIDEO\\nANDLANGUAGE WITHRINGATTENTION\\nHao Liu∗, Wilson Yan∗, Matei Zaharia, Pieter Abbeel\\nUC Berkeley\\nABSTRACT\\nCurrent language models fall short in understanding aspects of the world not\\neasily described in words, and struggle with complex, long-form tasks. Video\\nsequences offer valuable temporal information absent in language and static images,\\nmaking them attractive for joint modeling with language. Such models could\\ndevelop a understanding of both human textual knowledge and the physical world,\\nenabling broader AI capabilities for assisting humans. However, learning from\\nmillions of tokens of video and language sequences poses challenges due to memory\\nconstraints, computational complexity, and limited datasets. To address these\\nchallenges, we curate a large dataset of diverse videos and books, utilize the\\nRingAttention technique to scalably train on long sequences, and gradually increase\\ncontext size from 4K to 1M tokens. This paper makes the following contributions:\\n(a) Largest context size neural network: We train one of the largest context size\\ntransformers on long video and language sequences, setting new benchmarks in\\ndifficult retrieval tasks and long video understanding. (b) Solutions for overcoming\\nvision-language training challenges, including using masked sequence packing\\nfor mixing different sequence lengths, loss weighting to balance language and\\nvision, and model-generated QA dataset for long sequence chat. (c) A highly-\\noptimized implementation with RingAttention, masked sequence packing, and\\nother key features for training on millions-length multimodal sequences. (d) Fully\\nopen-sourced a family of 7B parameter models capable of processing long text\\ndocuments (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over\\n1M tokens. This work paves the way for training on massive datasets of long\\nvideo and language to develop understanding of both human knowledge and the\\nmultimodal world, and broader capabilities.\\n∗Equal contribution. Correspondence: hao.liu@cs.berkeley.edu ,wilson1.yan@berkeley.edu\\nCode and models of Large World Model (LWM) are available at largeworldmodel.github.io.arXiv:2402.08268v1  [cs.LG]  13 Feb 2024', metadata={'page': 0, 'source': 'https://arxiv.org/pdf/2402.08268v1.pdf'})]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(query= \"Large Language Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"Large Language Models\")\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "# Create a chain to answer questions\n",
    "\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm = ChatOpenAI(),\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=retriever,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These documents discuss training compute-optimal large language models, improving outage understanding with large language models, and using large language models for tasks such as following instructions with human feedback, character role-playing, and more. \n",
      "\n",
      "\n",
      "\n",
      "Sources:\n",
      "https://arxiv.org/pdf/2402.14652v1.pdf\n",
      " https://arxiv.org/pdf/2402.07939v3.pdf\n",
      " https://arxiv.org/pdf/2402.08268v1.pdf\n"
     ]
    }
   ],
   "source": [
    "## Cite sources\n",
    "\n",
    "def process_llm_resoonse(llm_response):\n",
    "    print(llm_response['answer'])\n",
    "    print('\\n\\nSources:')\n",
    "    # print(llm_response['sources'])\n",
    "    for source in llm_response['sources'].split(','):\n",
    "        print(source)\n",
    "\n",
    "\n",
    "query = \"What do these documents say about large language Models\"\n",
    "llm_response = qa_chain.invoke(query)\n",
    "process_llm_resoonse(llm_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://arxiv.org/pdf/2402.07939v3.pdf', ' https://arxiv.org/pdf/2402.08268v1.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(llm_response['sources'].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "# retriever = ParentDocumentRetriever(\n",
    "#     vectorstore=vectorstore,\n",
    "#     docstore=store,\n",
    "#     child_splitter=child_splitter,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SME_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
