{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sqlite3.connect(\"data/papers.db\") as conn:\n",
    "#     c = conn.cursor()\n",
    "#     c.execute(\"Select * from papers\")\n",
    "    \n",
    "#     # Fetch all results\n",
    "#     all_rows = c.fetchall()\n",
    "#     # print(all_rows)\n",
    "    \n",
    "#     # Check if any rows were fetched\n",
    "#     if all_rows:\n",
    "#         print(\"Fetched records:\")\n",
    "#         for row in all_rows:\n",
    "#             print(row)\n",
    "#     else:\n",
    "#         print(\"No records found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for paper in all_rows:\n",
    "#     print(\"\\n\" + paper[1])\n",
    "#     print(paper[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from get_data import get_paper_list\n",
    "\n",
    "# links = get_paper_list()\n",
    "links = [paper[2] for paper in all_rows]\n",
    "titles = [paper[1] for paper in all_rows]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://arxiv.org/pdf/2402.13616v2.pdf',\n",
       " 'YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0], titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for link, title in zip(links,titles):\n",
    "    loader = PyPDFLoader(link)\n",
    "    doc = loader.load_and_split()\n",
    "    for i in range(len(doc)):\n",
    "        doc[i].metadata['title'] = title\n",
    "    docs.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://arxiv.org/pdf/2402.15151v1.pdf',\n",
       " 'page': 0,\n",
       " 'title': 'Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing'}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[26].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG starts here (docs come in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost of embedding? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017815"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Create encoder\n",
    "encoder = tiktoken.get_encoding('cl100k_base')\n",
    "tokens_per_docs = [len(encoder.encode(doc.page_content)) for doc in docs]\n",
    "\n",
    "\n",
    "# Estimated cost = sum of tokens / 1000\n",
    "cost_per_1000_tokens = 0.0001\n",
    "cost = (sum(tokens_per_docs) / 1000) * cost_per_1000_tokens\n",
    "cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try ArxivRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import get_paper_info\n",
    "\n",
    "meta = get_paper_info()\n",
    "# meta[0]\n",
    "\n",
    "for paper in meta:\n",
    "    print(\"\\n\" + paper['title'])\n",
    "    print(paper['authors'])\n",
    "    print(paper['published'])\n",
    "    print(paper['arxiv_link'])\n",
    "\n",
    "    print(\"\\n\" + 'Summary:')\n",
    "    print(paper['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import ParentDocumentRetriever\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embed and store the texts\n",
    "# Supplying a persist dicrectory will store the embeddings on disk\n",
    "persist_direcory = 'data/vectordb'\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=persist_direcory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist DB to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now can load and use the db\n",
    "vectordb = Chroma(embedding_function=embeddings, persist_directory=persist_direcory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to pull specific paper from vector database?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': 'https://arxiv.org/pdf/2402.15151v1.pdf', 'title': 'Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing'}\n",
      "{'page': 7, 'source': 'https://arxiv.org/pdf/2402.15151v1.pdf', 'title': 'Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing'}\n",
      "{'page': 2, 'source': 'https://arxiv.org/pdf/2402.15151v1.pdf', 'title': 'Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing'}\n",
      "{'page': 1, 'source': 'https://arxiv.org/pdf/2402.15151v1.pdf', 'title': 'Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing'}\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query= \"Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing\")\n",
    "\n",
    "for result in results:\n",
    "    print(result.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"Large Language Models\")\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='pages 358–373.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\\nWelbl, Aidan Clark, et al. 2022. Training compute-\\noptimal large language models. ArXiv preprint .\\nChristian Kohlschütter, Peter Fankhauser, and Wolfgang\\nNejdl. 2010. Boilerplate detection using shallow text\\nfeatures. In Proceedings of the Third International\\nConference on Web Search and Web Data Mining,\\nWSDM 2010, New York, NY, USA, February 4-6,\\n2010 , pages 441–450.\\nJunlong Li, Yiheng Xu, Lei Cui, and Furu Wei. 2021.\\nMarkuplm: Pre-training of text and markup language\\nfor visually-rich document understanding. ArXiv\\npreprint .\\nStephen Merity, Caiming Xiong, James Bradbury, and\\nRichard Socher. 2017. Pointer sentinel mixture mod-\\nels. In Proceedings of ICLR .', metadata={'page': 4, 'source': 'https://arxiv.org/pdf/2402.14652v1.pdf'}),\n",
       " Document(page_content='pages 358–373.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\\nWelbl, Aidan Clark, et al. 2022. Training compute-\\noptimal large language models. ArXiv preprint .\\nChristian Kohlschütter, Peter Fankhauser, and Wolfgang\\nNejdl. 2010. Boilerplate detection using shallow text\\nfeatures. In Proceedings of the Third International\\nConference on Web Search and Web Data Mining,\\nWSDM 2010, New York, NY, USA, February 4-6,\\n2010 , pages 441–450.\\nJunlong Li, Yiheng Xu, Lei Cui, and Furu Wei. 2021.\\nMarkuplm: Pre-training of text and markup language\\nfor visually-rich document understanding. ArXiv\\npreprint .\\nStephen Merity, Caiming Xiong, James Bradbury, and\\nRichard Socher. 2017. Pointer sentinel mixture mod-\\nels. In Proceedings of ICLR .', metadata={'page': 4, 'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'title': 'Cleaner Pretraining Corpus Curation with Neural Web Scraping'}),\n",
       " Document(page_content='Pengxiang Jin, Shenglin Zhang, Minghua Ma, Haozhe Li, Yu Kang, Liqun Li, Yudong\\nLiu, Bo Qiao, Chaoyun Zhang, Pu Zhao, et al. Assess and summarize: Improve outage\\nunderstanding with large language models. arXiv preprint arXiv:2305.18084 , 2023.\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu.\\nGPTeval: Nlg evaluation using GPT-4 with better human alignment. arXiv preprint\\narXiv:2303.16634 , 2023.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language\\nmodels to follow instructions with human feedback. Advances in Neural Information\\nProcessing Systems , 35:27730–27744, 2022.\\nBo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang\\nDong, Jue Zhang, Lu Wang, et al. TaskWeaver: A code-first agent framework. arXiv\\npreprint arXiv:2311.17541 , 2023.\\nRudolf Ramler, Thomas Wetzlmaier, and Robert Hoschek. Gui scalability issues of windows\\ndesktop applications and how to find them. In Companion Proceedings for the ISSTA/ECOOP\\n2018 Workshops , pp. 63–67, 2018.\\nYunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-LLM: A trainable agent for\\nrole-playing. arXiv preprint arXiv:2310.10158 , 2023.\\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366 , 2023.\\nSignificant Gravitas. AutoGPT. URL https://github.com/Significant-Gravitas/\\nAutoGPT .\\nWilliam Stallings. The windows operating system. Operating Systems: Internals and Design\\nPrinciples , 2005.\\nYashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power\\nof intelligent LLM agents. arXiv preprint arXiv:2306.03314 , 2023.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\\nNaoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi.\\nGPT-4V (ision) for robotics: Multimodal task planning from human demonstration. arXiv\\npreprint arXiv:2311.12015 , 2023.\\nJunyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and\\nJitao Sang. Mobile-Agent: Autonomous multi-modal mobile device agent with visual\\nperception. arXiv preprint arXiv:2401.16158 , 2024.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan\\nChen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based\\nautonomous agents. arXiv preprint arXiv:2308.11432 , 2023.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V\\nLe, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language\\nmodels. Advances in Neural Information Processing Systems , 35:24824–24837, 2022.\\nChaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao\\nZhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can GPT-4V (ision) serve medical\\napplications? case studies on GPT-4V for multimodal medical diagnosis. arXiv preprint\\narXiv:2310.09909 , 2023a.\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\\nLi Jiang, Xiaoyun Zhang, and Chi Wang. AutoGen: Enabling next-gen LLM applications\\nvia multi-agent conversation framework. arXiv preprint arXiv:2308.08155 , 2023b.\\n18', metadata={'page': 17, 'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'title': 'UFO: A UI-Focused Agent for Windows OS Interaction'}),\n",
       " Document(page_content='Pengxiang Jin, Shenglin Zhang, Minghua Ma, Haozhe Li, Yu Kang, Liqun Li, Yudong\\nLiu, Bo Qiao, Chaoyun Zhang, Pu Zhao, et al. Assess and summarize: Improve outage\\nunderstanding with large language models. arXiv preprint arXiv:2305.18084 , 2023.\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu.\\nGPTeval: Nlg evaluation using GPT-4 with better human alignment. arXiv preprint\\narXiv:2303.16634 , 2023.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language\\nmodels to follow instructions with human feedback. Advances in Neural Information\\nProcessing Systems , 35:27730–27744, 2022.\\nBo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang\\nDong, Jue Zhang, Lu Wang, et al. TaskWeaver: A code-first agent framework. arXiv\\npreprint arXiv:2311.17541 , 2023.\\nRudolf Ramler, Thomas Wetzlmaier, and Robert Hoschek. Gui scalability issues of windows\\ndesktop applications and how to find them. In Companion Proceedings for the ISSTA/ECOOP\\n2018 Workshops , pp. 63–67, 2018.\\nYunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-LLM: A trainable agent for\\nrole-playing. arXiv preprint arXiv:2310.10158 , 2023.\\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366 , 2023.\\nSignificant Gravitas. AutoGPT. URL https://github.com/Significant-Gravitas/\\nAutoGPT .\\nWilliam Stallings. The windows operating system. Operating Systems: Internals and Design\\nPrinciples , 2005.\\nYashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power\\nof intelligent LLM agents. arXiv preprint arXiv:2306.03314 , 2023.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\\nNaoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi.\\nGPT-4V (ision) for robotics: Multimodal task planning from human demonstration. arXiv\\npreprint arXiv:2311.12015 , 2023.\\nJunyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and\\nJitao Sang. Mobile-Agent: Autonomous multi-modal mobile device agent with visual\\nperception. arXiv preprint arXiv:2401.16158 , 2024.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan\\nChen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based\\nautonomous agents. arXiv preprint arXiv:2308.11432 , 2023.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V\\nLe, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language\\nmodels. Advances in Neural Information Processing Systems , 35:24824–24837, 2022.\\nChaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao\\nZhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can GPT-4V (ision) serve medical\\napplications? case studies on GPT-4V for multimodal medical diagnosis. arXiv preprint\\narXiv:2310.09909 , 2023a.\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\\nLi Jiang, Xiaoyun Zhang, and Chi Wang. AutoGen: Enabling next-gen LLM applications\\nvia multi-agent conversation framework. arXiv preprint arXiv:2308.08155 , 2023b.\\n18', metadata={'page': 17, 'source': 'https://arxiv.org/pdf/2402.07939v3.pdf'}),\n",
       " Document(page_content='Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,\\nJunzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model\\nbased agents: A survey. arXiv preprint arXiv:2309.07864 , 2023.\\nAn Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang,\\nYiwu Zhong, Julian McAuley, Jianfeng Gao, et al. GPT-4V in wonderland: Large multi-\\nmodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562 ,\\n2023.\\nZhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu.\\nAppAgent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771 ,\\n2023a.\\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\\nLijuan Wang. The dawn of lmms: Preliminary explorations with GPT-4V (ision). arXiv\\npreprint arXiv:2309.17421 , 9(1):1, 2023b.\\nXinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan,\\nWilliam Yang Wang, and Linda Ruth Petzold. GPT-4V (ision) as a generalist evaluator for\\nvision-language tasks. arXiv preprint arXiv:2311.01361 , 2023.\\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. GPT-4V (ision) is a generalist\\nweb agent, if grounded. arXiv preprint arXiv:2401.01614 , 2024.\\nA Requests in WindoesBench and Detailed Evaluations\\nIn Table 5, 6, and 7, we present the complete user requests included in WindowsBench, along\\nwith the detailed results achieved by UFO . These requests span across nine different popular\\nWindows applications, incorporating various commonly used functions. Requests with\\nfollow-up tasks are sequentially numbered for clarity. In the Safeguard column, “-” denotes\\nthat the request is not sensitive and does not require user confirmation. “ ✓” indicates that\\nthe request is sensitive, and UFO successfully activates the safeguard for user confirmation,\\nwhile “ ✗” signifies that the safeguard fails to trigger for a sensitive request. In the success\\ncolumn, “ ✓” denotes that UFO completes the request successfully, while “ ✗” indicates a\\nfailure to fulfill the request.\\n19', metadata={'page': 18, 'source': 'https://arxiv.org/pdf/2402.07939v3.pdf'}),\n",
       " Document(page_content='Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,\\nJunzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model\\nbased agents: A survey. arXiv preprint arXiv:2309.07864 , 2023.\\nAn Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang,\\nYiwu Zhong, Julian McAuley, Jianfeng Gao, et al. GPT-4V in wonderland: Large multi-\\nmodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562 ,\\n2023.\\nZhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu.\\nAppAgent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771 ,\\n2023a.\\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\\nLijuan Wang. The dawn of lmms: Preliminary explorations with GPT-4V (ision). arXiv\\npreprint arXiv:2309.17421 , 9(1):1, 2023b.\\nXinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan,\\nWilliam Yang Wang, and Linda Ruth Petzold. GPT-4V (ision) as a generalist evaluator for\\nvision-language tasks. arXiv preprint arXiv:2311.01361 , 2023.\\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. GPT-4V (ision) is a generalist\\nweb agent, if grounded. arXiv preprint arXiv:2401.01614 , 2024.\\nA Requests in WindoesBench and Detailed Evaluations\\nIn Table 5, 6, and 7, we present the complete user requests included in WindowsBench, along\\nwith the detailed results achieved by UFO . These requests span across nine different popular\\nWindows applications, incorporating various commonly used functions. Requests with\\nfollow-up tasks are sequentially numbered for clarity. In the Safeguard column, “-” denotes\\nthat the request is not sensitive and does not require user confirmation. “ ✓” indicates that\\nthe request is sensitive, and UFO successfully activates the safeguard for user confirmation,\\nwhile “ ✗” signifies that the safeguard fails to trigger for a sensitive request. In the success\\ncolumn, “ ✓” denotes that UFO completes the request successfully, while “ ✗” indicates a\\nfailure to fulfill the request.\\n19', metadata={'page': 18, 'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'title': 'UFO: A UI-Focused Agent for Windows OS Interaction'}),\n",
       " Document(page_content='WORLD MODEL ON MILLION -LENGTH VIDEO\\nANDLANGUAGE WITHRINGATTENTION\\nHao Liu∗, Wilson Yan∗, Matei Zaharia, Pieter Abbeel\\nUC Berkeley\\nABSTRACT\\nCurrent language models fall short in understanding aspects of the world not\\neasily described in words, and struggle with complex, long-form tasks. Video\\nsequences offer valuable temporal information absent in language and static images,\\nmaking them attractive for joint modeling with language. Such models could\\ndevelop a understanding of both human textual knowledge and the physical world,\\nenabling broader AI capabilities for assisting humans. However, learning from\\nmillions of tokens of video and language sequences poses challenges due to memory\\nconstraints, computational complexity, and limited datasets. To address these\\nchallenges, we curate a large dataset of diverse videos and books, utilize the\\nRingAttention technique to scalably train on long sequences, and gradually increase\\ncontext size from 4K to 1M tokens. This paper makes the following contributions:\\n(a) Largest context size neural network: We train one of the largest context size\\ntransformers on long video and language sequences, setting new benchmarks in\\ndifficult retrieval tasks and long video understanding. (b) Solutions for overcoming\\nvision-language training challenges, including using masked sequence packing\\nfor mixing different sequence lengths, loss weighting to balance language and\\nvision, and model-generated QA dataset for long sequence chat. (c) A highly-\\noptimized implementation with RingAttention, masked sequence packing, and\\nother key features for training on millions-length multimodal sequences. (d) Fully\\nopen-sourced a family of 7B parameter models capable of processing long text\\ndocuments (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over\\n1M tokens. This work paves the way for training on massive datasets of long\\nvideo and language to develop understanding of both human knowledge and the\\nmultimodal world, and broader capabilities.\\n∗Equal contribution. Correspondence: hao.liu@cs.berkeley.edu ,wilson1.yan@berkeley.edu\\nCode and models of Large World Model (LWM) are available at largeworldmodel.github.io.arXiv:2402.08268v1  [cs.LG]  13 Feb 2024', metadata={'page': 0, 'source': 'https://arxiv.org/pdf/2402.08268v1.pdf', 'title': 'World Model on Million-Length Video And Language With RingAttention'}),\n",
       " Document(page_content='teng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan\\nXiong, Jay Mahadeokar, Ozlem Kalinli, et al. 2023.\\nPrompting large language models with speech recog-\\nnition abilities. arXiv preprint arXiv:2307.11795 .\\nAlex Graves and Alex Graves. 2012. Connectionist tem-\\nporal classification. Supervised sequence labelling\\nwith recurrent neural networks , pages 61–93.\\nAlexandros Haliassos, Pingchuan Ma, Rodrigo Mira,\\nStavros Petridis, and Maja Pantic. 2022. Jointly learn-\\ning visual and auditory speech representations from\\nraw data. In The Eleventh International Conference\\non Learning Representations .\\nJiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao,\\nPeng Xu, Han Xiao, Kaipeng Zhang, Chris Liu,\\nSong Wen, Ziyu Guo, et al. 2023. Imagebind-llm:\\nMulti-modality instruction tuning. arXiv preprint\\narXiv:2309.03905 .\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\\nSun. 2016. Deep residual learning for image recog-\\nnition. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 770–\\n778.\\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\\nshort-term memory. Neural computation , 9(8):1735–\\n1780.', metadata={'page': 8, 'source': 'https://arxiv.org/pdf/2402.15151v1.pdf', 'title': 'Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing'}),\n",
       " Document(page_content='videos, leading to more advanced AI systems with a multimodal understanding, capable of assisting\\nhumans in a broader range of tasks.\\nTo learn from video and language sequences, we need to train a model that is capable of processing\\nmore than millions of tokens per sequence and train it on a very large dataset. However, modeling\\nmillions of tokens is extremely difficult due to high memory cost, computational complexity, and lack\\nof suitable datasets. Luckily, we have RingAttention [ LZA24 ], a technique for scaling up context size\\narbitrarily without approximations or overheads, allowing for scalable training on long sequences.\\nWe curated a large dataset of videos and languages from public book and video datasets, consisting\\nof videos of diverse activities and long-form books. Considering the high compute cost, we grow\\ncontext size from a smaller 4K context to a larger 1M context size gradually to reduce this cost, and\\nthis approach performs well in extending context effectively. Furthermore, we identify challenges\\nassociated with training on video and language: we discovered that training on a mixture of video,\\nimage, and text is crucial for optimal performance, due to images represent higher visual quality,\\nvideos offer sequential information, and text retains language understanding. To achieve this, we\\nimplemented an efficient masked sequence packing to effectively train with different sequence lengths,\\nrather than standard sequence packing mechanism. Moreover, determining the right balance between\\nimage, video, and text training is crucial for cross modality understanding, and we suggest a ratio\\nthat proved effective. Furthermore, to address the lack of long-form chat datasets, we developed a\\nmodel-generated question-answering (QA) approach by using a short-context model to generate a\\nQA dataset from books. We found this to be crucial for chat abilities over long sequences.\\nThe specific contributions of this paper are as follows: (a) we train one of the largest context size\\ntransformers to date on video and text sequences and achieved by far the best results ever reported\\nin terms of long video understanding (see e.g., Figure 1) and long context fact retrieval (see e.g.,\\nFigure 2). (b) We discover a branch of challenges associated with training on video and text sequences,\\nand propose solutions for them: loss weighting to balance language and vision, masked sequence\\npacking to effectively train with different sequence lengths, and model-generated QA dataset for long\\nsequence chat. (c) A highly-optimized, open-source implementation with RingAttention, masked\\nsequence packing and other key features for millions-length multimodal training. (d) Fully open-\\nsourced a family of 7B parameter models capable of processing long text documents ( LWM-Text ,\\nLWM-Text-Chat ) and videos ( LWM ,LWM-Chat ) of 1M tokens. Our work paves the way for\\ntraining on massive datasets of long video and language, and is useful for future development of AI\\nsystems with an understanding of both human knowledge and the multimodal world, and broader\\ncapabilities.\\n2 Overview\\nWe train a large autoregressive transformer model with a very large context window of up to one\\nmillion tokens, building upon Llama2 7B [ TMS+23]. To achieve this goal, we leverage several\\nstrategies: extending the context to 1M using books (Section 3), followed by joint training on long\\nmultimodal sequences, including text-image, text-video data, and books (Section 4).\\nOur training stages and datasets are shown in Figure 3 and the model architecture is shown in Figure 4.\\n3 Stage I: Learning Long-Context Language Models\\nThis stage aims at first developing LWM-Text andLWM-Text-Chat , a set of long-context language\\nmodels learned by training on progressively increasing sequence length data with RingAttention and\\nmodifying positional encoding parameters to account for longer sequence lengths (see Section 3.1).', metadata={'page': 3, 'source': 'https://arxiv.org/pdf/2402.08268v1.pdf', 'title': 'World Model on Million-Length Video And Language With RingAttention'}),\n",
       " Document(page_content='Figure 3 This figure illustrates the multimodal training of a Large World Model. Stage 1, LLM\\nContext Extension, focuses on expanding context size using the Books3 dataset, with context size\\ngrowing from 32K to 1M. Stage 2, Vision-Language Training, focuses on training on visual and\\nvideo contents of varying lengths. The pie chart details the allocation of 495B tokens across images,\\nshort and long videos, and 33B tokens of text data. The lower panel shows interactive capabilities in\\nunderstanding and responding to queries about complex multimodal world.\\nattention weights. In order to address these computational constraints, we use the RingAtten-\\ntion [ LZA24 ,LA23 ] implementation that leverages block-wise computation with sequence paral-\\nlelism to theoretically extend to an infinite context, bounded only by the number of devices available.\\nWe further fuse RingAttention with FlashAttention [ DFE+22,RS21 ] using Pallas [ BFH+18] to\\noptimize performance compared with using XLA compiler. In general, given a large enough tokens\\nper device, the communication cost during RingAttention fully overlaps with computation, and does\\nnot add any extra overhead.\\nProgressive Training on Increasing Context Length. Although our implementation allows us to\\ntrain on long documents of millions of tokens, it still remains costly since the quadratic computational\\ncomplexity of attention remains, where gradient step time scales roughly linearly with context size\\n5', metadata={'page': 4, 'source': 'https://arxiv.org/pdf/2402.08268v1.pdf', 'title': 'World Model on Million-Length Video And Language With RingAttention'})]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "# Create a chain to answer questions\n",
    "\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm = ChatOpenAI(),\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=retriever,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RetrievalQA' object has no attribute 'llm_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[233], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mqa_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241m.\u001b[39mprompt_template\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RetrievalQA' object has no attribute 'llm_chain'"
     ]
    }
   ],
   "source": [
    "qa_chain.llm_chain.prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer.\n",
      "\n",
      "\n",
      "\n",
      "Sources:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Cite sources\n",
    "\n",
    "def process_llm_resoonse(llm_response):\n",
    "    print(llm_response['answer'])\n",
    "    print('\\n\\nSources:')\n",
    "    # print(llm_response['sources'])\n",
    "    for source in llm_response['sources'].split(','):\n",
    "        print(source)\n",
    "\n",
    "\n",
    "query = \"What are the document titles?\"\n",
    "llm_response = qa_chain.invoke(query)\n",
    "process_llm_resoonse(llm_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What do these documents say about large language Models. Include sources and titles',\n",
       " 'answer': 'The documents discuss various aspects of large language models, including their training, evaluation, applications, and potential. Titles include \"Training compute-optimal large language models,\" \"Prompting large language models with speech recognition abilities,\" \"Assess and summarize: Improve outage understanding with large language models,\" \"The rise and potential of large language model-based agents: A survey,\" and more.\\n',\n",
       " 'sources': 'https://arxiv.org/pdf/2402.14652v1.pdf, https://arxiv.org/pdf/2402.07939v3.pdf, https://arxiv.org/pdf/2402.03099v1.pdf'}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2024-02-13',\n",
       " 'Title': 'World Model on Million-Length Video And Language With RingAttention',\n",
       " 'Authors': 'Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel',\n",
       " 'Summary': 'Current language models fall short in understanding aspects of the world not\\neasily described in words, and struggle with complex, long-form tasks. Video\\nsequences offer valuable temporal information absent in language and static\\nimages, making them attractive for joint modeling with language. Such models\\ncould develop a understanding of both human textual knowledge and the physical\\nworld, enabling broader AI capabilities for assisting humans. However, learning\\nfrom millions of tokens of video and language sequences poses challenges due to\\nmemory constraints, computational complexity, and limited datasets. To address\\nthese challenges, we curate a large dataset of diverse videos and books,\\nutilize the RingAttention technique to scalably train on long sequences, and\\ngradually increase context size from 4K to 1M tokens. This paper makes the\\nfollowing contributions: (a) Largest context size neural network: We train one\\nof the largest context size transformers on long video and language sequences,\\nsetting new benchmarks in difficult retrieval tasks and long video\\nunderstanding. (b) Solutions for overcoming vision-language training\\nchallenges, including using masked sequence packing for mixing different\\nsequence lengths, loss weighting to balance language and vision, and\\nmodel-generated QA dataset for long sequence chat. (c) A highly-optimized\\nimplementation with RingAttention, masked sequence packing, and other key\\nfeatures for training on millions-length multimodal sequences. (d) Fully\\nopen-sourced a family of 7B parameter models capable of processing long text\\ndocuments (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M\\ntokens. This work paves the way for training on massive datasets of long video\\nand language to develop understanding of both human knowledge and the\\nmultimodal world, and broader capabilities.'}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parent document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SME_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
