{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from SQL Lite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched records.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "with sqlite3.connect(\"data/papers.db\") as conn:\n",
    "    conn.row_factory = sqlite3.Row  # Set row factory to sqlite3.Row for named columns\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Execute the query to select all columns from the papers table\n",
    "    c.execute(\"SELECT * FROM papers\")\n",
    "\n",
    "    # Fetch all results\n",
    "    all_rows = c.fetchall()\n",
    "\n",
    "    # Convert fetched rows to list of dictionaries\n",
    "    paper_metadata = [dict(row) for row in all_rows]\n",
    "\n",
    "    # Check if any rows were fetched and print the results\n",
    "    if paper_metadata:\n",
    "        print(\"Fetched records.\")\n",
    "        # for row in list_of_dicts:\n",
    "        #     print(row)\n",
    "    else:\n",
    "        print(\"No records found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'https://paperswithcode.com/paper/transparent-image-layer-diffusion-using',\n",
       " 'title': 'Transparent Image Layer Diffusion using Latent Transparency',\n",
       " 'arxiv_link': 'https://arxiv.org/pdf/2402.17113v2.pdf',\n",
       " 'published': '2024-02-28',\n",
       " 'authors': 'Lvmin Zhang, Maneesh Agrawala',\n",
       " 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(paper_metadata))\n",
    "paper_metadata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load From JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the filename\n",
    "filename = \"data/paper_metadata.json\"\n",
    "\n",
    "# Write the dictionary to a file\n",
    "with open(filename, \"r\") as file:\n",
    "    paper_metadata = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'https://paperswithcode.com/paper/transparent-image-layer-diffusion-using',\n",
       " 'title': 'Transparent Image Layer Diffusion using Latent Transparency',\n",
       " 'arxiv_link': 'https://arxiv.org/pdf/2402.17113v2.pdf',\n",
       " 'published': '2024-02-28',\n",
       " 'authors': 'Lvmin Zhang, Maneesh Agrawala',\n",
       " 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(paper_metadata))\n",
    "paper_metadata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing - Load & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for paper in paper_metadata:\n",
    "    link = paper[\"arxiv_link\"]\n",
    "    loader = PyPDFLoader(link)\n",
    "    doc = loader.load_and_split()\n",
    "    for idoc in doc:\n",
    "        idoc.metadata[\"title\"] = paper[\"title\"]\n",
    "        idoc.metadata[\"published\"] = paper[\"published\"]\n",
    "        idoc.metadata[\"authors\"] = paper[\"authors\"]\n",
    "        idoc.metadata[\"summary\"] = paper[\"summary\"]\n",
    "    docs.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://arxiv.org/pdf/2402.17113v2.pdf',\n",
       " 'page': 0,\n",
       " 'title': 'Transparent Image Layer Diffusion using Latent Transparency',\n",
       " 'published': '2024-02-28',\n",
       " 'authors': 'Lvmin Zhang, Maneesh Agrawala',\n",
       " 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing - Store\n",
    "### Vectorization & Embedding\n",
    "\n",
    "Cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Embedding Cost= ~$0.034\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Create encoder\n",
    "encoder = tiktoken.get_encoding('cl100k_base')\n",
    "tokens_per_docs = [len(encoder.encode(doc.page_content)) for doc in splits]\n",
    "\n",
    "\n",
    "# Estimated cost = sum of tokens / 1000\n",
    "cost_per_1000_tokens = 0.0001\n",
    "cost = (sum(tokens_per_docs) / 1000) * cost_per_1000_tokens\n",
    "print(f\"Total Embedding Cost= ~${round(cost,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#Embed and store the texts\n",
    "# Supplying a persist dicrectory will store the embeddings on disk\n",
    "persist_direcory = 'data/vectordb'\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=persist_direcory)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull specific paper from vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = vectordb.similarity_search(query= \"Transparent Image Layer Diffusion using Latent Transparency\")\n",
    "# for result in results:\n",
    "#     print(result.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval & Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})\n",
    "# search_kwargs={\"score_threshold\": 0.5}\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_chain = (\n",
    "#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "#     | custom_rag_prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in rag_chain.stream(\"Describe the currently trending research.\"):\n",
    "#     print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "response = rag_chain_with_source.invoke(\"Summarize the paper titled Transparent Image Layer Diffusion using Latent Transparency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'authors': 'Lvmin Zhang, Maneesh Agrawala', 'page': 0, 'published': '2024-02-28', 'source': 'https://arxiv.org/pdf/2402.17113v2.pdf', 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.', 'title': 'Transparent Image Layer Diffusion using Latent Transparency'}\n",
      "{'authors': 'Lvmin Zhang, Maneesh Agrawala', 'page': 0, 'published': '2024-02-28', 'source': 'https://arxiv.org/pdf/2402.17113v2.pdf', 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.', 'title': 'Transparent Image Layer Diffusion using Latent Transparency'}\n",
      "{'authors': 'Lvmin Zhang, Maneesh Agrawala', 'page': 0, 'published': '2024-02-28', 'source': 'https://arxiv.org/pdf/2402.17113v2.pdf', 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.', 'title': 'Transparent Image Layer Diffusion using Latent Transparency'}\n",
      "{'authors': 'Lvmin Zhang, Maneesh Agrawala', 'page': 16, 'published': '2024-02-28', 'source': 'https://arxiv.org/pdf/2402.17113v2.pdf', 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.', 'title': 'Transparent Image Layer Diffusion using Latent Transparency'}\n",
      "{'authors': 'Lvmin Zhang, Maneesh Agrawala', 'page': 16, 'published': '2024-02-28', 'source': 'https://arxiv.org/pdf/2402.17113v2.pdf', 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.', 'title': 'Transparent Image Layer Diffusion using Latent Transparency'}\n",
      "{'authors': 'Lvmin Zhang, Maneesh Agrawala', 'page': 2, 'published': '2024-02-28', 'source': 'https://arxiv.org/pdf/2402.17113v2.pdf', 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.', 'title': 'Transparent Image Layer Diffusion using Latent Transparency'}\n",
      "{'authors': 'Lvmin Zhang, Maneesh Agrawala', 'page': 2, 'published': '2024-02-28', 'source': 'https://arxiv.org/pdf/2402.17113v2.pdf', 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.', 'title': 'Transparent Image Layer Diffusion using Latent Transparency'}\n",
      "{'authors': 'Lvmin Zhang, Maneesh Agrawala', 'page': 16, 'published': '2024-02-28', 'source': 'https://arxiv.org/pdf/2402.17113v2.pdf', 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.', 'title': 'Transparent Image Layer Diffusion using Latent Transparency'}\n",
      "{'authors': 'Lvmin Zhang, Maneesh Agrawala', 'page': 16, 'published': '2024-02-28', 'source': 'https://arxiv.org/pdf/2402.17113v2.pdf', 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.', 'title': 'Transparent Image Layer Diffusion using Latent Transparency'}\n",
      "{'authors': 'Lvmin Zhang, Maneesh Agrawala', 'page': 1, 'published': '2024-02-28', 'source': 'https://arxiv.org/pdf/2402.17113v2.pdf', 'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.', 'title': 'Transparent Image Layer Diffusion using Latent Transparency'}\n"
     ]
    }
   ],
   "source": [
    "for doc in response['context']:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper introduces \"latent transparency\" to enable large-scale pretrained latent diffusion models to generate transparent images and layers. It encodes alpha channel transparency into the latent distribution of Stable Diffusion while preserving high-quality output. The method was trained with 1M transparent image layer pairs and user studies showed preference for the generated transparent content over traditional methods.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The research introduces the concept of \"latent transparency,\" which allows latent diffusion models to generate transparent images and multiple transparent layers without altering the original latent distribution. This methodology enables encoding alpha channel transparency into the latent space. The approach maintains high-quality results and is preferred over traditional methods like matting, showing comparable quality to commercial assets.')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"Summarize the paper titled Transparent Image Layer Diffusion using Latent Transparency\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are the notable methodologies in this research?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SME_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
