{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/paper_metadata.json\"\n",
    "\n",
    "# Write the dictionary to a file\n",
    "with open(filename, \"r\") as file:\n",
    "    paper_metadata = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://paperswithcode.com/paper/transparent-image-layer-diffusion-using',\n",
       "  'title': 'Transparent Image Layer Diffusion using Latent Transparency',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.17113v2.pdf',\n",
       "  'published': '2024-02-28',\n",
       "  'authors': 'Lvmin Zhang, Maneesh Agrawala',\n",
       "  'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.'},\n",
       " {'url': 'https://paperswithcode.com/paper/intent-based-prompt-calibration-enhancing',\n",
       "  'title': 'Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.03099v1.pdf',\n",
       "  'published': '2024-02-05',\n",
       "  'authors': 'Elad Levi, Eli Brosh, Matan Friedmann',\n",
       "  'summary': \"Prompt engineering is a challenging and important task due to the high\\nsensitivity of Large Language Models (LLMs) to the given prompt and the\\ninherent ambiguity of a textual task instruction. Automatic prompt engineering\\nis essential to achieve optimized performance from LLMs. Recent studies have\\ndemonstrated the capabilities of LLMs to automatically conduct prompt\\nengineering by employing a meta-prompt that incorporates the outcomes of the\\nlast trials and proposes an improved prompt. However, this requires a\\nhigh-quality benchmark to compare different prompts, which is difficult and\\nexpensive to acquire in many real-world use cases. In this work, we introduce a\\nnew method for automatic prompt engineering, using a calibration process that\\niteratively refines the prompt to the user intent. During the optimization\\nprocess, the system jointly generates synthetic data of boundary use cases and\\noptimizes the prompt according to the generated dataset. We demonstrate the\\neffectiveness of our method with respect to strong proprietary models on\\nreal-world tasks such as moderation and generation. Our method outperforms\\nstate-of-the-art methods with a limited number of annotated samples.\\nFurthermore, we validate the advantages of each one of the system's key\\ncomponents. Our system is built in a modular way, facilitating easy adaptation\\nto other tasks. The code is available\\n$\\\\href{https://github.com/Eladlev/AutoPrompt}{here}$.\"},\n",
       " {'url': 'https://paperswithcode.com/paper/sora-a-review-on-background-technology',\n",
       "  'title': 'Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.17177v2.pdf',\n",
       "  'published': '2024-02-28',\n",
       "  'authors': 'Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, Lichao Sun',\n",
       "  'summary': 'Sora is a text-to-video generative AI model, released by OpenAI in February\\n2024. The model is trained to generate videos of realistic or imaginative\\nscenes from text instructions and show potential in simulating the physical\\nworld. Based on public technical reports and reverse engineering, this paper\\npresents a comprehensive review of the model\\'s background, related\\ntechnologies, applications, remaining challenges, and future directions of\\ntext-to-video AI models. We first trace Sora\\'s development and investigate the\\nunderlying technologies used to build this \"world simulator\". Then, we describe\\nin detail the applications and potential impact of Sora in multiple industries\\nranging from film-making and education to marketing. We discuss the main\\nchallenges and limitations that need to be addressed to widely deploy Sora,\\nsuch as ensuring safe and unbiased video generation. Lastly, we discuss the\\nfuture development of Sora and video generation models in general, and how\\nadvancements in the field could enable new ways of human-AI interaction,\\nboosting productivity and creativity of video generation.'},\n",
       " {'url': 'https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using',\n",
       "  'title': 'YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.13616v2.pdf',\n",
       "  'published': '2024-02-29',\n",
       "  'authors': 'Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao',\n",
       "  'summary': \"Today's deep learning methods focus on how to design the most appropriate\\nobjective functions so that the prediction results of the model can be closest\\nto the ground truth. Meanwhile, an appropriate architecture that can facilitate\\nacquisition of enough information for prediction has to be designed. Existing\\nmethods ignore a fact that when input data undergoes layer-by-layer feature\\nextraction and spatial transformation, large amount of information will be\\nlost. This paper will delve into the important issues of data loss when data is\\ntransmitted through deep networks, namely information bottleneck and reversible\\nfunctions. We proposed the concept of programmable gradient information (PGI)\\nto cope with the various changes required by deep networks to achieve multiple\\nobjectives. PGI can provide complete input information for the target task to\\ncalculate objective function, so that reliable gradient information can be\\nobtained to update network weights. In addition, a new lightweight network\\narchitecture -- Generalized Efficient Layer Aggregation Network (GELAN), based\\non gradient path planning is designed. GELAN's architecture confirms that PGI\\nhas gained superior results on lightweight models. We verified the proposed\\nGELAN and PGI on MS COCO dataset based object detection. The results show that\\nGELAN only uses conventional convolution operators to achieve better parameter\\nutilization than the state-of-the-art methods developed based on depth-wise\\nconvolution. PGI can be used for variety of models from lightweight to large.\\nIt can be used to obtain complete information, so that train-from-scratch\\nmodels can achieve better results than state-of-the-art models pre-trained\\nusing large datasets, the comparison results are shown in Figure 1. The source\\ncodes are at: https://github.com/WongKinYiu/yolov9.\"},\n",
       " {'url': 'https://paperswithcode.com/paper/datasets-for-large-language-models-a',\n",
       "  'title': 'Datasets for Large Language Models: A Comprehensive Survey',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.18041v1.pdf',\n",
       "  'published': '2024-02-28',\n",
       "  'authors': 'Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin',\n",
       "  'summary': 'This paper embarks on an exploration into the Large Language Model (LLM)\\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\\ndatasets serve as the foundational infrastructure analogous to a root system\\nthat sustains and nurtures the development of LLMs. Consequently, examination\\nof these datasets emerges as a critical topic in research. In order to address\\nthe current lack of a comprehensive overview and thorough analysis of LLM\\ndatasets, and to gain insights into their current status and future trends,\\nthis survey consolidates and categorizes the fundamental aspects of LLM\\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\\non the prevailing challenges and points out potential avenues for future\\ninvestigation. Additionally, a comprehensive review of the existing available\\ndataset resources is also provided, including statistics from 444 datasets,\\ncovering 8 language categories and spanning 32 domains. Information from 20\\ndimensions is incorporated into the dataset statistics. The total data size\\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\\nother datasets. We aim to present the entire landscape of LLM text datasets,\\nserving as a comprehensive reference for researchers in this field and\\ncontributing to future studies. Related resources are available at:\\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.'},\n",
       " {'url': 'https://paperswithcode.com/paper/learning-to-generate-instruction-tuning',\n",
       "  'title': 'Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.18334v1.pdf',\n",
       "  'published': '2024-02-28',\n",
       "  'authors': 'Nihal V. Nayak, Yiyang Nan, Avi Trost, Stephen H. Bach',\n",
       "  'summary': \"We introduce Bonito, an open-source model for conditional task generation:\\nthe task of converting unannotated text into task-specific training datasets\\nfor instruction tuning. Our goal is to enable zero-shot task adaptation of\\nlarge language models on users' specialized, private data. We train Bonito on a\\nnew large-scale dataset with 1.65M examples created by remixing existing\\ninstruction tuning datasets into meta-templates. The meta-templates for a\\ndataset produce training examples where the input is the unannotated text and\\nthe task attribute and the output consists of the instruction and the response.\\nWe use Bonito to generate synthetic tasks for seven datasets from specialized\\ndomains across three task types -- yes-no question answering, extractive\\nquestion answering, and natural language inference -- and adapt language\\nmodels. We show that Bonito significantly improves the average performance of\\npretrained and instruction tuned models over the de facto self supervised\\nbaseline. For example, adapting Mistral-Instruct-v2 and instruction tuned\\nvariants of Mistral and Llama2 with Bonito improves the strong zero-shot\\nperformance by 22.1 F1 points whereas the next word prediction objective undoes\\nsome of the benefits of instruction tuning and reduces the average performance\\nby 0.8 F1 points. We conduct additional experiments with Bonito to understand\\nthe effects of the domain, the size of the training set, and the choice of\\nalternative synthetic task generators. Overall, we show that learning with\\nsynthetic instruction tuning datasets is an effective way to adapt language\\nmodels to new domains. The model, dataset, and code are available at\\nhttps://github.com/BatsResearch/bonito.\"},\n",
       " {'url': 'https://paperswithcode.com/paper/mobillama-towards-accurate-and-lightweight',\n",
       "  'title': 'MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.16840v1.pdf',\n",
       "  'published': '2024-02-26',\n",
       "  'authors': 'Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz Khan',\n",
       "  'summary': '\"Bigger the better\" has been the predominant trend in recent Large Language\\nModels (LLMs) development. However, LLMs do not suit well for scenarios that\\nrequire on-device processing, energy efficiency, low memory footprint, and\\nresponse efficiency. These requisites are crucial for privacy, security, and\\nsustainable deployment. This paper explores the \"less is more\" paradigm by\\naddressing the challenge of designing accurate yet efficient Small Language\\nModels (SLMs) for resource constrained devices. Our primary contribution is the\\nintroduction of an accurate and fully transparent open-source 0.5 billion\\n(0.5B) parameter SLM, named MobiLlama, catering to the specific needs of\\nresource-constrained computing with an emphasis on enhanced performance with\\nreduced resource demands. MobiLlama is a SLM design that initiates from a\\nlarger model and applies a careful parameter sharing scheme to reduce both the\\npre-training and the deployment cost. Our work strives to not only bridge the\\ngap in open-source SLMs but also ensures full transparency, where complete\\ntraining data pipeline, training code, model weights, and over 300 checkpoints\\nalong with evaluation codes is available at :\\nhttps://github.com/mbzuai-oryx/MobiLlama.'},\n",
       " {'url': 'https://paperswithcode.com/paper/training-free-long-context-scaling-of-large',\n",
       "  'title': 'Training-Free Long-Context Scaling of Large Language Models',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.17463v1.pdf',\n",
       "  'published': '2024-02-27',\n",
       "  'authors': 'Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, Lingpeng Kong',\n",
       "  'summary': 'The ability of Large Language Models (LLMs) to process and generate coherent\\ntext is markedly weakened when the number of input tokens exceeds their\\npretraining length. Given the expensive overhead of finetuning large-scale\\nmodels with longer sequences, we propose Dual Chunk Attention (DCA), which\\nenables Llama2 70B to support context windows of more than 100k tokens without\\ncontinual training. By decomposing the attention computation for long sequences\\ninto chunk-based modules, DCA manages to effectively capture the relative\\npositional information of tokens within the same chunk (Intra-Chunk) and across\\ndistinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash\\nAttention. In addition to its impressive extrapolation capability, DCA achieves\\nperformance on practical long-context tasks that is comparable to or even\\nbetter than that of finetuned models. When compared with proprietary models,\\nour training-free 70B model attains 94% of the performance of gpt-3.5-16k,\\nindicating it is a viable open-source alternative. All code and data used in\\nthis work are released at \\\\url{https://github.com/HKUNLP/ChunkLlama}.'},\n",
       " {'url': 'https://paperswithcode.com/paper/bitnet-scaling-1-bit-transformers-for-large',\n",
       "  'title': 'BitNet: Scaling 1-bit Transformers for Large Language Models',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2310.11453v1.pdf',\n",
       "  'published': '2023-10-17',\n",
       "  'authors': 'Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei',\n",
       "  'summary': 'The increasing size of large language models has posed challenges for\\ndeployment and raised concerns about environmental impact due to high energy\\nconsumption. In this work, we introduce BitNet, a scalable and stable 1-bit\\nTransformer architecture designed for large language models. Specifically, we\\nintroduce BitLinear as a drop-in replacement of the nn.Linear layer in order to\\ntrain 1-bit weights from scratch. Experimental results on language modeling\\nshow that BitNet achieves competitive performance while substantially reducing\\nmemory footprint and energy consumption, compared to state-of-the-art 8-bit\\nquantization methods and FP16 Transformer baselines. Furthermore, BitNet\\nexhibits a scaling law akin to full-precision Transformers, suggesting its\\npotential for effective scaling to even larger language models while\\nmaintaining efficiency and performance benefits.'},\n",
       " {'url': 'https://paperswithcode.com/paper/the-first-place-solution-of-wsdm-cup-2024',\n",
       "  'title': 'The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.18385v1.pdf',\n",
       "  'published': '2024-02-28',\n",
       "  'authors': 'Yiming Li, Zhao Zhang',\n",
       "  'summary': 'Conversational multi-doc question answering aims to answer specific questions\\nbased on the retrieved documents as well as the contextual conversations. In\\nthis paper, we introduce our winning approach for the \"Conversational Multi-Doc\\nQA\" challenge in WSDM Cup 2024, which exploits the superior natural language\\nunderstanding and generation capability of Large Language Models (LLMs). We\\nfirst adapt LLMs to the task, then devise a hybrid training strategy to make\\nthe most of in-domain unlabeled data. Moreover, an advanced text embedding\\nmodel is adopted to filter out potentially irrelevant documents and several\\napproaches are designed and compared for the model ensemble. Equipped with all\\nthese techniques, our solution finally ranked 1st place in WSDM Cup 2024,\\nsurpassing its rivals to a large extent. The source codes have been released at\\nhttps://github.com/zhangzhao219/WSDM-Cup-2024.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2?usp=sharing#scrollTo=cIu9afMo1YYg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "titles = []\n",
    "for paper in paper_metadata:\n",
    "    abstracts.append(paper['summary'])\n",
    "    titles.append(paper['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.',\n",
       " \"Prompt engineering is a challenging and important task due to the high\\nsensitivity of Large Language Models (LLMs) to the given prompt and the\\ninherent ambiguity of a textual task instruction. Automatic prompt engineering\\nis essential to achieve optimized performance from LLMs. Recent studies have\\ndemonstrated the capabilities of LLMs to automatically conduct prompt\\nengineering by employing a meta-prompt that incorporates the outcomes of the\\nlast trials and proposes an improved prompt. However, this requires a\\nhigh-quality benchmark to compare different prompts, which is difficult and\\nexpensive to acquire in many real-world use cases. In this work, we introduce a\\nnew method for automatic prompt engineering, using a calibration process that\\niteratively refines the prompt to the user intent. During the optimization\\nprocess, the system jointly generates synthetic data of boundary use cases and\\noptimizes the prompt according to the generated dataset. We demonstrate the\\neffectiveness of our method with respect to strong proprietary models on\\nreal-world tasks such as moderation and generation. Our method outperforms\\nstate-of-the-art methods with a limited number of annotated samples.\\nFurthermore, we validate the advantages of each one of the system's key\\ncomponents. Our system is built in a modular way, facilitating easy adaptation\\nto other tasks. The code is available\\n$\\\\href{https://github.com/Eladlev/AutoPrompt}{here}$.\",\n",
       " 'Sora is a text-to-video generative AI model, released by OpenAI in February\\n2024. The model is trained to generate videos of realistic or imaginative\\nscenes from text instructions and show potential in simulating the physical\\nworld. Based on public technical reports and reverse engineering, this paper\\npresents a comprehensive review of the model\\'s background, related\\ntechnologies, applications, remaining challenges, and future directions of\\ntext-to-video AI models. We first trace Sora\\'s development and investigate the\\nunderlying technologies used to build this \"world simulator\". Then, we describe\\nin detail the applications and potential impact of Sora in multiple industries\\nranging from film-making and education to marketing. We discuss the main\\nchallenges and limitations that need to be addressed to widely deploy Sora,\\nsuch as ensuring safe and unbiased video generation. Lastly, we discuss the\\nfuture development of Sora and video generation models in general, and how\\nadvancements in the field could enable new ways of human-AI interaction,\\nboosting productivity and creativity of video generation.',\n",
       " \"Today's deep learning methods focus on how to design the most appropriate\\nobjective functions so that the prediction results of the model can be closest\\nto the ground truth. Meanwhile, an appropriate architecture that can facilitate\\nacquisition of enough information for prediction has to be designed. Existing\\nmethods ignore a fact that when input data undergoes layer-by-layer feature\\nextraction and spatial transformation, large amount of information will be\\nlost. This paper will delve into the important issues of data loss when data is\\ntransmitted through deep networks, namely information bottleneck and reversible\\nfunctions. We proposed the concept of programmable gradient information (PGI)\\nto cope with the various changes required by deep networks to achieve multiple\\nobjectives. PGI can provide complete input information for the target task to\\ncalculate objective function, so that reliable gradient information can be\\nobtained to update network weights. In addition, a new lightweight network\\narchitecture -- Generalized Efficient Layer Aggregation Network (GELAN), based\\non gradient path planning is designed. GELAN's architecture confirms that PGI\\nhas gained superior results on lightweight models. We verified the proposed\\nGELAN and PGI on MS COCO dataset based object detection. The results show that\\nGELAN only uses conventional convolution operators to achieve better parameter\\nutilization than the state-of-the-art methods developed based on depth-wise\\nconvolution. PGI can be used for variety of models from lightweight to large.\\nIt can be used to obtain complete information, so that train-from-scratch\\nmodels can achieve better results than state-of-the-art models pre-trained\\nusing large datasets, the comparison results are shown in Figure 1. The source\\ncodes are at: https://github.com/WongKinYiu/yolov9.\",\n",
       " 'This paper embarks on an exploration into the Large Language Model (LLM)\\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\\ndatasets serve as the foundational infrastructure analogous to a root system\\nthat sustains and nurtures the development of LLMs. Consequently, examination\\nof these datasets emerges as a critical topic in research. In order to address\\nthe current lack of a comprehensive overview and thorough analysis of LLM\\ndatasets, and to gain insights into their current status and future trends,\\nthis survey consolidates and categorizes the fundamental aspects of LLM\\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\\non the prevailing challenges and points out potential avenues for future\\ninvestigation. Additionally, a comprehensive review of the existing available\\ndataset resources is also provided, including statistics from 444 datasets,\\ncovering 8 language categories and spanning 32 domains. Information from 20\\ndimensions is incorporated into the dataset statistics. The total data size\\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\\nother datasets. We aim to present the entire landscape of LLM text datasets,\\nserving as a comprehensive reference for researchers in this field and\\ncontributing to future studies. Related resources are available at:\\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.',\n",
       " \"We introduce Bonito, an open-source model for conditional task generation:\\nthe task of converting unannotated text into task-specific training datasets\\nfor instruction tuning. Our goal is to enable zero-shot task adaptation of\\nlarge language models on users' specialized, private data. We train Bonito on a\\nnew large-scale dataset with 1.65M examples created by remixing existing\\ninstruction tuning datasets into meta-templates. The meta-templates for a\\ndataset produce training examples where the input is the unannotated text and\\nthe task attribute and the output consists of the instruction and the response.\\nWe use Bonito to generate synthetic tasks for seven datasets from specialized\\ndomains across three task types -- yes-no question answering, extractive\\nquestion answering, and natural language inference -- and adapt language\\nmodels. We show that Bonito significantly improves the average performance of\\npretrained and instruction tuned models over the de facto self supervised\\nbaseline. For example, adapting Mistral-Instruct-v2 and instruction tuned\\nvariants of Mistral and Llama2 with Bonito improves the strong zero-shot\\nperformance by 22.1 F1 points whereas the next word prediction objective undoes\\nsome of the benefits of instruction tuning and reduces the average performance\\nby 0.8 F1 points. We conduct additional experiments with Bonito to understand\\nthe effects of the domain, the size of the training set, and the choice of\\nalternative synthetic task generators. Overall, we show that learning with\\nsynthetic instruction tuning datasets is an effective way to adapt language\\nmodels to new domains. The model, dataset, and code are available at\\nhttps://github.com/BatsResearch/bonito.\",\n",
       " '\"Bigger the better\" has been the predominant trend in recent Large Language\\nModels (LLMs) development. However, LLMs do not suit well for scenarios that\\nrequire on-device processing, energy efficiency, low memory footprint, and\\nresponse efficiency. These requisites are crucial for privacy, security, and\\nsustainable deployment. This paper explores the \"less is more\" paradigm by\\naddressing the challenge of designing accurate yet efficient Small Language\\nModels (SLMs) for resource constrained devices. Our primary contribution is the\\nintroduction of an accurate and fully transparent open-source 0.5 billion\\n(0.5B) parameter SLM, named MobiLlama, catering to the specific needs of\\nresource-constrained computing with an emphasis on enhanced performance with\\nreduced resource demands. MobiLlama is a SLM design that initiates from a\\nlarger model and applies a careful parameter sharing scheme to reduce both the\\npre-training and the deployment cost. Our work strives to not only bridge the\\ngap in open-source SLMs but also ensures full transparency, where complete\\ntraining data pipeline, training code, model weights, and over 300 checkpoints\\nalong with evaluation codes is available at :\\nhttps://github.com/mbzuai-oryx/MobiLlama.',\n",
       " 'The ability of Large Language Models (LLMs) to process and generate coherent\\ntext is markedly weakened when the number of input tokens exceeds their\\npretraining length. Given the expensive overhead of finetuning large-scale\\nmodels with longer sequences, we propose Dual Chunk Attention (DCA), which\\nenables Llama2 70B to support context windows of more than 100k tokens without\\ncontinual training. By decomposing the attention computation for long sequences\\ninto chunk-based modules, DCA manages to effectively capture the relative\\npositional information of tokens within the same chunk (Intra-Chunk) and across\\ndistinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash\\nAttention. In addition to its impressive extrapolation capability, DCA achieves\\nperformance on practical long-context tasks that is comparable to or even\\nbetter than that of finetuned models. When compared with proprietary models,\\nour training-free 70B model attains 94% of the performance of gpt-3.5-16k,\\nindicating it is a viable open-source alternative. All code and data used in\\nthis work are released at \\\\url{https://github.com/HKUNLP/ChunkLlama}.',\n",
       " 'The increasing size of large language models has posed challenges for\\ndeployment and raised concerns about environmental impact due to high energy\\nconsumption. In this work, we introduce BitNet, a scalable and stable 1-bit\\nTransformer architecture designed for large language models. Specifically, we\\nintroduce BitLinear as a drop-in replacement of the nn.Linear layer in order to\\ntrain 1-bit weights from scratch. Experimental results on language modeling\\nshow that BitNet achieves competitive performance while substantially reducing\\nmemory footprint and energy consumption, compared to state-of-the-art 8-bit\\nquantization methods and FP16 Transformer baselines. Furthermore, BitNet\\nexhibits a scaling law akin to full-precision Transformers, suggesting its\\npotential for effective scaling to even larger language models while\\nmaintaining efficiency and performance benefits.',\n",
       " 'Conversational multi-doc question answering aims to answer specific questions\\nbased on the retrieved documents as well as the contextual conversations. In\\nthis paper, we introduce our winning approach for the \"Conversational Multi-Doc\\nQA\" challenge in WSDM Cup 2024, which exploits the superior natural language\\nunderstanding and generation capability of Large Language Models (LLMs). We\\nfirst adapt LLMs to the task, then devise a hybrid training strategy to make\\nthe most of in-domain unlabeled data. Moreover, an advanced text embedding\\nmodel is adopted to filter out potentially irrelevant documents and several\\napproaches are designed and compared for the model ensemble. Equipped with all\\nthese techniques, our solution finally ranked 1st place in WSDM Cup 2024,\\nsurpassing its rivals to a large extent. The source codes have been released at\\nhttps://github.com/zhangzhao219/WSDM-Cup-2024.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transparent Image Layer Diffusion using Latent Transparency',\n",
       " 'Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases',\n",
       " 'Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models',\n",
       " 'YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information',\n",
       " 'Datasets for Large Language Models: A Comprehensive Survey',\n",
       " 'Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation',\n",
       " 'MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT',\n",
       " 'Training-Free Long-Context Scaling of Large Language Models',\n",
       " 'BitNet: Scaling 1-bit Transformers for Large Language Models',\n",
       " 'The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentences = [sent_tokenize(abstract) for abstract in abstracts]\n",
    "sentences = [sentence for doc in sentences for sentence in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-calcuate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Pre-calculate embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevent stochastic behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=2, n_components=1, min_dist=0.0, metric='cosine', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Number of Topics\n",
    "\n",
    "Use MLFlow for tracking best input parameters (n_neighbors, n_components, min_dfm min_cluster_size, etc) and resulting number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Allign metric with UMAP?\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=2, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Default Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=1, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
    "\n",
    "# KeyBERT\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "# Part-of-Speech\n",
    "# pos_model = PartOfSpeech(\"en_core_web_sm\")\n",
    "\n",
    "# MMR\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# GPT-3.5\n",
    "prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n",
    "topic: <topic label>\n",
    "\"\"\"\n",
    "    \n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"OpenAI\": openai_model,  # Uncomment if you will use OpenAI\n",
    "    \"MMR\": mmr_model,\n",
    "    # \"POS\": pos_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 17:55:58,773 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 17:55:59,695 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-03-05 17:55:59,697 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-03-05 17:55:59,724 - BERTopic - Cluster - Completed ✓\n",
      "2024-03-05 17:55:59,731 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n",
      "2024-03-05 17:56:11,340 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\n",
    "  # Pipeline models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  representation_model=representation_model,\n",
    "\n",
    "  # Hyperparameters\n",
    "  top_n_words=10,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(abstracts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 0, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [('datasets', 0.047192280763216785),\n",
       "  ('model', 0.03656682618864347),\n",
       "  ('task', 0.03094053284629336),\n",
       "  ('prompt', 0.029016748790358354),\n",
       "  ('generation', 0.029016748790358354),\n",
       "  ('latent', 0.029016748790358354),\n",
       "  ('instruction', 0.029016748790358354),\n",
       "  ('language', 0.028128327837418053),\n",
       "  ('models', 0.02618493118479418),\n",
       "  ('transparent', 0.02579266559142965)],\n",
       " 1: [('models', 0.054457542880977906),\n",
       "  ('information', 0.041262874629549424),\n",
       "  ('large', 0.03649971392021519),\n",
       "  ('results', 0.0334742279690802),\n",
       "  ('language', 0.031499612271014554),\n",
       "  ('language models', 0.02969903184180259),\n",
       "  ('pgi', 0.02884357679044659),\n",
       "  ('chunk', 0.02884357679044659),\n",
       "  ('performance', 0.025789296643468386),\n",
       "  ('data', 0.025789296643468386)]}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>OpenAI</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0_datasets_model_task_prompt</td>\n",
       "      <td>[datasets, model, task, prompt, generation, la...</td>\n",
       "      <td>[language models, video generation, question a...</td>\n",
       "      <td>[Automated Prompt Engineering]</td>\n",
       "      <td>[datasets, model, task, prompt, generation, la...</td>\n",
       "      <td>[We present LayerDiffusion, an approach enabli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1_models_information_large_results</td>\n",
       "      <td>[models, information, large, results, language...</td>\n",
       "      <td>[large language, memory, memory footprint, dee...</td>\n",
       "      <td>[Efficient Small Language Models]</td>\n",
       "      <td>[models, information, large, results, language...</td>\n",
       "      <td>[The ability of Large Language Models (LLMs) t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                Name  \\\n",
       "0      0      6        0_datasets_model_task_prompt   \n",
       "1      1      4  1_models_information_large_results   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [datasets, model, task, prompt, generation, la...   \n",
       "1  [models, information, large, results, language...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [language models, video generation, question a...   \n",
       "1  [large language, memory, memory footprint, dee...   \n",
       "\n",
       "                              OpenAI  \\\n",
       "0     [Automated Prompt Engineering]   \n",
       "1  [Efficient Small Language Models]   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [datasets, model, task, prompt, generation, la...   \n",
       "1  [models, information, large, results, language...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [We present LayerDiffusion, an approach enabli...  \n",
       "1  [The ability of Large Language Models (LLMs) t...  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Main': [('datasets', 0.16955208084578507),\n",
       "  ('model', 0.13226718741478377),\n",
       "  ('task', 0.11726174577864121),\n",
       "  ('language', 0.10174399031906442),\n",
       "  ('transparent', 0.0996209403156825),\n",
       "  ('llms', 0.09380939662291297),\n",
       "  ('models', 0.0912251082638667),\n",
       "  ('text', 0.09017109174198108),\n",
       "  ('large', 0.08310533484956989),\n",
       "  ('dataset', 0.08021949231172919)],\n",
       " 'KeyBERT': [('language models', 0.5080408),\n",
       "  ('large language', 0.395446),\n",
       "  ('learning', 0.32574537),\n",
       "  ('language', 0.30879468),\n",
       "  ('generate', 0.30399048),\n",
       "  ('text', 0.29888886),\n",
       "  ('models llms', 0.27738488),\n",
       "  ('pre training', 0.27479458),\n",
       "  ('trained', 0.27347666),\n",
       "  ('datasets', 0.27270696)],\n",
       " 'OpenAI': [('Automatic Prompt Engineering', 1)],\n",
       " 'MMR': [('datasets', 0.16955208084578507),\n",
       "  ('model', 0.13226718741478377),\n",
       "  ('task', 0.11726174577864121),\n",
       "  ('language', 0.10174399031906442),\n",
       "  ('transparent', 0.0996209403156825),\n",
       "  ('llms', 0.09380939662291297),\n",
       "  ('models', 0.0912251082638667),\n",
       "  ('text', 0.09017109174198108),\n",
       "  ('large', 0.08310533484956989),\n",
       "  ('dataset', 0.08021949231172919)]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(0, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or ChatGPT's labels\n",
    "chatgpt_topic_labels = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"OpenAI\"].items()}\n",
    "chatgpt_topic_labels[-1] = \"Outlier Topic\"\n",
    "topic_model.set_topic_labels(chatgpt_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KeyBERT': {0: [('language models', 0.47570348),\n",
       "   ('video generation', 0.45058262),\n",
       "   ('question answering', 0.3853972),\n",
       "   ('natural language', 0.38166407),\n",
       "   ('prompt engineering', 0.37694418),\n",
       "   ('large language', 0.35815015),\n",
       "   ('automatic prompt', 0.35813928),\n",
       "   ('pretrained latent', 0.3552686),\n",
       "   ('generate', 0.34009475),\n",
       "   ('pretrained', 0.3373075)],\n",
       "  1: [('large language', 0.42704934),\n",
       "   ('memory', 0.41538286),\n",
       "   ('memory footprint', 0.41059476),\n",
       "   ('deep networks', 0.40945095),\n",
       "   ('language models', 0.3942625),\n",
       "   ('bitnet', 0.3673072),\n",
       "   ('resource constrained', 0.3126595),\n",
       "   ('attention', 0.3055657),\n",
       "   ('slm', 0.30371237),\n",
       "   ('slms', 0.29135308)]},\n",
       " 'OpenAI': {0: [('Automated Prompt Engineering', 1)],\n",
       "  1: [('Efficient Small Language Models', 1)]},\n",
       " 'MMR': {0: [('datasets', 0.047192280763216785),\n",
       "   ('model', 0.03656682618864347),\n",
       "   ('task', 0.03094053284629336),\n",
       "   ('prompt', 0.029016748790358354),\n",
       "   ('generation', 0.029016748790358354),\n",
       "   ('latent', 0.029016748790358354),\n",
       "   ('instruction', 0.029016748790358354),\n",
       "   ('language', 0.028128327837418053),\n",
       "   ('models', 0.02618493118479418),\n",
       "   ('transparent', 0.02579266559142965)],\n",
       "  1: [('models', 0.054457542880977906),\n",
       "   ('information', 0.041262874629549424),\n",
       "   ('large', 0.03649971392021519),\n",
       "   ('results', 0.0334742279690802),\n",
       "   ('language', 0.031499612271014554),\n",
       "   ('language models', 0.02969903184180259),\n",
       "   ('pgi', 0.02884357679044659),\n",
       "   ('chunk', 0.02884357679044659),\n",
       "   ('performance', 0.025789296643468386),\n",
       "   ('data', 0.025789296643468386)]}}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.topic_aspects_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>CustomName</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>OpenAI</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0_datasets_model_task_prompt</td>\n",
       "      <td>Automated Prompt Engineering</td>\n",
       "      <td>[datasets, model, task, prompt, generation, la...</td>\n",
       "      <td>[language models, video generation, question a...</td>\n",
       "      <td>[Automated Prompt Engineering]</td>\n",
       "      <td>[datasets, model, task, prompt, generation, la...</td>\n",
       "      <td>[We present LayerDiffusion, an approach enabli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1_models_information_large_results</td>\n",
       "      <td>Efficient Small Language Models</td>\n",
       "      <td>[models, information, large, results, language...</td>\n",
       "      <td>[large language, memory, memory footprint, dee...</td>\n",
       "      <td>[Efficient Small Language Models]</td>\n",
       "      <td>[models, information, large, results, language...</td>\n",
       "      <td>[The ability of Large Language Models (LLMs) t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                Name  \\\n",
       "0      0      6        0_datasets_model_task_prompt   \n",
       "1      1      4  1_models_information_large_results   \n",
       "\n",
       "                        CustomName  \\\n",
       "0     Automated Prompt Engineering   \n",
       "1  Efficient Small Language Models   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [datasets, model, task, prompt, generation, la...   \n",
       "1  [models, information, large, results, language...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [language models, video generation, question a...   \n",
       "1  [large language, memory, memory footprint, dee...   \n",
       "\n",
       "                              OpenAI  \\\n",
       "0     [Automated Prompt Engineering]   \n",
       "1  [Efficient Small Language Models]   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [datasets, model, task, prompt, generation, la...   \n",
       "1  [models, information, large, results, language...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [We present LayerDiffusion, an approach enabli...  \n",
       "1  [The ability of Large Language Models (LLMs) t...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['large language',\n",
       " 'memory',\n",
       " 'memory footprint',\n",
       " 'deep networks',\n",
       " 'language models',\n",
       " 'bitnet',\n",
       " 'resource constrained',\n",
       " 'attention',\n",
       " 'slm',\n",
       " 'slms']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()['KeyBERT'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topic_list = len(topic_model.get_topic_info()['KeyBERT'])\n",
    "\n",
    "topic_lists = []\n",
    "for i in range(n_topic_list):\n",
    "    topic_lists.append(topic_model.get_topic_info()['KeyBERT'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['language models',\n",
       "  'video generation',\n",
       "  'question answering',\n",
       "  'natural language',\n",
       "  'prompt engineering',\n",
       "  'large language',\n",
       "  'automatic prompt',\n",
       "  'pretrained latent',\n",
       "  'generate',\n",
       "  'pretrained'],\n",
       " ['large language',\n",
       "  'memory',\n",
       "  'memory footprint',\n",
       "  'deep networks',\n",
       "  'language models',\n",
       "  'bitnet',\n",
       "  'resource constrained',\n",
       "  'attention',\n",
       "  'slm',\n",
       "  'slms']]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Models\n",
      "Video Generation\n",
      "Question Answering\n",
      "Natural Language\n",
      "Prompt Engineering\n",
      "Large Language\n",
      "Automatic Prompt\n",
      "Pretrained Latent\n",
      "Generate\n",
      "Pretrained\n"
     ]
    }
   ],
   "source": [
    "n_topic_list = len(topic_model.get_topic_info()['KeyBERT'])\n",
    "topic_list = topic_model.get_topic_info()['KeyBERT'][0]\n",
    "\n",
    "for topic in topic_list:\n",
    "    print(topic.title())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, paper in enumerate(paper_metadata):\n",
    "    topic_id = topics[i]\n",
    "    topic_list = topic_model.get_topic_info()['KeyBERT'][topic_id]\n",
    "\n",
    "    paper['topic_id'] = topic_id\n",
    "    paper['topics'] = topic_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://paperswithcode.com/paper/transparent-image-layer-diffusion-using',\n",
       "  'title': 'Transparent Image Layer Diffusion using Latent Transparency',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.17113v2.pdf',\n",
       "  'published': '2024-02-28',\n",
       "  'authors': 'Lvmin Zhang, Maneesh Agrawala',\n",
       "  'summary': 'We present LayerDiffusion, an approach enabling large-scale pretrained latent\\ndiffusion models to generate transparent images. The method allows generation\\nof single transparent images or of multiple transparent layers. The method\\nlearns a \"latent transparency\" that encodes alpha channel transparency into the\\nlatent manifold of a pretrained latent diffusion model. It preserves the\\nproduction-ready quality of the large diffusion model by regulating the added\\ntransparency as a latent offset with minimal changes to the original latent\\ndistribution of the pretrained model. In this way, any latent diffusion model\\ncan be converted into a transparent image generator by finetuning it with the\\nadjusted latent space. We train the model with 1M transparent image layer pairs\\ncollected using a human-in-the-loop collection scheme. We show that latent\\ntransparency can be applied to different open source image generators, or be\\nadapted to various conditional control systems to achieve applications like\\nforeground/background-conditioned layer generation, joint layer generation,\\nstructural control of layer contents, etc. A user study finds that in most\\ncases (97%) users prefer our natively generated transparent content over\\nprevious ad-hoc solutions such as generating and then matting. Users also\\nreport the quality of our generated transparent images is comparable to real\\ncommercial transparent assets like Adobe Stock.',\n",
       "  'topic_id': 0,\n",
       "  'topics': ['language models',\n",
       "   'video generation',\n",
       "   'question answering',\n",
       "   'natural language',\n",
       "   'prompt engineering',\n",
       "   'large language',\n",
       "   'automatic prompt',\n",
       "   'pretrained latent',\n",
       "   'generate',\n",
       "   'pretrained']},\n",
       " {'url': 'https://paperswithcode.com/paper/intent-based-prompt-calibration-enhancing',\n",
       "  'title': 'Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.03099v1.pdf',\n",
       "  'published': '2024-02-05',\n",
       "  'authors': 'Elad Levi, Eli Brosh, Matan Friedmann',\n",
       "  'summary': \"Prompt engineering is a challenging and important task due to the high\\nsensitivity of Large Language Models (LLMs) to the given prompt and the\\ninherent ambiguity of a textual task instruction. Automatic prompt engineering\\nis essential to achieve optimized performance from LLMs. Recent studies have\\ndemonstrated the capabilities of LLMs to automatically conduct prompt\\nengineering by employing a meta-prompt that incorporates the outcomes of the\\nlast trials and proposes an improved prompt. However, this requires a\\nhigh-quality benchmark to compare different prompts, which is difficult and\\nexpensive to acquire in many real-world use cases. In this work, we introduce a\\nnew method for automatic prompt engineering, using a calibration process that\\niteratively refines the prompt to the user intent. During the optimization\\nprocess, the system jointly generates synthetic data of boundary use cases and\\noptimizes the prompt according to the generated dataset. We demonstrate the\\neffectiveness of our method with respect to strong proprietary models on\\nreal-world tasks such as moderation and generation. Our method outperforms\\nstate-of-the-art methods with a limited number of annotated samples.\\nFurthermore, we validate the advantages of each one of the system's key\\ncomponents. Our system is built in a modular way, facilitating easy adaptation\\nto other tasks. The code is available\\n$\\\\href{https://github.com/Eladlev/AutoPrompt}{here}$.\",\n",
       "  'topic_id': 0,\n",
       "  'topics': ['language models',\n",
       "   'video generation',\n",
       "   'question answering',\n",
       "   'natural language',\n",
       "   'prompt engineering',\n",
       "   'large language',\n",
       "   'automatic prompt',\n",
       "   'pretrained latent',\n",
       "   'generate',\n",
       "   'pretrained']},\n",
       " {'url': 'https://paperswithcode.com/paper/sora-a-review-on-background-technology',\n",
       "  'title': 'Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.17177v2.pdf',\n",
       "  'published': '2024-02-28',\n",
       "  'authors': 'Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, Lichao Sun',\n",
       "  'summary': 'Sora is a text-to-video generative AI model, released by OpenAI in February\\n2024. The model is trained to generate videos of realistic or imaginative\\nscenes from text instructions and show potential in simulating the physical\\nworld. Based on public technical reports and reverse engineering, this paper\\npresents a comprehensive review of the model\\'s background, related\\ntechnologies, applications, remaining challenges, and future directions of\\ntext-to-video AI models. We first trace Sora\\'s development and investigate the\\nunderlying technologies used to build this \"world simulator\". Then, we describe\\nin detail the applications and potential impact of Sora in multiple industries\\nranging from film-making and education to marketing. We discuss the main\\nchallenges and limitations that need to be addressed to widely deploy Sora,\\nsuch as ensuring safe and unbiased video generation. Lastly, we discuss the\\nfuture development of Sora and video generation models in general, and how\\nadvancements in the field could enable new ways of human-AI interaction,\\nboosting productivity and creativity of video generation.',\n",
       "  'topic_id': 0,\n",
       "  'topics': ['language models',\n",
       "   'video generation',\n",
       "   'question answering',\n",
       "   'natural language',\n",
       "   'prompt engineering',\n",
       "   'large language',\n",
       "   'automatic prompt',\n",
       "   'pretrained latent',\n",
       "   'generate',\n",
       "   'pretrained']},\n",
       " {'url': 'https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using',\n",
       "  'title': 'YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.13616v2.pdf',\n",
       "  'published': '2024-02-29',\n",
       "  'authors': 'Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao',\n",
       "  'summary': \"Today's deep learning methods focus on how to design the most appropriate\\nobjective functions so that the prediction results of the model can be closest\\nto the ground truth. Meanwhile, an appropriate architecture that can facilitate\\nacquisition of enough information for prediction has to be designed. Existing\\nmethods ignore a fact that when input data undergoes layer-by-layer feature\\nextraction and spatial transformation, large amount of information will be\\nlost. This paper will delve into the important issues of data loss when data is\\ntransmitted through deep networks, namely information bottleneck and reversible\\nfunctions. We proposed the concept of programmable gradient information (PGI)\\nto cope with the various changes required by deep networks to achieve multiple\\nobjectives. PGI can provide complete input information for the target task to\\ncalculate objective function, so that reliable gradient information can be\\nobtained to update network weights. In addition, a new lightweight network\\narchitecture -- Generalized Efficient Layer Aggregation Network (GELAN), based\\non gradient path planning is designed. GELAN's architecture confirms that PGI\\nhas gained superior results on lightweight models. We verified the proposed\\nGELAN and PGI on MS COCO dataset based object detection. The results show that\\nGELAN only uses conventional convolution operators to achieve better parameter\\nutilization than the state-of-the-art methods developed based on depth-wise\\nconvolution. PGI can be used for variety of models from lightweight to large.\\nIt can be used to obtain complete information, so that train-from-scratch\\nmodels can achieve better results than state-of-the-art models pre-trained\\nusing large datasets, the comparison results are shown in Figure 1. The source\\ncodes are at: https://github.com/WongKinYiu/yolov9.\",\n",
       "  'topic_id': 1,\n",
       "  'topics': ['large language',\n",
       "   'memory',\n",
       "   'memory footprint',\n",
       "   'deep networks',\n",
       "   'language models',\n",
       "   'bitnet',\n",
       "   'resource constrained',\n",
       "   'attention',\n",
       "   'slm',\n",
       "   'slms']},\n",
       " {'url': 'https://paperswithcode.com/paper/datasets-for-large-language-models-a',\n",
       "  'title': 'Datasets for Large Language Models: A Comprehensive Survey',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.18041v1.pdf',\n",
       "  'published': '2024-02-28',\n",
       "  'authors': 'Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin',\n",
       "  'summary': 'This paper embarks on an exploration into the Large Language Model (LLM)\\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\\ndatasets serve as the foundational infrastructure analogous to a root system\\nthat sustains and nurtures the development of LLMs. Consequently, examination\\nof these datasets emerges as a critical topic in research. In order to address\\nthe current lack of a comprehensive overview and thorough analysis of LLM\\ndatasets, and to gain insights into their current status and future trends,\\nthis survey consolidates and categorizes the fundamental aspects of LLM\\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\\non the prevailing challenges and points out potential avenues for future\\ninvestigation. Additionally, a comprehensive review of the existing available\\ndataset resources is also provided, including statistics from 444 datasets,\\ncovering 8 language categories and spanning 32 domains. Information from 20\\ndimensions is incorporated into the dataset statistics. The total data size\\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\\nother datasets. We aim to present the entire landscape of LLM text datasets,\\nserving as a comprehensive reference for researchers in this field and\\ncontributing to future studies. Related resources are available at:\\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.',\n",
       "  'topic_id': 0,\n",
       "  'topics': ['language models',\n",
       "   'video generation',\n",
       "   'question answering',\n",
       "   'natural language',\n",
       "   'prompt engineering',\n",
       "   'large language',\n",
       "   'automatic prompt',\n",
       "   'pretrained latent',\n",
       "   'generate',\n",
       "   'pretrained']},\n",
       " {'url': 'https://paperswithcode.com/paper/learning-to-generate-instruction-tuning',\n",
       "  'title': 'Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.18334v1.pdf',\n",
       "  'published': '2024-02-28',\n",
       "  'authors': 'Nihal V. Nayak, Yiyang Nan, Avi Trost, Stephen H. Bach',\n",
       "  'summary': \"We introduce Bonito, an open-source model for conditional task generation:\\nthe task of converting unannotated text into task-specific training datasets\\nfor instruction tuning. Our goal is to enable zero-shot task adaptation of\\nlarge language models on users' specialized, private data. We train Bonito on a\\nnew large-scale dataset with 1.65M examples created by remixing existing\\ninstruction tuning datasets into meta-templates. The meta-templates for a\\ndataset produce training examples where the input is the unannotated text and\\nthe task attribute and the output consists of the instruction and the response.\\nWe use Bonito to generate synthetic tasks for seven datasets from specialized\\ndomains across three task types -- yes-no question answering, extractive\\nquestion answering, and natural language inference -- and adapt language\\nmodels. We show that Bonito significantly improves the average performance of\\npretrained and instruction tuned models over the de facto self supervised\\nbaseline. For example, adapting Mistral-Instruct-v2 and instruction tuned\\nvariants of Mistral and Llama2 with Bonito improves the strong zero-shot\\nperformance by 22.1 F1 points whereas the next word prediction objective undoes\\nsome of the benefits of instruction tuning and reduces the average performance\\nby 0.8 F1 points. We conduct additional experiments with Bonito to understand\\nthe effects of the domain, the size of the training set, and the choice of\\nalternative synthetic task generators. Overall, we show that learning with\\nsynthetic instruction tuning datasets is an effective way to adapt language\\nmodels to new domains. The model, dataset, and code are available at\\nhttps://github.com/BatsResearch/bonito.\",\n",
       "  'topic_id': 0,\n",
       "  'topics': ['language models',\n",
       "   'video generation',\n",
       "   'question answering',\n",
       "   'natural language',\n",
       "   'prompt engineering',\n",
       "   'large language',\n",
       "   'automatic prompt',\n",
       "   'pretrained latent',\n",
       "   'generate',\n",
       "   'pretrained']},\n",
       " {'url': 'https://paperswithcode.com/paper/mobillama-towards-accurate-and-lightweight',\n",
       "  'title': 'MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.16840v1.pdf',\n",
       "  'published': '2024-02-26',\n",
       "  'authors': 'Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz Khan',\n",
       "  'summary': '\"Bigger the better\" has been the predominant trend in recent Large Language\\nModels (LLMs) development. However, LLMs do not suit well for scenarios that\\nrequire on-device processing, energy efficiency, low memory footprint, and\\nresponse efficiency. These requisites are crucial for privacy, security, and\\nsustainable deployment. This paper explores the \"less is more\" paradigm by\\naddressing the challenge of designing accurate yet efficient Small Language\\nModels (SLMs) for resource constrained devices. Our primary contribution is the\\nintroduction of an accurate and fully transparent open-source 0.5 billion\\n(0.5B) parameter SLM, named MobiLlama, catering to the specific needs of\\nresource-constrained computing with an emphasis on enhanced performance with\\nreduced resource demands. MobiLlama is a SLM design that initiates from a\\nlarger model and applies a careful parameter sharing scheme to reduce both the\\npre-training and the deployment cost. Our work strives to not only bridge the\\ngap in open-source SLMs but also ensures full transparency, where complete\\ntraining data pipeline, training code, model weights, and over 300 checkpoints\\nalong with evaluation codes is available at :\\nhttps://github.com/mbzuai-oryx/MobiLlama.',\n",
       "  'topic_id': 1,\n",
       "  'topics': ['large language',\n",
       "   'memory',\n",
       "   'memory footprint',\n",
       "   'deep networks',\n",
       "   'language models',\n",
       "   'bitnet',\n",
       "   'resource constrained',\n",
       "   'attention',\n",
       "   'slm',\n",
       "   'slms']},\n",
       " {'url': 'https://paperswithcode.com/paper/training-free-long-context-scaling-of-large',\n",
       "  'title': 'Training-Free Long-Context Scaling of Large Language Models',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.17463v1.pdf',\n",
       "  'published': '2024-02-27',\n",
       "  'authors': 'Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, Lingpeng Kong',\n",
       "  'summary': 'The ability of Large Language Models (LLMs) to process and generate coherent\\ntext is markedly weakened when the number of input tokens exceeds their\\npretraining length. Given the expensive overhead of finetuning large-scale\\nmodels with longer sequences, we propose Dual Chunk Attention (DCA), which\\nenables Llama2 70B to support context windows of more than 100k tokens without\\ncontinual training. By decomposing the attention computation for long sequences\\ninto chunk-based modules, DCA manages to effectively capture the relative\\npositional information of tokens within the same chunk (Intra-Chunk) and across\\ndistinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash\\nAttention. In addition to its impressive extrapolation capability, DCA achieves\\nperformance on practical long-context tasks that is comparable to or even\\nbetter than that of finetuned models. When compared with proprietary models,\\nour training-free 70B model attains 94% of the performance of gpt-3.5-16k,\\nindicating it is a viable open-source alternative. All code and data used in\\nthis work are released at \\\\url{https://github.com/HKUNLP/ChunkLlama}.',\n",
       "  'topic_id': 1,\n",
       "  'topics': ['large language',\n",
       "   'memory',\n",
       "   'memory footprint',\n",
       "   'deep networks',\n",
       "   'language models',\n",
       "   'bitnet',\n",
       "   'resource constrained',\n",
       "   'attention',\n",
       "   'slm',\n",
       "   'slms']},\n",
       " {'url': 'https://paperswithcode.com/paper/bitnet-scaling-1-bit-transformers-for-large',\n",
       "  'title': 'BitNet: Scaling 1-bit Transformers for Large Language Models',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2310.11453v1.pdf',\n",
       "  'published': '2023-10-17',\n",
       "  'authors': 'Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei',\n",
       "  'summary': 'The increasing size of large language models has posed challenges for\\ndeployment and raised concerns about environmental impact due to high energy\\nconsumption. In this work, we introduce BitNet, a scalable and stable 1-bit\\nTransformer architecture designed for large language models. Specifically, we\\nintroduce BitLinear as a drop-in replacement of the nn.Linear layer in order to\\ntrain 1-bit weights from scratch. Experimental results on language modeling\\nshow that BitNet achieves competitive performance while substantially reducing\\nmemory footprint and energy consumption, compared to state-of-the-art 8-bit\\nquantization methods and FP16 Transformer baselines. Furthermore, BitNet\\nexhibits a scaling law akin to full-precision Transformers, suggesting its\\npotential for effective scaling to even larger language models while\\nmaintaining efficiency and performance benefits.',\n",
       "  'topic_id': 1,\n",
       "  'topics': ['large language',\n",
       "   'memory',\n",
       "   'memory footprint',\n",
       "   'deep networks',\n",
       "   'language models',\n",
       "   'bitnet',\n",
       "   'resource constrained',\n",
       "   'attention',\n",
       "   'slm',\n",
       "   'slms']},\n",
       " {'url': 'https://paperswithcode.com/paper/the-first-place-solution-of-wsdm-cup-2024',\n",
       "  'title': 'The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA',\n",
       "  'arxiv_link': 'https://arxiv.org/pdf/2402.18385v1.pdf',\n",
       "  'published': '2024-02-28',\n",
       "  'authors': 'Yiming Li, Zhao Zhang',\n",
       "  'summary': 'Conversational multi-doc question answering aims to answer specific questions\\nbased on the retrieved documents as well as the contextual conversations. In\\nthis paper, we introduce our winning approach for the \"Conversational Multi-Doc\\nQA\" challenge in WSDM Cup 2024, which exploits the superior natural language\\nunderstanding and generation capability of Large Language Models (LLMs). We\\nfirst adapt LLMs to the task, then devise a hybrid training strategy to make\\nthe most of in-domain unlabeled data. Moreover, an advanced text embedding\\nmodel is adopted to filter out potentially irrelevant documents and several\\napproaches are designed and compared for the model ensemble. Equipped with all\\nthese techniques, our solution finally ranked 1st place in WSDM Cup 2024,\\nsurpassing its rivals to a large extent. The source codes have been released at\\nhttps://github.com/zhangzhao219/WSDM-Cup-2024.',\n",
       "  'topic_id': 0,\n",
       "  'topics': ['language models',\n",
       "   'video generation',\n",
       "   'question answering',\n",
       "   'natural language',\n",
       "   'prompt engineering',\n",
       "   'large language',\n",
       "   'automatic prompt',\n",
       "   'pretrained latent',\n",
       "   'generate',\n",
       "   'pretrained']}]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# `topic_distr` contains the distribution of topics in each document\n",
    "topic_distr, _ = topic_model.approximate_distribution(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67922139, 0.32077861],\n",
       "       [0.57147287, 0.42852713],\n",
       "       [0.64964919, 0.35035081],\n",
       "       [0.29195086, 0.70804914],\n",
       "       [0.77898086, 0.22101914],\n",
       "       [0.63608148, 0.36391852],\n",
       "       [0.49602461, 0.50397539],\n",
       "       [0.41885281, 0.58114719],\n",
       "       [0.3826809 , 0.6173191 ],\n",
       "       [0.53250429, 0.46749571]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We introduce Bonito, an open-source model for conditional task generation:\n",
      "the task of converting unannotated text into task-specific training datasets\n",
      "for instruction tuning. Our goal is to enable zero-shot task adaptation of\n",
      "large language models on users' specialized, private data. We train Bonito on a\n",
      "new large-scale dataset with 1.65M examples created by remixing existing\n",
      "instruction tuning datasets into meta-templates. The meta-templates for a\n",
      "dataset produce training examples where the input is the unannotated text and\n",
      "the task attribute and the output consists of the instruction and the response.\n",
      "We use Bonito to generate synthetic tasks for seven datasets from specialized\n",
      "domains across three task types -- yes-no question answering, extractive\n",
      "question answering, and natural language inference -- and adapt language\n",
      "models. We show that Bonito significantly improves the average performance of\n",
      "pretrained and instruction tuned models over the de facto self supervised\n",
      "baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned\n",
      "variants of Mistral and Llama2 with Bonito improves the strong zero-shot\n",
      "performance by 22.1 F1 points whereas the next word prediction objective undoes\n",
      "some of the benefits of instruction tuning and reduces the average performance\n",
      "by 0.8 F1 points. We conduct additional experiments with Bonito to understand\n",
      "the effects of the domain, the size of the training set, and the choice of\n",
      "alternative synthetic task generators. Overall, we show that learning with\n",
      "synthetic instruction tuning datasets is an effective way to adapt language\n",
      "models to new domains. The model, dataset, and code are available at\n",
      "https://github.com/BatsResearch/bonito.\n"
     ]
    }
   ],
   "source": [
    "abstract_id = 5\n",
    "print(abstracts[abstract_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/IPython/core/formatters.py:925\u001b[0m, in \u001b[0;36mIPythonDisplayFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    923\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/plotly/basedatatypes.py:832\u001b[0m, in \u001b[0;36mBaseFigure._ipython_display_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mrenderers\u001b[38;5;241m.\u001b[39mrender_on_display \u001b[38;5;129;01mand\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mrenderers\u001b[38;5;241m.\u001b[39mdefault:\n\u001b[0;32m--> 832\u001b[0m     \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/plotly/io/_renderers.py:394\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    398\u001b[0m     ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#C8D2D7",
          "line": {
           "color": "#6E8484",
           "width": 1
          }
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.6360814791449138,
          0.3639185208550862
         ],
         "y": [
          "<b>Topic 0</b>: datasets_model_task_lang...",
          "<b>Topic 1</b>: models_information_large..."
         ]
        }
       ],
       "layout": {
        "height": 600,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Topic Probability Distribution</b>",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 800,
        "xaxis": {
         "title": {
          "text": "Probability"
         }
        }
       }
      },
      "text/html": [
       "<div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.29.1.min.js\"></script>                <div id=\"e8178ff0-2e7b-4605-a34b-a06ba3f8c2a2\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e8178ff0-2e7b-4605-a34b-a06ba3f8c2a2\")) {                    Plotly.newPlot(                        \"e8178ff0-2e7b-4605-a34b-a06ba3f8c2a2\",                        [{\"marker\":{\"color\":\"#C8D2D7\",\"line\":{\"color\":\"#6E8484\",\"width\":1}},\"orientation\":\"h\",\"x\":[0.6360814791449138,0.3639185208550862],\"y\":[\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e: datasets_model_task_lang...\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e: models_information_large...\"],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eTopic Probability Distribution\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"xaxis\":{\"title\":{\"text\":\"Probability\"}},\"width\":800,\"height\":600},                        {\"responsive\": true}                    )                };                            </script>        </div>"
      ],
      "text/plain": [
       "Figure({\n",
       "    'data': [{'marker': {'color': '#C8D2D7', 'line': {'color': '#6E8484', 'width': 1}},\n",
       "              'orientation': 'h',\n",
       "              'type': 'bar',\n",
       "              'x': [0.6360814791449138, 0.3639185208550862],\n",
       "              'y': [<b>Topic 0</b>: datasets_model_task_lang..., <b>Topic 1</b>:\n",
       "                    models_information_large...]}],\n",
       "    'layout': {'height': 600,\n",
       "               'hoverlabel': {'bgcolor': 'white', 'font': {'family': 'Rockwell', 'size': 16}},\n",
       "               'template': '...',\n",
       "               'title': {'font': {'color': 'Black', 'size': 22},\n",
       "                         'text': '<b>Topic Probability Distribution</b>',\n",
       "                         'x': 0.5,\n",
       "                         'xanchor': 'center',\n",
       "                         'y': 0.95,\n",
       "                         'yanchor': 'top'},\n",
       "               'width': 800,\n",
       "               'xaxis': {'title': {'text': 'Probability'}}}\n",
       "})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topic-document distribution for a single document\n",
    "topic_model.visualize_distribution(topic_distr[abstract_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/IPython/core/formatters.py:925\u001b[0m, in \u001b[0;36mIPythonDisplayFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    923\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/plotly/basedatatypes.py:832\u001b[0m, in \u001b[0;36mBaseFigure._ipython_display_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mrenderers\u001b[38;5;241m.\u001b[39mrender_on_display \u001b[38;5;129;01mand\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mrenderers\u001b[38;5;241m.\u001b[39mdefault:\n\u001b[0;32m--> 832\u001b[0m     \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/plotly/io/_renderers.py:394\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    398\u001b[0m     ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#C8D2D7",
          "line": {
           "color": "#6E8484",
           "width": 1
          }
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.6360814791449138,
          0.3639185208550862
         ],
         "y": [
          "Automatic Prompt Engineering",
          "Efficient Large Language Models"
         ]
        }
       ],
       "layout": {
        "height": 600,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Topic Probability Distribution</b>",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 800,
        "xaxis": {
         "title": {
          "text": "Probability"
         }
        }
       }
      },
      "text/html": [
       "<div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.29.1.min.js\"></script>                <div id=\"59ee92d1-6920-4005-acd6-6a778a98bfe8\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"59ee92d1-6920-4005-acd6-6a778a98bfe8\")) {                    Plotly.newPlot(                        \"59ee92d1-6920-4005-acd6-6a778a98bfe8\",                        [{\"marker\":{\"color\":\"#C8D2D7\",\"line\":{\"color\":\"#6E8484\",\"width\":1}},\"orientation\":\"h\",\"x\":[0.6360814791449138,0.3639185208550862],\"y\":[\"Automatic Prompt Engineering\",\"Efficient Large Language Models\"],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eTopic Probability Distribution\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"xaxis\":{\"title\":{\"text\":\"Probability\"}},\"width\":800,\"height\":600},                        {\"responsive\": true}                    )                };                            </script>        </div>"
      ],
      "text/plain": [
       "Figure({\n",
       "    'data': [{'marker': {'color': '#C8D2D7', 'line': {'color': '#6E8484', 'width': 1}},\n",
       "              'orientation': 'h',\n",
       "              'type': 'bar',\n",
       "              'x': [0.6360814791449138, 0.3639185208550862],\n",
       "              'y': [Automatic Prompt Engineering, Efficient Large Language Models]}],\n",
       "    'layout': {'height': 600,\n",
       "               'hoverlabel': {'bgcolor': 'white', 'font': {'family': 'Rockwell', 'size': 16}},\n",
       "               'template': '...',\n",
       "               'title': {'font': {'color': 'Black', 'size': 22},\n",
       "                         'text': '<b>Topic Probability Distribution</b>',\n",
       "                         'x': 0.5,\n",
       "                         'xanchor': 'center',\n",
       "                         'y': 0.95,\n",
       "                         'yanchor': 'top'},\n",
       "               'width': 800,\n",
       "               'xaxis': {'title': {'text': 'Probability'}}}\n",
       "})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topic-document distribution for a single document\n",
    "topic_model.visualize_distribution(topic_distr[abstract_id], custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# # Calculate the topic distributions on a token-level\n",
    "# topic_distr, topic_token_distr = topic_model.approximate_distribution(abstracts[abstract_id], calculate_tokens=True)\n",
    "\n",
    "# # Visualize the token-level distributions\n",
    "# df = topic_model.visualize_approximate_distribution(abstracts[abstract_id], topic_token_distr[0])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 60/60 [00:06<00:00,  9.78it/s]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.27s/it]\n"
     ]
    }
   ],
   "source": [
    "topic_distr, _ = topic_model.approximate_distribution(abstracts, use_embedding_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6648464 , 0.3351536 ],\n",
       "       [0.661491  , 0.33850902],\n",
       "       [0.63655037, 0.36344963],\n",
       "       [0.48688585, 0.5131142 ],\n",
       "       [0.5654748 , 0.43452513],\n",
       "       [0.58394575, 0.41605422],\n",
       "       [0.43747273, 0.56252724],\n",
       "       [0.46877176, 0.53122824],\n",
       "       [0.3860429 , 0.61395705],\n",
       "       [0.63068044, 0.36931956]], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/IPython/core/formatters.py:925\u001b[0m, in \u001b[0;36mIPythonDisplayFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    923\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/plotly/basedatatypes.py:832\u001b[0m, in \u001b[0;36mBaseFigure._ipython_display_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mrenderers\u001b[38;5;241m.\u001b[39mrender_on_display \u001b[38;5;129;01mand\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mrenderers\u001b[38;5;241m.\u001b[39mdefault:\n\u001b[0;32m--> 832\u001b[0m     \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/plotly/io/_renderers.py:394\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    398\u001b[0m     ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.0996209403156825,
          0.10174399031906442,
          0.11726174577864121,
          0.13226718741478377,
          0.16955208084578507
         ],
         "xaxis": "x",
         "y": [
          "transparent  ",
          "language  ",
          "task  ",
          "model  ",
          "datasets  "
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#0072B2"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.09050785232429104,
          0.09161898157372063,
          0.10690734822881563,
          0.1281531513769702,
          0.15255849415971884
         ],
         "xaxis": "x2",
         "y": [
          "language models  ",
          "language  ",
          "large  ",
          "information  ",
          "models  "
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 0",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 1",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 325,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "Topic Word Scores",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "showgrid": true
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "showgrid": true
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "showgrid": true
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          1
         ],
         "showgrid": true
        }
       }
      },
      "text/html": [
       "<div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.29.1.min.js\"></script>                <div id=\"2c35bd26-4326-49bb-b352-e5759785f569\" class=\"plotly-graph-div\" style=\"height:325.0px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2c35bd26-4326-49bb-b352-e5759785f569\")) {                    Plotly.newPlot(                        \"2c35bd26-4326-49bb-b352-e5759785f569\",                        [{\"marker\":{\"color\":\"#D55E00\"},\"orientation\":\"h\",\"x\":[0.0996209403156825,0.10174399031906442,0.11726174577864121,0.13226718741478377,0.16955208084578507],\"y\":[\"transparent  \",\"language  \",\"task  \",\"model  \",\"datasets  \"],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"#0072B2\"},\"orientation\":\"h\",\"x\":[0.09050785232429104,0.09161898157372063,0.10690734822881563,0.1281531513769702,0.15255849415971884],\"y\":[\"language models  \",\"language  \",\"large  \",\"information  \",\"models  \"],\"type\":\"bar\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"showgrid\":true},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"showgrid\":true},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,1.0],\"showgrid\":true},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,1.0],\"showgrid\":true},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 0\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 1\",\"x\":0.36250000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"Topic Word Scores\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"showlegend\":false,\"width\":1000,\"height\":325.0},                        {\"responsive\": true}                    )                };                            </script>        </div>"
      ],
      "text/plain": [
       "Figure({\n",
       "    'data': [{'marker': {'color': '#D55E00'},\n",
       "              'orientation': 'h',\n",
       "              'type': 'bar',\n",
       "              'x': [0.0996209403156825, 0.10174399031906442, 0.11726174577864121,\n",
       "                    0.13226718741478377, 0.16955208084578507],\n",
       "              'xaxis': 'x',\n",
       "              'y': [transparent  , language  , task  , model  , datasets  ],\n",
       "              'yaxis': 'y'},\n",
       "             {'marker': {'color': '#0072B2'},\n",
       "              'orientation': 'h',\n",
       "              'type': 'bar',\n",
       "              'x': [0.09050785232429104, 0.09161898157372063, 0.10690734822881563,\n",
       "                    0.1281531513769702, 0.15255849415971884],\n",
       "              'xaxis': 'x2',\n",
       "              'y': [language models  , language  , large  , information  , models\n",
       "                    ],\n",
       "              'yaxis': 'y2'}],\n",
       "    'layout': {'annotations': [{'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Topic 0',\n",
       "                                'x': 0.0875,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.0,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'},\n",
       "                               {'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Topic 1',\n",
       "                                'x': 0.36250000000000004,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.0,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'}],\n",
       "               'height': 325.0,\n",
       "               'hoverlabel': {'bgcolor': 'white', 'font': {'family': 'Rockwell', 'size': 16}},\n",
       "               'showlegend': False,\n",
       "               'template': '...',\n",
       "               'title': {'font': {'color': 'Black', 'size': 22},\n",
       "                         'text': 'Topic Word Scores',\n",
       "                         'x': 0.5,\n",
       "                         'xanchor': 'center',\n",
       "                         'yanchor': 'top'},\n",
       "               'width': 1000,\n",
       "               'xaxis': {'anchor': 'y', 'domain': [0.0, 0.175], 'showgrid': True},\n",
       "               'xaxis2': {'anchor': 'y2', 'domain': [0.275, 0.45], 'showgrid': True},\n",
       "               'xaxis3': {'anchor': 'y3', 'domain': [0.55, 0.7250000000000001], 'showgrid': True},\n",
       "               'xaxis4': {'anchor': 'y4', 'domain': [0.825, 1.0], 'showgrid': True},\n",
       "               'yaxis': {'anchor': 'x', 'domain': [0.0, 1.0], 'showgrid': True},\n",
       "               'yaxis2': {'anchor': 'x2', 'domain': [0.0, 1.0], 'showgrid': True},\n",
       "               'yaxis3': {'anchor': 'x3', 'domain': [0.0, 1.0], 'showgrid': True},\n",
       "               'yaxis4': {'anchor': 'x4', 'domain': [0.0, 1.0], 'showgrid': True}}\n",
       "})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reduce outliers\n",
    "# new_topics = topic_model.reduce_outliers(abstracts, topics)\n",
    "\n",
    "# # Reduce outliers with pre-calculate embeddings instead\n",
    "# new_topics = topic_model.reduce_outliers(abstracts, topics, strategy=\"embeddings\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Topics with Outlier Reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.update_topics(docs, topics=new_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.visualize_topics(custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.visualize_hierarchy(custom_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/IPython/core/formatters.py:925\u001b[0m, in \u001b[0;36mIPythonDisplayFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    923\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/plotly/basedatatypes.py:832\u001b[0m, in \u001b[0;36mBaseFigure._ipython_display_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mrenderers\u001b[38;5;241m.\u001b[39mrender_on_display \u001b[38;5;129;01mand\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mrenderers\u001b[38;5;241m.\u001b[39mdefault:\n\u001b[0;32m--> 832\u001b[0m     \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/SME_LLM/lib/python3.10/site-packages/plotly/io/_renderers.py:394\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    398\u001b[0m     ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "hovertext": [
          null
         ],
         "marker": {
          "color": "#CFD8DC",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "other",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          null
         ],
         "y": [
          null
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "Datasets for Large Language Models: A Comprehensive Survey",
          "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation",
          "Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases",
          "The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA",
          "Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models",
          "Transparent Image Layer Diffusion using Latent Transparency",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "Automatic Prompt Engineering",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "Automatic Prompt Engineering"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          7.450292,
          7.8840213,
          8.027403,
          7.806713,
          8.817042,
          9.373167,
          8.22644
         ],
         "y": [
          11.692401,
          10.66457,
          11.221424,
          12.234926,
          10.866113,
          11.261085,
          11.323421
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "BitNet: Scaling 1-bit Transformers for Large Language Models",
          "Training-Free Long-Context Scaling of Large Language Models",
          "MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT",
          "YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "Efficient Large Language Models",
         "text": [
          "",
          "",
          "",
          "",
          "Efficient Large Language Models"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          9.425789,
          8.484374,
          8.938329,
          9.740575,
          9.147266
         ],
         "y": [
          12.739389,
          11.733148,
          12.426593,
          12.11696,
          12.254023
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "D1",
          "x": 6.332748293876648,
          "y": 11.857591104507446,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "D2",
          "x": 8.767204678058624,
          "xshift": 10,
          "y": 14.650297832489013
         }
        ],
        "height": 750,
        "shapes": [
         {
          "line": {
           "color": "#CFD8DC",
           "width": 2
          },
          "type": "line",
          "x0": 8.767204678058624,
          "x1": 8.767204678058624,
          "y0": 9.064884376525878,
          "y1": 14.650297832489013
         },
         {
          "line": {
           "color": "#9E9E9E",
           "width": 2
          },
          "type": "line",
          "x0": 6.332748293876648,
          "x1": 11.2016610622406,
          "y0": 11.857591104507446,
          "y1": 11.857591104507446
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Documents and Topics</b>",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1200,
        "xaxis": {
         "visible": false
        },
        "yaxis": {
         "visible": false
        }
       }
      },
      "text/html": [
       "<div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.29.1.min.js\"></script>                <div id=\"1c07a410-9104-4c1b-8dff-8105b55f6978\" class=\"plotly-graph-div\" style=\"height:750px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1c07a410-9104-4c1b-8dff-8105b55f6978\")) {                    Plotly.newPlot(                        \"1c07a410-9104-4c1b-8dff-8105b55f6978\",                        [{\"hoverinfo\":\"text\",\"hovertext\":[null],\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[null],\"y\":[null],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Datasets for Large Language Models: A Comprehensive Survey\",\"Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation\",\"Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases\",\"The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA\",\"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models\",\"Transparent Image Layer Diffusion using Latent Transparency\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"Automatic Prompt Engineering\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"Automatic Prompt Engineering\"],\"textfont\":{\"size\":12},\"x\":[7.450292,7.8840213,8.027403,7.806713,8.817042,9.373167,8.22644],\"y\":[11.692401,10.66457,11.221424,12.234926,10.866113,11.261085,11.323421],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"BitNet: Scaling 1-bit Transformers for Large Language Models\",\"Training-Free Long-Context Scaling of Large Language Models\",\"MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT\",\"YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"Efficient Large Language Models\",\"text\":[\"\",\"\",\"\",\"\",\"Efficient Large Language Models\"],\"textfont\":{\"size\":12},\"x\":[9.425789,8.484374,8.938329,9.740575,9.147266],\"y\":[12.739389,11.733148,12.426593,12.11696,12.254023],\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"shapes\":[{\"line\":{\"color\":\"#CFD8DC\",\"width\":2},\"type\":\"line\",\"x0\":8.767204678058624,\"x1\":8.767204678058624,\"y0\":9.064884376525878,\"y1\":14.650297832489013},{\"line\":{\"color\":\"#9E9E9E\",\"width\":2},\"type\":\"line\",\"x0\":6.332748293876648,\"x1\":11.2016610622406,\"y0\":11.857591104507446,\"y1\":11.857591104507446}],\"annotations\":[{\"showarrow\":false,\"text\":\"D1\",\"x\":6.332748293876648,\"y\":11.857591104507446,\"yshift\":10},{\"showarrow\":false,\"text\":\"D2\",\"x\":8.767204678058624,\"xshift\":10,\"y\":14.650297832489013}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eDocuments and Topics\\u003c\\u002fb\\u003e\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1200,\"height\":750,\"xaxis\":{\"visible\":false},\"yaxis\":{\"visible\":false}},                        {\"responsive\": true}                    )                };                            </script>        </div>"
      ],
      "text/plain": [
       "Figure({\n",
       "    'data': [{'hoverinfo': 'text',\n",
       "              'hovertext': array([None], dtype=object),\n",
       "              'marker': {'color': '#CFD8DC', 'opacity': 0.5, 'size': 5},\n",
       "              'mode': 'markers+text',\n",
       "              'name': 'other',\n",
       "              'showlegend': False,\n",
       "              'type': 'scattergl',\n",
       "              'x': array([nan], dtype=float32),\n",
       "              'y': array([nan], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'hovertext': array(['Datasets for Large Language Models: A Comprehensive Survey',\n",
       "                                  'Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation',\n",
       "                                  'Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases',\n",
       "                                  'The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA',\n",
       "                                  'Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models',\n",
       "                                  'Transparent Image Layer Diffusion using Latent Transparency', None],\n",
       "                                 dtype=object),\n",
       "              'marker': {'opacity': 0.5, 'size': 5},\n",
       "              'mode': 'markers+text',\n",
       "              'name': 'Automatic Prompt Engineering',\n",
       "              'text': array(['', '', '', '', '', '', 'Automatic Prompt Engineering'], dtype=object),\n",
       "              'textfont': {'size': 12},\n",
       "              'type': 'scattergl',\n",
       "              'x': array([7.450292 , 7.8840213, 8.027403 , 7.806713 , 8.817042 , 9.373167 ,\n",
       "                          8.22644  ], dtype=float32),\n",
       "              'y': array([11.692401, 10.66457 , 11.221424, 12.234926, 10.866113, 11.261085,\n",
       "                          11.323421], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'hovertext': array(['BitNet: Scaling 1-bit Transformers for Large Language Models',\n",
       "                                  'Training-Free Long-Context Scaling of Large Language Models',\n",
       "                                  'MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT',\n",
       "                                  'YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information',\n",
       "                                  None], dtype=object),\n",
       "              'marker': {'opacity': 0.5, 'size': 5},\n",
       "              'mode': 'markers+text',\n",
       "              'name': 'Efficient Large Language Models',\n",
       "              'text': array(['', '', '', '', 'Efficient Large Language Models'], dtype=object),\n",
       "              'textfont': {'size': 12},\n",
       "              'type': 'scattergl',\n",
       "              'x': array([9.425789, 8.484374, 8.938329, 9.740575, 9.147266], dtype=float32),\n",
       "              'y': array([12.739389, 11.733148, 12.426593, 12.11696 , 12.254023], dtype=float32)}],\n",
       "    'layout': {'annotations': [{'showarrow': False,\n",
       "                                'text': 'D1',\n",
       "                                'x': 6.332748293876648,\n",
       "                                'y': 11.857591104507446,\n",
       "                                'yshift': 10},\n",
       "                               {'showarrow': False,\n",
       "                                'text': 'D2',\n",
       "                                'x': 8.767204678058624,\n",
       "                                'xshift': 10,\n",
       "                                'y': 14.650297832489013}],\n",
       "               'height': 750,\n",
       "               'shapes': [{'line': {'color': '#CFD8DC', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': 8.767204678058624,\n",
       "                           'x1': 8.767204678058624,\n",
       "                           'y0': 9.064884376525878,\n",
       "                           'y1': 14.650297832489013},\n",
       "                          {'line': {'color': '#9E9E9E', 'width': 2},\n",
       "                           'type': 'line',\n",
       "                           'x0': 6.332748293876648,\n",
       "                           'x1': 11.2016610622406,\n",
       "                           'y0': 11.857591104507446,\n",
       "                           'y1': 11.857591104507446}],\n",
       "               'template': '...',\n",
       "               'title': {'font': {'color': 'Black', 'size': 22},\n",
       "                         'text': '<b>Documents and Topics</b>',\n",
       "                         'x': 0.5,\n",
       "                         'xanchor': 'center',\n",
       "                         'yanchor': 'top'},\n",
       "               'width': 1200,\n",
       "               'xaxis': {'visible': False},\n",
       "               'yaxis': {'visible': False}}\n",
       "})"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the documents in 2-dimensional space and show the titles on hover instead of the abstracts\n",
    "# NOTE: You can hide the hover with `hide_document_hover=True` which is especially helpful if you have a large dataset\n",
    "topic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can also hide the annotation to have a more clear overview of the topics\n",
    "# topic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, custom_labels=True, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# topic_model.save(\"my_model_dir\", serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # Define embedding model\n",
    "# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Load model and add embedding model\n",
    "# loaded_model = BERTopic.load(\"path/to/my/model_dir\", embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"new_model\")\n",
    "\n",
    "# model = BERTopic.load(\"new_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the inference, we can leverage a \"best practice\" that we used before, namely serialization. When you save a model as safetensors and then load it in, we are removing the dimensionality reduction and clustering steps from the pipeline.\n",
    "\n",
    "Instead, the assignment of topics is done through cosine similarity of document embeddings and topic embeddings. This speeds up inferences significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SME_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
