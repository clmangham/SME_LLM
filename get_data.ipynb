{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL you want to scrape\n",
    "url = \"https://paperswithcode.com/\"\n",
    "\n",
    "# Make an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Now you can use Beautiful Soup methods to find data in the soup object\n",
    "    # For example, to find all 'a' tags (hyperlinks) in the document:\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    pattern = r\"\\/paper\\/(?!.*(#code|#tasks)$).*$\"\n",
    "    unique_papers = []\n",
    "\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "        # Check if the href attribute is a string and if it matches the pattern\n",
    "        if isinstance(href, str) and re.search(pattern, href):\n",
    "            # If the link is not already in the list, add it\n",
    "            if href not in unique_papers:\n",
    "                unique_papers.append(href)\n",
    "\n",
    "    # Now, unique_papers contains all unique links that include \"/paper/\"\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage: HTTP {response.status_code}\")\n",
    "\n",
    "\n",
    "paper_urls = []\n",
    "for paper in unique_papers:\n",
    "    paper_urls.append(\"https://paperswithcode.com\" + paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using',\n",
       " 'https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers',\n",
       " 'https://paperswithcode.com/paper/neural-network-diffusion',\n",
       " 'https://paperswithcode.com/paper/intent-based-prompt-calibration-enhancing',\n",
       " 'https://paperswithcode.com/paper/gaussianobject-just-taking-four-images-to-get',\n",
       " 'https://paperswithcode.com/paper/ufo-a-ui-focused-agent-for-windows-os',\n",
       " 'https://paperswithcode.com/paper/cleaner-pretraining-corpus-curation-with',\n",
       " 'https://paperswithcode.com/paper/fit-flexible-vision-transformer-for-diffusion',\n",
       " 'https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object',\n",
       " 'https://paperswithcode.com/paper/revisiting-feature-prediction-for-learning']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get each paper from arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_link(url):\n",
    "    # URL you want to scrape\n",
    "\n",
    "    # Make an HTTP GET request to the URL\n",
    "\n",
    "    pdf_link = None\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using Beautiful Soup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Define a function that checks if an 'a' tag's 'href' attribute contains the arXiv PDF URL\n",
    "        def is_arxiv_pdf(tag):\n",
    "            return tag.name == 'a' and tag.get('href') and 'https://arxiv.org/pdf' in tag.get('href')\n",
    "\n",
    "        # Use the function to filter 'a' tags\n",
    "        links = soup.find_all(is_arxiv_pdf)\n",
    "\n",
    "        # Print the 'href' attribute of each link\n",
    "        for link in links:\n",
    "            pdf_link = link.get('href')\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage: HTTP {response.status_code}, {url}\")\n",
    "        \n",
    "\n",
    "\n",
    "    return pdf_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/pdf/2402.13616v1.pdf'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using\"\n",
    "get_paper_link(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = WebBaseLoader(link.get('href'))\n",
    "# data = loader.load()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "# loader = UnstructuredPDFLoader(link.get('href'))\n",
    "# data = loader.load()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL you want to scrape\n",
    "url = \"https://paperswithcode.com/\"\n",
    "\n",
    "# Make an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Now you can use Beautiful Soup methods to find data in the soup object\n",
    "    # For example, to find all 'a' tags (hyperlinks) in the document:\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    pattern = r\"\\/paper\\/(?!.*(#code|#tasks)$).*$\"\n",
    "    unique_papers = []\n",
    "\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "        # Check if the href attribute is a string and if it matches the pattern\n",
    "        if isinstance(href, str) and re.search(pattern, href):\n",
    "            # If the link is not already in the list, add it\n",
    "            if href not in unique_papers:\n",
    "                unique_papers.append(href)\n",
    "\n",
    "    # Now, unique_papers contains all unique links that include \"/paper/\"\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage: HTTP {response.status_code}\")\n",
    "\n",
    "\n",
    "paper_urls = []\n",
    "for paper in unique_papers:\n",
    "    paper_urls.append(\"https://paperswithcode.com\" + paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using',\n",
       " 'https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers',\n",
       " 'https://paperswithcode.com/paper/neural-network-diffusion',\n",
       " 'https://paperswithcode.com/paper/intent-based-prompt-calibration-enhancing',\n",
       " 'https://paperswithcode.com/paper/gaussianobject-just-taking-four-images-to-get',\n",
       " 'https://paperswithcode.com/paper/ufo-a-ui-focused-agent-for-windows-os',\n",
       " 'https://paperswithcode.com/paper/cleaner-pretraining-corpus-curation-with',\n",
       " 'https://paperswithcode.com/paper/fit-flexible-vision-transformer-for-diffusion',\n",
       " 'https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object',\n",
       " 'https://paperswithcode.com/paper/revisiting-feature-prediction-for-learning']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/pdf/2402.13616v1.pdf',\n",
       " 'https://arxiv.org/pdf/2212.09748v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.13144v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.03099v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.10259v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.07939v3.pdf',\n",
       " 'https://arxiv.org/pdf/2402.14652v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.12376v1.pdf',\n",
       " 'https://arxiv.org/pdf/2401.17270v3.pdf']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/2402.13616v1.pdf\n",
      "https://arxiv.org/pdf/2212.09748v2.pdf\n",
      "https://arxiv.org/pdf/2402.13144v1.pdf\n",
      "https://arxiv.org/pdf/2402.03099v1.pdf\n",
      "https://arxiv.org/pdf/2402.10259v2.pdf\n",
      "https://arxiv.org/pdf/2402.07939v3.pdf\n",
      "https://arxiv.org/pdf/2402.14652v1.pdf\n",
      "https://arxiv.org/pdf/2402.12376v1.pdf\n",
      "https://arxiv.org/pdf/2401.17270v3.pdf\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/pdf/2402.13616v1.pdf',\n",
       " 'https://arxiv.org/pdf/2212.09748v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.13144v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.03099v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.10259v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.07939v3.pdf',\n",
       " 'https://arxiv.org/pdf/2402.14652v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.12376v1.pdf',\n",
       " 'https://arxiv.org/pdf/2401.17270v3.pdf',\n",
       " None]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = []\n",
    "for url in paper_urls:\n",
    "    # print(get_paper_link(url))\n",
    "    links.append(get_paper_link(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/pdf/2402.13616v1.pdf',\n",
       " 'https://arxiv.org/pdf/2212.09748v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.13144v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.03099v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.10259v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.07939v3.pdf',\n",
       " 'https://arxiv.org/pdf/2402.14652v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.12376v1.pdf',\n",
       " 'https://arxiv.org/pdf/2401.17270v3.pdf',\n",
       " None]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/pdf/2402.13616v1.pdf'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(links[0])\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='YOLOv9: Learning What You Want to Learn\\nUsing Programmable Gradient Information\\nChien-Yao Wang1,2, I-Hau Yeh2, and Hong-Yuan Mark Liao1,2,3\\n1Institute of Information Science, Academia Sinica, Taiwan\\n2National Taipei University of Technology, Taiwan\\n3Department of Information and Computer Engineering, Chung Yuan Christian University, Taiwan\\nkinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, and liao@iis.sinica.edu.tw\\nAbstract\\nToday’s deep learning methods focus on how to design\\nthe most appropriate objective functions so that the pre-\\ndiction results of the model can be closest to the ground\\ntruth. Meanwhile, an appropriate architecture that can\\nfacilitate acquisition of enough information for prediction\\nhas to be designed. Existing methods ignore a fact that\\nwhen input data undergoes layer-by-layer feature extrac-\\ntion and spatial transformation, large amount of informa-\\ntion will be lost. This paper will delve into the important is-\\nsues of data loss when data is transmitted through deep net-\\nworks, namely information bottleneck and reversible func-\\ntions. We proposed the concept of programmable gradi-\\nent information (PGI) to cope with the various changes\\nrequired by deep networks to achieve multiple objectives.\\nPGI can provide complete input information for the tar-\\nget task to calculate objective function, so that reliable\\ngradient information can be obtained to update network\\nweights. In addition, a new lightweight network architec-\\nture – Generalized Efficient Layer Aggregation Network\\n(GELAN), based on gradient path planning is designed.\\nGELAN’s architecture confirms that PGI has gained su-\\nperior results on lightweight models. We verified the pro-\\nposed GELAN and PGI on MS COCO dataset based ob-\\nject detection. The results show that GELAN only uses\\nconventional convolution operators to achieve better pa-\\nrameter utilization than the state-of-the-art methods devel-\\noped based on depth-wise convolution. PGI can be used\\nfor variety of models from lightweight to large. It can be\\nused to obtain complete information, so that train-from-\\nscratch models can achieve better results than state-of-the-\\nart models pre-trained using large datasets, the compari-\\nson results are shown in Figure 1. The source codes are at:\\nhttps://github.com/WongKinYiu/yolov9 .\\n1. Introduction\\nDeep learning-based models have demonstrated far bet-\\nter performance than past artificial intelligence systems in\\nvarious fields, such as computer vision, language process-\\ning, and speech recognition. In recent years, researchers\\nFigure 1. Comparisons of the real-time object detecors on MS\\nCOCO dataset. The GELAN and PGI-based object detection\\nmethod surpassed all previous train-from-scratch methods in terms\\nof object detection performance. In terms of accuracy, the new\\nmethod outperforms RT DETR [43] pre-trained with a large\\ndataset, and it also outperforms depth-wise convolution-based de-\\nsign YOLO MS [7] in terms of parameters utilization.\\nin the field of deep learning have mainly focused on how\\nto develop more powerful system architectures and learn-\\ning methods, such as CNNs [21–23, 42, 55, 71, 72], Trans-\\nformers [8, 9, 40, 41, 60, 69, 70], Perceivers [26, 26, 32, 52,\\n56, 81, 81], and Mambas [17, 38, 80]. In addition, some\\nresearchers have tried to develop more general objective\\nfunctions, such as loss function [5, 45, 46, 50, 77, 78], la-\\nbel assignment [10, 12, 33, 67, 79] and auxiliary supervi-\\nsion [18, 20, 24, 28, 29, 51, 54, 68, 76]. The above studies\\nall try to precisely find the mapping between input and tar-\\nget tasks. However, most past approaches have ignored that\\ninput data may have a non-negligible amount of informa-\\ntion loss during the feedforward process. This loss of in-\\nformation can lead to biased gradient flows, which are sub-\\nsequently used to update the model. The above problems\\ncan result in deep networks to establish incorrect associa-\\ntions between targets and inputs, causing the trained model\\nto produce incorrect predictions.', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 0}),\n",
       " Document(page_content='can result in deep networks to establish incorrect associa-\\ntions between targets and inputs, causing the trained model\\nto produce incorrect predictions.\\n1arXiv:2402.13616v1  [cs.CV]  21 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 0}),\n",
       " Document(page_content='Figure 2. Visualization results of random initial weight output feature maps for different network architectures: (a) input image, (b)\\nPlainNet, (c) ResNet, (d) CSPNet, and (e) proposed GELAN. From the figure, we can see that in different architectures, the information\\nprovided to the objective function to calculate the loss is lost to varying degrees, and our architecture can retain the most complete\\ninformation and provide the most reliable gradient information for calculating the objective function.\\nIn deep networks, the phenomenon of input data losing\\ninformation during the feedforward process is commonly\\nknown as information bottleneck [59], and its schematic di-\\nagram is as shown in Figure 2. At present, the main meth-\\nods that can alleviate this phenomenon are as follows: (1)\\nThe use of reversible architectures [3, 16, 19]: this method\\nmainly uses repeated input data and maintains the informa-\\ntion of the input data in an explicit way; (2) The use of\\nmasked modeling [1, 6, 9, 27, 71, 73]: it mainly uses recon-\\nstruction loss and adopts an implicit way to maximize the\\nextracted features and retain the input information; and (3)\\nIntroduction of the deep supervision concept [28,51,54,68]:\\nit uses shallow features that have not lost too much impor-\\ntant information to pre-establish a mapping from features\\nto targets to ensure that important information can be trans-\\nferred to deeper layers. However, the above methods have\\ndifferent drawbacks in the training process and inference\\nprocess. For example, a reversible architecture requires ad-\\nditional layers to combine repeatedly fed input data, which\\nwill significantly increase the inference cost. In addition,\\nsince the input data layer to the output layer cannot have a\\ntoo deep path, this limitation will make it difficult to model\\nhigh-order semantic information during the training pro-\\ncess. As for masked modeling, its reconstruction loss some-\\ntimes conflicts with the target loss. In addition, most mask\\nmechanisms also produce incorrect associations with data.\\nFor the deep supervision mechanism, it will produce error\\naccumulation, and if the shallow supervision loses informa-\\ntion during the training process, the subsequent layers will\\nnot be able to retrieve the required information. The above\\nphenomenon will be more significant on difficult tasks and\\nsmall models.\\nTo address the above-mentioned issues, we propose a\\nnew concept, which is programmable gradient information\\n(PGI). The concept is to generate reliable gradients through\\nauxiliary reversible branch, so that the deep features can\\nstill maintain key characteristics for executing target task.\\nThe design of auxiliary reversible branch can avoid the se-\\nmantic loss that may be caused by a traditional deep super-\\nvision process that integrates multi-path features. In other\\nwords, we are programming gradient information propaga-\\ntion at different semantic levels, and thereby achieving the\\nbest training results. The reversible architecture of PGI isbuilt on auxiliary branch, so there is no additional cost.\\nSince PGI can freely select loss function suitable for the\\ntarget task, it also overcomes the problems encountered by\\nmask modeling. The proposed PGI mechanism can be ap-\\nplied to deep neural networks of various sizes and is more\\ngeneral than the deep supervision mechanism, which is only\\nsuitable for very deep neural networks.\\nIn this paper, we also designed generalized ELAN\\n(GELAN) based on ELAN [65], the design of GELAN si-\\nmultaneously takes into account the number of parameters,\\ncomputational complexity, accuracy and inference speed.\\nThis design allows users to arbitrarily choose appropriate\\ncomputational blocks for different inference devices. We\\ncombined the proposed PGI and GELAN, and then de-\\nsigned a new generation of YOLO series object detection\\nsystem, which we call YOLOv9. We used the MS COCO\\ndataset to conduct experiments, and the experimental results', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 1}),\n",
       " Document(page_content='signed a new generation of YOLO series object detection\\nsystem, which we call YOLOv9. We used the MS COCO\\ndataset to conduct experiments, and the experimental results\\nverified that our proposed YOLOv9 achieved the top perfor-\\nmance in all comparisons.\\nWe summarize the contributions of this paper as follows:\\n1. We theoretically analyzed the existing deep neural net-\\nwork architecture from the perspective of reversible\\nfunction, and through this process we successfully ex-\\nplained many phenomena that were difficult to explain\\nin the past. We also designed PGI and auxiliary re-\\nversible branch based on this analysis and achieved ex-\\ncellent results.\\n2. The PGI we designed solves the problem that deep su-\\npervision can only be used for extremely deep neu-\\nral network architectures, and therefore allows new\\nlightweight architectures to be truly applied in daily\\nlife.\\n3. The GELAN we designed only uses conventional con-\\nvolution to achieve a higher parameter usage than the\\ndepth-wise convolution design that based on the most\\nadvanced technology, while showing great advantages\\nof being light, fast, and accurate.\\n4. Combining the proposed PGI and GELAN, the object\\ndetection performance of the YOLOv9 on MS COCO\\ndataset greatly surpasses the existing real-time object\\ndetectors in all aspects.\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 1}),\n",
       " Document(page_content='2. Related work\\n2.1. Real-time Object Detectors\\nThe current mainstream real-time object detectors are the\\nYOLO series [2, 7, 13–15, 25, 30, 31, 47–49, 61–63, 74, 75],\\nand most of these models use CSPNet [64] or ELAN [65]\\nand their variants as the main computing units. In terms of\\nfeature integration, improved PAN [37] or FPN [35] is of-\\nten used as a tool, and then improved YOLOv3 head [49] or\\nFCOS head [57, 58] is used as prediction head. Recently\\nsome real-time object detectors, such as RT DETR [43],\\nwhich puts its fundation on DETR [4], have also been pro-\\nposed. However, since it is extremely difficult for DETR\\nseries object detector to be applied to new domains without\\na corresponding domain pre-trained model, the most widely\\nused real-time object detector at present is still YOLO se-\\nries. This paper chooses YOLOv7 [63], which has been\\nproven effective in a variety of computer vision tasks and\\nvarious scenarios, as a base to develop the proposed method.\\nWe use GELAN to improve the architecture and the training\\nprocess with the proposed PGI. The above novel approach\\nmakes the proposed YOLOv9 the top real-time object de-\\ntector of the new generation.\\n2.2. Reversible Architectures\\nThe operation unit of reversible architectures [3, 16, 19]\\nmust maintain the characteristics of reversible conversion,\\nso it can be ensured that the output feature map of each\\nlayer of operation unit can retain complete original informa-\\ntion. Before, RevCol [3] generalizes traditional reversible\\nunit to multiple levels, and in doing so can expand the se-\\nmantic levels expressed by different layer units. Through\\na literature review of various neural network architectures,\\nwe found that there are many high-performing architectures\\nwith varying degree of reversible properties. For exam-\\nple, Res2Net module [11] combines different input parti-\\ntions with the next partition in a hierarchical manner, and\\nconcatenates all converted partitions before passing them\\nbackwards. CBNet [34, 39] re-introduces the original in-\\nput data through composite backbone to obtain complete\\noriginal information, and obtains different levels of multi-\\nlevel reversible information through various composition\\nmethods. These network architectures generally have ex-\\ncellent parameter utilization, but the extra composite layers\\ncause slow inference speeds. DynamicDet [36] combines\\nCBNet [34] and the high-efficiency real-time object detec-\\ntor YOLOv7 [63] to achieve a very good trade-off among\\nspeed, number of parameters, and accuracy. This paper in-\\ntroduces the DynamicDet architecture as the basis for de-\\nsigning reversible branches. In addition, reversible infor-\\nmation is further introduced into the proposed PGI. The\\nproposed new architecture does not require additional con-\\nnections during the inference process, so it can fully retain\\nthe advantages of speed, parameter amount, and accuracy.2.3. Auxiliary Supervision\\nDeep supervision [28,54,68] is the most common auxil-\\niary supervision method, which performs training by insert-\\ning additional prediction layers in the middle layers. Es-\\npecially the application of multi-layer decoders introduced\\nin the transformer-based methods is the most common one.\\nAnother common auxiliary supervision method is to utilize\\nthe relevant meta information to guide the feature maps pro-\\nduced by the intermediate layers and make them have the\\nproperties required by the target tasks [18, 20, 24, 29, 76].\\nExamples of this type include using segmentation loss or\\ndepth loss to enhance the accuracy of object detectors. Re-\\ncently, there are many reports in the literature [53, 67, 82]\\nthat use different label assignment methods to generate dif-\\nferent auxiliary supervision mechanisms to speed up the\\nconvergence speed of the model and improve the robustness\\nat the same time. However, the auxiliary supervision mech-\\nanism is usually only applicable to large models, so when\\nit is applied to lightweight models, it is easy to cause an', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 2}),\n",
       " Document(page_content='at the same time. However, the auxiliary supervision mech-\\nanism is usually only applicable to large models, so when\\nit is applied to lightweight models, it is easy to cause an\\nunder parameterization phenomenon, which makes the per-\\nformance worse. The PGI we proposed designed a way to\\nreprogram multi-level semantic information, and this design\\nallows lightweight models to also benefit from the auxiliary\\nsupervision mechanism.\\n3. Problem Statement\\nUsually, people attribute the difficulty of deep neural net-\\nwork convergence problem due to factors such as gradient\\nvanish or gradient saturation, and these phenomena do ex-\\nist in traditional deep neural networks. However, modern\\ndeep neural networks have already fundamentally solved\\nthe above problem by designing various normalization and\\nactivation functions. Nevertheless, deep neural networks\\nstill have the problem of slow convergence or poor conver-\\ngence results.\\nIn this paper, we explore the nature of the above issue\\nfurther. Through in-depth analysis of information bottle-\\nneck, we deduced that the root cause of this problem is that\\nthe initial gradient originally coming from a very deep net-\\nwork has lost a lot of information needed to achieve the\\ngoal soon after it is transmitted. In order to confirm this\\ninference, we feedforward deep networks of different archi-\\ntectures with initial weights, and then visualize and illus-\\ntrate them in Figure 2. Obviously, PlainNet has lost a lot of\\nimportant information required for object detection in deep\\nlayers. As for the proportion of important information that\\nResNet, CSPNet, and GELAN can retain, it is indeed posi-\\ntively related to the accuracy that can be obtained after train-\\ning. We further design reversible network-based methods to\\nsolve the causes of the above problems. In this section we\\nshall elaborate our analysis of information bottleneck prin-\\nciple and reversible functions.\\n3', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 2}),\n",
       " Document(page_content='3.1. Information Bottleneck Principle\\nAccording to information bottleneck principle, we know\\nthat data Xmay cause information loss when going through\\ntransformation, as shown in Eq. 1 below:\\nI(X, X )≥I(X, f θ(X))≥I(X, gϕ(fθ(X))),(1)\\nwhere Iindicates mutual information, fandgare transfor-\\nmation functions, and θandϕare parameters of fandg,\\nrespectively.\\nIn deep neural networks, fθ(·)andgϕ(·)respectively\\nrepresent the operations of two consecutive layers in deep\\nneural network. From Eq. 1, we can predict that as the num-\\nber of network layer becomes deeper, the original data will\\nbe more likely to be lost. However, the parameters of the\\ndeep neural network are based on the output of the network\\nas well as the given target, and then update the network after\\ngenerating new gradients by calculating the loss function.\\nAs one can imagine, the output of a deeper neural network\\nis less able to retain complete information about the pre-\\ndiction target. This will make it possible to use incomplete\\ninformation during network training, resulting in unreliable\\ngradients and poor convergence.\\nOne way to solve the above problem is to directly in-\\ncrease the size of the model. When we use a large number\\nof parameters to construct a model, it is more capable of\\nperforming a more complete transformation of the data. The\\nabove approach allows even if information is lost during the\\ndata feedforward process, there is still a chance to retain\\nenough information to perform the mapping to the target.\\nThe above phenomenon explains why the width is more im-\\nportant than the depth in most modern models. However,\\nthe above conclusion cannot fundamentally solve the prob-\\nlem of unreliable gradients in very deep neural network.\\nBelow, we will introduce how to use reversible functions\\nto solve problems and conduct relative analysis.\\n3.2. Reversible Functions\\nWhen a function rhas an inverse transformation func-\\ntionv, we call this function reversible function, as shown in\\nEq. 2.\\nX=vζ(rψ(X)), (2)\\nwhere ψandζare parameters of randv, respectively. Data\\nXis converted by reversible function without losing infor-\\nmation, as shown in Eq. 3.\\nI(X, X ) =I(X, rψ(X)) =I(X, v ζ(rψ(X))).(3)\\nWhen the network’s transformation function is composed\\nof reversible functions, more reliable gradients can be ob-\\ntained to update the model. Almost all of today’s populardeep learning methods are architectures that conform to the\\nreversible property, such as Eq. 4.\\nXl+1=Xl+fl+1\\nθ(Xl), (4)\\nwhere lindicates the l-th layer of a PreAct ResNet and\\nfis the transformation function of the l-th layer. PreAct\\nResNet [22] repeatedly passes the original data Xto sub-\\nsequent layers in an explicit way. Although such a design\\ncan make a deep neural network with more than a thousand\\nlayers converge very well, it destroys an important reason\\nwhy we need deep neural networks. That is, for difficult\\nproblems, it is difficult for us to directly find simple map-\\nping functions to map data to targets. This also explains\\nwhy PreAct ResNet performs worse than ResNet [21] when\\nthe number of layers is small.\\nIn addition, we tried to use masked modeling that al-\\nlowed the transformer model to achieve significant break-\\nthroughs. We use approximation methods, such as Eq. 5,\\nto try to find the inverse transformation vofr, so that the\\ntransformed features can retain enough information using\\nsparse features. The form of Eq. 5 is as follows:\\nX=vζ(rψ(X)·M), (5)\\nwhere Mis a dynamic binary mask. Other methods that\\nare commonly used to perform the above tasks are diffusion\\nmodel and variational autoencoder, and they both have the\\nfunction of finding the inverse function. However, when\\nwe apply the above approach to a lightweight model, there\\nwill be defects because the lightweight model will be under\\nparameterized to a large amount of raw data. Because of\\nthe above reason, important information I(Y, X)that maps\\ndataXto target Ywill also face the same problem. For this', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 3}),\n",
       " Document(page_content='parameterized to a large amount of raw data. Because of\\nthe above reason, important information I(Y, X)that maps\\ndataXto target Ywill also face the same problem. For this\\nissue, we will explore it using the concept of information\\nbottleneck [59]. The formula for information bottleneck is\\nas follows:\\nI(X, X )≥I(Y, X)≥I(Y, fθ(X))≥...≥I(Y,ˆY).(6)\\nGenerally speaking, I(Y, X)will only occupy a very small\\npart of I(X, X ). However, it is critical to the target mis-\\nsion. Therefore, even if the amount of information lost in\\nthe feedforward stage is not significant, as long as I(Y, X)\\nis covered, the training effect will be greatly affected. The\\nlightweight model itself is in an under parameterized state,\\nso it is easy to lose a lot of important information in the\\nfeedforward stage. Therefore, our goal for the lightweight\\nmodel is how to accurately filter I(Y, X)fromI(X, X ). As\\nfor fully preserving the information of X, that is difficult to\\nachieve. Based on the above analysis, we hope to propose a\\nnew deep neural network training method that can not only\\ngenerate reliable gradients to update the model, but also be\\nsuitable for shallow and lightweight neural networks.\\n4', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 3. PGI and related network architectures and methods. (a) Path Aggregation Network (PAN)) [37], (b) Reversible Columns\\n(RevCol) [3], (c) conventional deep supervision, and (d) our proposed Programmable Gradient Information (PGI). PGI is mainly composed\\nof three components: (1) main branch: architecture used for inference, (2) auxiliary reversible branch: generate reliable gradients to supply\\nmain branch for backward transmission, and (3) multi-level auxiliary information: control main branch learning plannable multi-level of\\nsemantic information.\\n4. Methodology\\n4.1. Programmable Gradient Information\\nIn order to solve the aforementioned problems, we pro-\\npose a new auxiliary supervision framework called Pro-\\ngrammable Gradient Information (PGI), as shown in Fig-\\nure 3 (d). PGI mainly includes three components, namely\\n(1) main branch, (2) auxiliary reversible branch, and (3)\\nmulti-level auxiliary information. From Figure 3 (d) we\\nsee that the inference process of PGI only uses main branch\\nand therefore does not require any additional inference cost.\\nAs for the other two components, they are used to solve or\\nslow down several important issues in deep learning meth-\\nods. Among them, auxiliary reversible branch is designed\\nto deal with the problems caused by the deepening of neural\\nnetworks. Network deepening will cause information bot-\\ntleneck, which will make the loss function unable to gener-\\nate reliable gradients. As for multi-level auxiliary informa-\\ntion, it is designed to handle the error accumulation problem\\ncaused by deep supervision, especially for the architecture\\nand lightweight model of multiple prediction branch. Next,\\nwe will introduce these two components step by step.\\n4.1.1 Auxiliary Reversible Branch\\nIn PGI, we propose auxiliary reversible branch to gener-\\nate reliable gradients and update network parameters. By\\nproviding information that maps from data to targets, the\\nloss function can provide guidance and avoid the possibil-\\nity of finding false correlations from incomplete feedfor-\\nward features that are less relevant to the target. We pro-pose the maintenance of complete information by introduc-\\ning reversible architecture, but adding main branch to re-\\nversible architecture will consume a lot of inference costs.\\nWe analyzed the architecture of Figure 3 (b) and found that\\nwhen additional connections from deep to shallow layers\\nare added, the inference time will increase by 20%. When\\nwe repeatedly add the input data to the high-resolution com-\\nputing layer of the network (yellow box), the inference time\\neven exceeds twice the time.\\nSince our goal is to use reversible architecture to ob-\\ntain reliable gradients, “reversible” is not the only neces-\\nsary condition in the inference stage. In view of this, we\\nregard reversible branch as an expansion of deep supervi-\\nsion branch, and then design auxiliary reversible branch, as\\nshown in Figure 3 (d). As for the main branch deep fea-\\ntures that would have lost important information due to in-\\nformation bottleneck, they will be able to receive reliable\\ngradient information from the auxiliary reversible branch.\\nThese gradient information will drive parameter learning to\\nassist in extracting correct and important information, and\\nthe above actions can enable the main branch to obtain fea-\\ntures that are more effective for the target task. Moreover,\\nthe reversible architecture performs worse on shallow net-\\nworks than on general networks because complex tasks re-\\nquire conversion in deeper networks. Our proposed method\\ndoes not force the main branch to retain complete origi-\\nnal information but updates it by generating useful gradient\\nthrough the auxiliary supervision mechanism. The advan-\\ntage of this design is that the proposed method can also be\\napplied to shallower networks.\\n5', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 4}),\n",
       " Document(page_content='Figure 4. The architecture of GELAN: (a) CSPNet [64], (b) ELAN [65], and (c) proposed GELAN. We imitate CSPNet and extend ELAN\\ninto GELAN that can support any computational blocks.\\nFinally, since auxiliary reversible branch can be removed\\nduring the inference phase, the inference capabilities of the\\noriginal network can be retained. We can also choose any\\nreversible architectures in PGI to play the role of auxiliary\\nreversible branch.\\n4.1.2 Multi-level Auxiliary Information\\nIn this section we will discuss how multi-level auxiliary in-\\nformation works. The deep supervision architecture includ-\\ning multiple prediction branch is shown in Figure 3 (c). For\\nobject detection, different feature pyramids can be used to\\nperform different tasks, for example together they can de-\\ntect objects of different sizes. Therefore, after connecting\\nto the deep supervision branch, the shallow features will be\\nguided to learn the features required for small object detec-\\ntion, and at this time the system will regard the positions\\nof objects of other sizes as the background. However, the\\nabove deed will cause the deep feature pyramids to lose a lot\\nof information needed to predict the target object. Regard-\\ning this issue, we believe that each feature pyramid needs\\nto receive information about all target objects so that subse-\\nquent main branch can retain complete information to learn\\npredictions for various targets.\\nThe concept of multi-level auxiliary information is to in-\\nsert an integration network between the feature pyramid hi-\\nerarchy layers of auxiliary supervision and the main branch,\\nand then uses it to combine returned gradients from differ-\\nent prediction heads, as shown in Figure 3 (d). Multi-level\\nauxiliary information is then to aggregate the gradient infor-\\nmation containing all target objects, and pass it to the main\\nbranch and then update parameters. At this time, the charac-\\nteristics of the main branch’s feature pyramid hierarchy will\\nnot be dominated by some specific object’s information. As\\na result, our method can alleviate the broken information\\nproblem in deep supervision. In addition, any integrated\\nnetwork can be used in multi-level auxiliary information.\\nTherefore, we can plan the required semantic levels to guide\\nthe learning of network architectures of different sizes.4.2. Generalized ELAN\\nIn this Section we describe the proposed new network\\narchitecture – GELAN. By combining two neural network\\narchitectures, CSPNet [64] and ELAN [65], which are de-\\nsigned with gradient path planning, we designed gener-\\nalized efficient layer aggregation network (GELAN) that\\ntakes into account lighweight, inference speed, and accu-\\nracy. Its overall architecture is shown in Figure 4. We gen-\\neralized the capability of ELAN [65], which originally only\\nused stacking of convolutional layers, to a new architecture\\nthat can use any computational blocks.\\n5. Experiments\\n5.1. Experimental Setup\\nWe verify the proposed method with MS COCO dataset.\\nAll experimental setups follow YOLOv7 AF [63], while the\\ndataset is MS COCO 2017 splitting. All models we men-\\ntioned are trained using the train-from-scratch strategy, and\\nthe total number of training times is 500 epochs. In setting\\nthe learning rate, we use linear warm-up in the first three\\nepochs, and the subsequent epochs set the corresponding\\ndecay manner according to the model scale. As for the last\\n15 epochs, we turn mosaic data augmentation off. For more\\nsettings, please refer to Appendix.\\n5.2. Implimentation Details\\nWe built general and extended version of YOLOv9 based\\non YOLOv7 [63] and Dynamic YOLOv7 [36] respectively.\\nIn the design of the network architecture, we replaced\\nELAN [65] with GELAN using CSPNet blocks [64] with\\nplanned RepConv [63] as computational blocks. We also\\nsimplified downsampling module and optimized anchor-\\nfree prediction head. As for the auxiliary loss part of PGI,\\nwe completely follow YOLOv7’s auxiliary head setting.\\nPlease see Appendix for more details.\\n6', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 5}),\n",
       " Document(page_content='Table 1. Comparison of state-of-the-art real-time object detectors.\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv5-N r7.0 [14] 1.9 4.5 28.0 45.7 – – – –\\nYOLOv5-S r7.0 [14] 7.2 16.5 37.4 56.8 – – – –\\nYOLOv5-M r7.0 [14] 21.2 49.0 45.4 64.1 – – – –\\nYOLOv5-L r7.0 [14] 46.5 109.1 49.0 67.3 – – – –\\nYOLOv5-X r7.0 [14] 86.7 205.7 50.7 68.9 – – – –\\nYOLOv6-N v3.0 [30] 4.7 11.4 37.0 52.7 – – – –\\nYOLOv6-S v3.0 [30] 18.5 45.3 44.3 61.2 – – – –\\nYOLOv6-M v3.0 [30] 34.9 85.8 49.1 66.1 – – – –\\nYOLOv6-L v3.0 [30] 59.6 150.7 51.8 69.2 – – – –\\nYOLOv7 [63] 36.9 104.7 51.2 69.7 55.9 31.8 55.5 65.0\\nYOLOv7-X [63] 71.3 189.9 52.9 71.1 51.4 36.9 57.7 68.6\\nYOLOv7-N AF [63] 3.1 8.7 37.6 53.3 40.6 18.7 41.7 52.8\\nYOLOv7-S AF [63] 11.0 28.1 45.1 61.8 48.9 25.7 50.2 61.2\\nYOLOv7 AF [63] 43.6 130.5 53.0 70.2 57.5 35.8 58.7 68.9\\nYOLOv8-N [15] 3.2 8.7 37.3 52.6 – – – –\\nYOLOv8-S [15] 11.2 28.6 44.9 61.8 – – – –\\nYOLOv8-M [15] 25.9 78.9 50.2 67.2 – – – –\\nYOLOv8-L [15] 43.7 165.2 52.9 69.8 57.5 35.3 58.3 69.8\\nYOLOv8-X [15] 68.2 257.8 53.9 71.0 58.7 35.7 59.3 70.7\\nDAMO YOLO-T [75] 8.5 18.1 42.0 58.0 45.2 23.0 46.1 58.5\\nDAMO YOLO-S [75] 12.3 37.8 46.0 61.9 49.5 25.9 50.6 62.5\\nDAMO YOLO-M [75] 28.2 61.8 49.2 65.5 53.0 29.7 53.1 66.1\\nDAMO YOLO-L [75] 42.1 97.3 50.8 67.5 55.5 33.2 55.7 66.6\\nGold YOLO-N [61] 5.6 12.1 39.6 55.7 – 19.7 44.1 57.0\\nGold YOLO-S [61] 21.5 46.0 45.4 62.5 – 25.3 50.2 62.6\\nGold YOLO-M [61] 41.3 87.5 49.8 67.0 – 32.3 55.3 66.3\\nGold YOLO-L [61] 75.1 151.7 51.8 68.9 – 34.1 57.4 68.2\\nYOLO MS-N [7] 4.5 17.4 43.4 60.4 47.6 23.7 48.3 60.3\\nYOLO MS-S [7] 8.1 31.2 46.2 63.7 50.5 26.9 50.5 63.0\\nYOLO MS [7] 22.2 80.2 51.0 68.6 55.7 33.1 56.1 66.5\\nGELAN-S (Ours) 7.2 26.7 46.7 63.0 50.7 25.9 51.5 64.0\\nGELAN-M (Ours) 20.1 76.8 51.1 67.9 55.7 33.6 56.4 67.3\\nGELAN-C (Ours) 25.5 102.8 52.5 69.5 57.3 35.8 57.6 69.4\\nGELAN-E (Ours) 58.1 192.5 55.0 71.9 60.0 38.0 60.6 70.9\\nYOLOv9-S (Ours) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nYOLOv9-M (Ours) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nYOLOv9-C (Ours) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nYOLOv9-E (Ours) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4\\n5.3. Comparison with state-of-the-arts\\nTable 1 lists comparison of our proposed YOLOv9 with\\nother train-from-scratch real-time object detectors. Over-\\nall, the best performing methods among existing methods\\nare YOLO MS-S [7] for lightweight models, YOLO MS [7]\\nfor medium models, YOLOv7 AF [63] for general mod-\\nels, and YOLOv8-X [15] for large models. Compared with\\nlightweight and medium model YOLO MS [7], YOLOv9\\nhas about 10% less parameters and 5 ∼15% less calcula-\\ntions, but still has a 0.4 ∼0.6% improvement in AP. Com-\\npared with YOLOv7 AF, YOLOv9-C has 42% less pa-\\nrameters and 21% less calculations, but achieves the same\\nAP (53%). Compared with YOLOv8-X, YOLOv9-X has\\n15% less parameters, 25% less calculations, and has sig-\\nnificant improvement of 1.7% AP. The above comparison\\nresults show that our proposed YOLOv9 has significantlyimproved in all aspects compared with existing methods.\\nOn the other hand, we also include ImageNet pretrained\\nmodel in the comparison, and the results are shown in Fig-\\nure 5. We compare them based on the parameters and the\\namount of computation respectively. In terms of the num-\\nber of parameters, the best performing large model is RT\\nDETR [43]. From Figure 5, we can see that YOLOv9 using\\nconventional convolution is even better than YOLO MS us-\\ning depth-wise convolution in parameter utilization. As for\\nthe parameter utilization of large models, it also greatly sur-\\npasses RT DETR using ImageNet pretrained model. Even\\nbetter is that in the deep model, YOLOv9 shows the huge\\nadvantages of using PGI. By accurately retaining and ex-\\ntracting the information needed to map the data to the tar-\\nget, our method requires only 64% of the parameters while\\nmaintaining the accuracy as RT DETR-X.\\n7', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 6}),\n",
       " Document(page_content='Figure 5. Comparison of state-of-the-art real-time object detectors. The methods participating in the comparison all use ImageNet as\\npre-trained weights, including RT DETR [43], RTMDet [44], and PP-YOLOE [74], etc. The YOLOv9 that uses train-from-scratch method\\nclearly surpasses the performance of other methods.\\nAs for the amount of computation, the best existing mod-\\nels from the smallest to the largest are YOLO MS [7], PP\\nYOLOE [74], and RT DETR [43]. From Figure 5, we can\\nsee that YOLOv9 is far superior to the train-from-scratch\\nmethods in terms of computational complexity. In addi-\\ntion, if compared with those based on depth-wise convo-\\nlution and ImageNet-based pretrained models, YOLOv9 is\\nalso very competitive.\\n5.4. Ablation Studies\\n5.4.1 Generalized ELAN\\nFor GELAN, we first do ablation studies for computational\\nblocks. We used Res blocks [21], Dark blocks [49], and\\nCSP blocks [64] to conduct experiments, respectively. Ta-\\nble 2 shows that after replacing convolutional layers in\\nELAN with different computational blocks, the system can\\nmaintain good performance. Users are indeed free to re-\\nplace computational blocks and use them on their respective\\ninference devices. Among different computational block re-\\nplacements, CSP blocks perform particularly well. They\\nnot only reduce the amount of parameters and computation,\\nbut also improve AP by 0.7%. Therefore, we choose CSP-\\nELAN as the component unit of GELAN in YOLOv9.\\nTable 2. Ablation study on various computational blocks.\\nModel CB type #Param. FLOPs APval\\n50:95\\nGELAN-S Conv 6.3M 24.0G 44.8%\\nGELAN-S Res [21] 5.5M 21.1G 44.3%\\nGELAN-S Dark [49] 5.7M 21.8G 44.5%\\nGELAN-S CSP [64] 5.9M 22.5G 45.5%\\n1CB type nedotes as computational block type.\\n2-S nedotes small size model.Next, we conduct ELAN block-depth and CSP block-\\ndepth experiments on GELAN of different sizes, and dis-\\nplay the results in Table 3. We can see that when the depth\\nof ELAN is increased from 1 to 2, the accuracy is signif-\\nicantly improved. But when the depth is greater than or\\nequal to 2, no matter it is improving the ELAN depth or the\\nCSP depth, the number of parameters, the amount of com-\\nputation, and the accuracy will always show a linear rela-\\ntionship. This means GELAN is not sensitive to the depth.\\nIn other words, users can arbitrarily combine the compo-\\nnents in GELAN to design the network architecture, and\\nhave a model with stable performance without special de-\\nsign. In Table 3, for YOLOv9- {S,M,C}, we set the pairing\\nof the ELAN depth and the CSP depth to {{2, 3},{2, 1},\\n{2, 1}}.\\nTable 3. Ablation study on ELAN and CSP depth.\\nModel D ELAN DCSP #Param. FLOPs APval\\n50:95\\nGELAN-S 2 1 5.9M 22.5G 45.5%\\nGELAN-S 2 2 6.6M 24.6G 46.0%\\nGELAN-S 3 1 7.1M 26.4G 46.5%\\nGELAN-S 2 3 7.2M 26.7G 46.7%\\nGELAN-M 2 1 20.1M 76.8G 51.1%\\nGELAN-M 2 2 22.4M 86.1G 51.7%\\nGELAN-M 3 1 24.5M 94.2G 51.8%\\nGELAN-M 2 3 24.8M 95.5G 52.3%\\nGELAN-C 1 1 19.0M 77.8G 50.7%\\nGELAN-C 2 1 25.5M 102.8G 52.5%\\nGELAN-C 2 2 28.9M 115.8G 53.0%\\nGELAN-C 3 1 32.0M 127.9G 53.2%\\nGELAN-C 2 3 32.4M 128.7G 53.3%\\n1DELAN andDCSP respectively nedotes depth of ELAN and CSP.\\n2-{S, M, C }indicate small, medium, and compact models.\\n8', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 7}),\n",
       " Document(page_content='5.4.2 Programmable Gradient Information\\nIn terms of PGI, we performed ablation studies on auxiliary\\nreversible branch and multi-level auxiliary information on\\nthe backbone and neck, respectively. We designed auxiliary\\nreversible branch ICN to use DHLC [34] linkage to obtain\\nmulti-level reversible information. As for multi-level aux-\\niliary information, we use FPN and PAN for ablation stud-\\nies and the role of PFH is equivalent to the traditional deep\\nsupervision. The results of all experiments are listed in Ta-\\nble 4. From Table 4, we can see that PFH is only effective in\\ndeep models, while our proposed PGI can improve accuracy\\nunder different combinations. Especially when using ICN,\\nwe get stable and better results. We also tried to apply the\\nlead-head guided assignment proposed in YOLOv7 [63] to\\nthe PGI’s auxiliary supervision, and achieved much better\\nperformance.\\nTable 4. Ablation study on PGI of backbone and neck.\\nModel G backbone Gneck APval\\n50:95APval\\nSAPval\\nMAPval\\nL\\nGELAN-C – – 52.5% 35.8% 57.6% 69.4%\\nGELAN-C PFH – 52.5% 35.3% 58.1% 68.9%\\nGELAN-C FPN – 52.6% 35.3% 58.1% 68.9%\\nGELAN-C – ICN 52.7% 35.3% 58.4% 68.9%\\nGELAN-C FPN ICN 52.8% 35.8% 58.2% 69.1%\\nGELAN-C ICN – 52.9% 35.2% 58.7% 68.6%\\nGELAN-C LHG-ICN – 53.0% 36.3% 58.5% 69.1%\\nGELAN-E – – 55.0% 38.0% 60.6% 70.9%\\nGELAN-E PFH – 55.3% 38.3% 60.3% 71.6%\\nGELAN-E FPN – 55.6% 40.2% 61.0% 71.4%\\nGELAN-E PAN – 55.5% 39.0% 61.1% 71.5%\\nGELAN-E FPN ICN 55.6% 39.8% 60.9% 71.9%\\n1DELAN andDCSP respectively nedotes depth of ELAN and CSP.\\n2LHG indicates lead head guided training proposed by YOLOv7 [63].\\nWe further implemented the concepts of PGI and deep\\nsupervision on models of various sizes and compared the\\nresults, these results are shown in Table 5. As analyzed at\\nthe beginning, introduction of deep supervision will cause\\na loss of accuracy for shallow models. As for general mod-\\nels, introducing deep supervision will cause unstable perfor-\\nmance, and the design concept of deep supervision can only\\nbring gains in extremely deep models. The proposed PGI\\ncan effectively handle problems such as information bottle-\\nneck and information broken, and can comprehensively im-\\nprove the accuracy of models of different sizes. The concept\\nof PGI brings two valuable contributions. The first one is to\\nmake the auxiliary supervision method applicable to shal-\\nlow models, while the second one is to make the deep model\\ntraining process obtain more reliable gradients. These gra-\\ndients enable deep models to use more accurate information\\nto establish correct correlations between data and targets.Table 5. Ablation study on PGI.\\nModel APval\\n50:95APval\\n50APval\\n75\\nGELAN-S 46.7% 63.0% 50.7%\\n+ DS 46.5% -0.2 62.9% -0.1 50.5% -0.2\\n+ PGI 46.8% +0.1 63.4% +0.4 50.7% =\\nGELAN-M 51.1% 67.9% 55.7%\\n+ DS 51.2% +0.1 68.2% +0.3 55.7% =\\n+ PGI 51.4% +0.3 68.1% +0.2 56.1% +0.4\\nGELAN-C 52.5% 69.5% 57.3%\\n+ DS 52.5% = 69.9% +0.4 57.1% -0.2\\n+ PGI 53.0% +0.5 70.3% +0.8 57.8% +0.5\\nGELAN-E 55.0% 71.9% 60.0%\\n+ DS 55.3% +0.3 72.3% +0.4 60.2% +0.2\\n+ PGI 55.6% +0.6 72.8% +0.9 60.6% +0.6\\n1DS indicates deep supervision.\\nFinally, we show in the table the results of gradually in-\\ncreasing components from baseline YOLOv7 to YOLOv9-\\nE. The GELAN and PGI we proposed have brought all-\\nround improvement to the model.\\nTable 6. Ablation study on GELAN and PGI.\\nModel #Param. FLOPs APval\\n50:95APval\\nSAPval\\nMAPval\\nL\\nYOLOv7 [63] 36.9 104.7 51.2% 31.8% 55.5% 65.0%\\n+ AF [63] 43.6 130.5 53.0% 35.8% 58.7% 68.9%\\n+ GELAN 41.7 127.9 53.2% 36.2% 58.5% 69.9%\\n+ DHLC [34] 58.1 192.5 55.0% 38.0% 60.6% 70.9%\\n+ PGI 58.1 192.5 55.6% 40.2% 61.0% 71.4%\\n5.5. Visualization\\nThis section will explore the information bottleneck is-\\nsues and visualize them. In addition, we will also visualize\\nhow the proposed PGI uses reliable gradients to find the\\ncorrect correlations between data and targets. In Figure 6\\nwe show the visualization results of feature maps obtained\\nby using random initial weights as feedforward under dif-', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 8}),\n",
       " Document(page_content='correct correlations between data and targets. In Figure 6\\nwe show the visualization results of feature maps obtained\\nby using random initial weights as feedforward under dif-\\nferent architectures. We can see that as the number of lay-\\ners increases, the original information of all architectures\\ngradually decreases. For example, at the 50thlayer of the\\nPlainNet, it is difficult to see the location of objects, and all\\ndistinguishable features will be lost at the 100thlayer. As\\nfor ResNet, although the position of object can still be seen\\nat the 50thlayer, the boundary information has been lost.\\nWhen the depth reached to the 100thlayer, the whole image\\nbecomes blurry. Both CSPNet and the proposed GELAN\\nperform very well, and they both can maintain features that\\nsupport clear identification of objects until the 200thlayer.\\nAmong the comparisons, GELAN has more stable results\\nand clearer boundary information.\\n9', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 8}),\n",
       " Document(page_content='Figure 6. Feature maps (visualization results) output by random initial weights of PlainNet, ResNet, CSPNet, and GELAN at different\\ndepths. After 100 layers, ResNet begins to produce feedforward output that is enough to obfuscate object information. Our proposed\\nGELAN can still retain quite complete information up to the 150thlayer, and is still sufficiently discriminative up to the 200thlayer.\\nFigure 7. PAN feature maps (visualization results) of GELAN\\nand YOLOv9 (GELAN + PGI) after one epoch of bias warm-up.\\nGELAN originally had some divergence, but after adding PGI’s\\nreversible branch, it is more capable of focusing on the target ob-\\nject.\\nFigure 7 is used to show whether PGI can provide more\\nreliable gradients during the training process, so that the\\nparameters used for updating can effectively capture the\\nrelationship between the input data and the target. Fig-\\nure 7 shows the visualization results of the feature map of\\nGELAN and YOLOv9 (GELAN + PGI) in PAN bias warm-\\nup. From the comparison of Figure 7(b) and (c), we can\\nclearly see that PGI accurately and concisely captures the\\narea containing objects. As for GELAN that does not use\\nPGI, we found that it had divergence when detecting ob-ject boundaries, and it also produced unexpected responses\\nin some background areas. This experiment confirms that\\nPGI can indeed provide better gradients to update parame-\\nters and enable the feedforward stage of the main branch to\\nretain more important features.\\n6. Conclusions\\nIn this paper, we propose to use PGI to solve the infor-\\nmation bottleneck problem and the problem that the deep\\nsupervision mechanism is not suitable for lightweight neu-\\nral networks. We designed GELAN, a highly efficient\\nand lightweight neural network. In terms of object detec-\\ntion, GELAN has strong and stable performance at different\\ncomputational blocks and depth settings. It can indeed be\\nwidely expanded into a model suitable for various inference\\ndevices. For the above two issues, the introduction of PGI\\nallows both lightweight models and deep models to achieve\\nsignificant improvements in accuracy. The YOLOv9, de-\\nsigned by combining PGI and GELAN, has shown strong\\ncompetitiveness. Its excellent design allows the deep model\\nto reduce the number of parameters by 49% and the amount\\nof calculations by 43% compared with YOLOv8, but it still\\nhas a 0.6% AP improvement on MS COCO dataset.\\n7. Acknowledgements\\nThe authors wish to thank National Center for High-\\nperformance Computing (NCHC) for providing computa-\\ntional and storage resources.\\n10', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 9}),\n",
       " Document(page_content='References\\n[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT:\\nBERT pre-training of image transformers. In International\\nConference on Learning Representations (ICLR) , 2022. 2\\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\\nYuan Mark Liao. YOLOv4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934 , 2020. 3\\n[3] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiang-\\nwen Kong, Jun Li, and Xiangyu Zhang. Reversible column\\nnetworks. In International Conference on Learning Repre-\\nsentations (ICLR) , 2023. 2, 3, 5\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\\nto-end object detection with transformers. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\npages 213–229, 2020. 3\\n[5] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\\nJunni Zou. AP-loss for accurate one-stage object detection.\\nIEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence (TPAMI) , 43(11):3782–3798, 2020. 1\\n[6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang,\\nWenrui Dai, Hongkai Xiong, and Qi Tian. SdAE: Self-\\ndistillated masked autoencoder. In Proceedings of the Euro-\\npean Conference on Computer Vision (ECCV) , pages 108–\\n124, 2022. 2\\n[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin\\nHou, and Ming-Ming Cheng. YOLO-MS: rethinking multi-\\nscale representation learning for real-time object detection.\\narXiv preprint arXiv:2308.05480 , 2023. 1, 3, 7, 8\\n[8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong\\nWang, and Lu Yuan. DaVIT: Dual attention vision trans-\\nformers. In Proceedings of the European Conference on\\nComputer Vision (ECCV) , pages 74–92, 2022. 1\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. In International Con-\\nference on Learning Representations (ICLR) , 2021. 1, 2\\n[10] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,\\nand Weilin Huang. TOOD: Task-aligned one-stage object\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV) , pages 3490–3499,\\n2021. 1\\n[11] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu\\nZhang, Ming-Hsuan Yang, and Philip Torr. Res2Net: A\\nnew multi-scale backbone architecture. IEEE Transac-\\ntions on Pattern Analysis and Machine Intelligence (TPAMI) ,\\n43(2):652–662, 2019. 3\\n[12] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian\\nSun. OTA: Optimal transport assignment for object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 303–\\n312, 2021. 1\\n[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: Exceeding YOLO series in 2021. arXiv\\npreprint arXiv:2107.08430 , 2021. 3[14] Jocher Glenn. YOLOv5 release v7.0. https://github.\\ncom/ultralytics/yolov5/releases/tag/v7.\\n0, 2022. 3, 7\\n[15] Jocher Glenn. YOLOv8 release v8.1.0. https :\\n/ / github . com / ultralytics / ultralytics /\\nreleases/tag/v8.1.0 , 2024. 3, 7\\n[16] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B\\nGrosse. The reversible residual network: Backpropagation\\nwithout storing activations. Advances in Neural Information\\nProcessing Systems (NeurIPS) , 2017. 2, 3\\n[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence\\nmodeling with selective state spaces. arXiv preprint\\narXiv:2312.00752 , 2023. 1\\n[18] Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, and\\nChunhong Pan. AugFPN: Improving multi-scale fea-\\nture learning for object detection. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 12595–12604, 2020. 1, 3\\n[19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Ex-\\nploring disentangled representations in masked image mod-\\neling. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 2, 3', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 10}),\n",
       " Document(page_content='[19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Ex-\\nploring disentangled representations in masked image mod-\\neling. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 2, 3\\n[20] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.\\nBoundary-aware instance segmentation. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 5696–5704, 2017. 1, 3\\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 770–778, 2016. 1, 4, 8\\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nIdentity mappings in deep residual networks. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\npages 630–645. Springer, 2016. 1, 4\\n[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4700–4708, 2017. 1\\n[24] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Win-\\nston H Hsu. MonoDTR: Monocular 3D object detection with\\ndepth-aware transformer. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 4012–4021, 2022. 1, 3\\n[25] Lin Huang, Weisheng Li, Linlin Shen, Haojie Fu, Xue Xiao,\\nand Suihan Xiao. YOLOCS: Object detection based on dense\\nchannel compression for feature spatial solidification. arXiv\\npreprint arXiv:2305.04170 , 2023. 3\\n[26] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,\\nAndrew Zisserman, and Joao Carreira. Perceiver: General\\nperception with iterative attention. In International Confer-\\nence on Machine Learning (ICML) , pages 4651–4664, 2021.\\n1\\n[27] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\\nToutanova. BERT: Pre-training of deep bidirectional trans-\\nformers for language understanding. In Proceedings of\\nNAACL-HLT , volume 1, page 2, 2019. 2\\n11', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 10}),\n",
       " Document(page_content='[28] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\\nZhang, and Zhuowen Tu. Deeply-supervised nets. In Ar-\\ntificial Intelligence and Statistics , pages 562–570, 2015. 1,\\n2, 3\\n[29] Alex Levinshtein, Alborz Rezazadeh Sereshkeh, and Kon-\\nstantinos Derpanis. DATNet: Dense auxiliary tasks for ob-\\nject detection. In Proceedings of the IEEE/CVF Winter Con-\\nference on Applications of Computer Vision (WACV) , pages\\n1419–1427, 2020. 1, 3\\n[30] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng\\nCheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-\\niang Chu. YOLOv6 v3.0: A full-scale reloading. arXiv\\npreprint arXiv:2301.05586 , 2023. 3, 7, 2, 4\\n[31] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei\\nGeng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, et al. YOLOv6: A single-stage object de-\\ntection framework for industrial applications. arXiv preprint\\narXiv:2209.02976 , 2022. 3\\n[32] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng\\nLi, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang,\\nWenhai Wang, et al. Uni-perceiver v2: A generalist model\\nfor large-scale vision and vision-language tasks. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 2691–2700, 2023. 1\\n[33] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A\\ndual weighting label assignment scheme for object detection.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 9387–9396,\\n2022. 1\\n[34] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\\nZhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-\\nNet: A composite backbone network architecture for object\\ndetection. IEEE Transactions on Image Processing (TIP) ,\\n2022. 3, 9\\n[35] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyra-\\nmid networks for object detection. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 2117–2125, 2017. 3\\n[36] Zhihao Lin, Yongtao Wang, Jinhe Zhang, and Xiaojie Chu.\\nDynamicDet: A unified dynamic architecture for object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n6282–6291, 2023. 3, 6, 2, 4\\n[37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\\nPath aggregation network for instance segmentation. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR) , pages 8759–8768, 2018.\\n3, 5\\n[38] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi\\nXie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba:\\nVisual state space model. arXiv preprint arXiv:2401.10166 ,\\n2024. 1\\n[39] Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang,\\nQijie Zhao, Zhi Tang, and Haibin Ling. CBNet: A novel\\ncomposite backbone network architecture for object detec-\\ntion. In Proceedings of the AAAI Conference on Artificial\\nIntelligence (AAAI) , pages 11653–11660, 2020. 3[40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\\nSwin transformer v2: Scaling up capacity and resolution. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR) , 2022. 1\\n[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV) , pages 10012–10022, 2021. 1\\n[42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A ConvNet for the\\n2020s. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 11976–\\n11986, 2022. 1\\n[43] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang,\\nJinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and\\nYi Liu. DETRs beat YOLOs on real-time object detection.\\narXiv preprint arXiv:2304.08069 , 2023. 1, 3, 7, 8, 2, 4', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 11}),\n",
       " Document(page_content='Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and\\nYi Liu. DETRs beat YOLOs on real-time object detection.\\narXiv preprint arXiv:2304.08069 , 2023. 1, 3, 7, 8, 2, 4\\n[44] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou,\\nYudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.\\nRTMDet: An empirical study of designing real-time object\\ndetectors. arXiv preprint arXiv:2212.07784 , 2022. 8, 2, 3, 4\\n[45] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. A ranking-based, balanced loss function unify-\\ning classification and localisation in object detection. Ad-\\nvances in Neural Information Processing Systems (NeurIPS) ,\\n33:15534–15545, 2020. 1\\n[46] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. Rank & sort loss for object detection and instance\\nsegmentation. In Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision (ICCV) , pages 3009–\\n3018, 2021. 1\\n[47] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Unified, real-time object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 779–\\n788, 2016. 3\\n[48] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7263–7271, 2017. 3\\n[49] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental\\nimprovement. arXiv preprint arXiv:1804.02767 , 2018. 3, 8\\n[50] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\\ntersection over union: A metric and a loss for bounding box\\nregression. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n658–666, 2019. 1\\n[51] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,\\nYurong Chen, and Xiangyang Xue. Object detection from\\nscratch with deep supervision. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence (TPAMI) , 42(2):398–412,\\n2019. 1, 2\\n[52] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-\\nactor: A multi-task transformer for robotic manipulation.\\n12', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 11}),\n",
       " Document(page_content='InConference on Robot Learning (CoRL) , pages 785–799,\\n2023. 1\\n[53] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan,\\nChanghu Wang, and Ping Luo. What makes for end-to-end\\nobject detection? In International Conference on Machine\\nLearning (ICML) , pages 9934–9944, 2021. 3\\n[54] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n1–9, 2015. 1, 2, 3\\n[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\\ntecture for computer vision. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 2818–2826, 2016. 1\\n[56] Zineng Tang, Jaemin Cho, Jie Lei, and Mohit Bansal.\\nPerceiver-VL: Efficient vision-and-language modeling with\\niterative latent attention. In Proceedings of the IEEE/CVF\\nWinter Conference on Applications of Computer Vision\\n(WACV) , pages 4410–4420, 2023. 1\\n[57] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nFully convolutional one-stage object detection. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV) , pages 9627–9636, 2019. 3\\n[58] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nA simple and strong anchor-free object detector. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(TPAMI) , 44(4):1922–1933, 2022. 3\\n[59] Naftali Tishby and Noga Zaslavsky. Deep learning and the\\ninformation bottleneck principle. In IEEE Information The-\\nory Workshop (ITW) , pages 1–5, 2015. 2, 4\\n[60] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\\nPeyman Milanfar, Alan Bovik, and Yinxiao Li. MaxVIT:\\nMulti-axis vision transformer. In Proceedings of the Euro-\\npean Conference on Computer Vision (ECCV) , pages 459–\\n479, 2022. 1\\n[61] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo,\\nChuanjian Liu, Kai Han, and Yunhe Wang. Gold-YOLO:\\nEfficient object detector via gather-and-distribute mecha-\\nnism. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 3, 7, 2, 4\\n[62] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Scaled-YOLOv4: Scaling cross stage\\npartial network. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 13029–13038, 2021. 3\\n[63] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. YOLOv7: Trainable bag-of-freebies\\nsets new state-of-the-art for real-time object detectors. In\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 7464–7475,\\n2023. 3, 6, 7, 9, 1\\n[64] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet: A\\nnew backbone that can enhance learning capability of CNN.InProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition Workshops (CVPRW) , pages\\n390–391, 2020. 3, 6, 8\\n[65] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh.\\nDesigning network design strategies through gradient path\\nanalysis. Journal of Information Science and Engineering\\n(JISE) , 39(4):975–995, 2023. 2, 3, 6\\n[66] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\\nYou only learn one representation: Unified network for mul-\\ntiple tasks. Journal of Information Science & Engineering\\n(JISE) , 39(3):691–709, 2023. 2, 3, 4\\n[67] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\\nSun, and Nanning Zheng. End-to-end object detection\\nwith fully convolutional network. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 15849–15858, 2021. 1, 3\\n[68] Liwei Wang, Chen-Yu Lee, Zhuowen Tu, and Svetlana\\nLazebnik. Training deeper convolutional networks with deep\\nsupervision. arXiv preprint arXiv:1505.02496 , 2015. 1, 2, 3\\n[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 12}),\n",
       " Document(page_content='supervision. arXiv preprint arXiv:1505.02496 , 2015. 1, 2, 3\\n[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\\nPyramid vision transformer: A versatile backbone for dense\\nprediction without convolutions. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision\\n(ICCV) , pages 568–578, 2021. 1\\n[70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. PVT\\nv2: Improved baselines with pyramid vision transformer.\\nComputational Visual Media , 8(3):415–424, 2022. 1\\n[71] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\\nChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\\nvNeXt v2: Co-designing and scaling convnets with masked\\nautoencoders. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n16133–16142, 2023. 1, 2\\n[72] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\\nKaiming He. Aggregated residual transformations for deep\\nneural networks. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 1492–1500, 2017. 1\\n[73] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\\nBao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: A simple\\nframework for masked image modeling. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 9653–9663, 2022. 2\\n[74] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,\\nCheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing\\nDang, Shengyu Wei, Yuning Du, et al. PP-YOLOE: An\\nevolved version of YOLO. arXiv preprint arXiv:2203.16250 ,\\n2022. 3, 8, 2, 4\\n[75] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang,\\nYuan Zhang, and Xiuyu Sun. DAMO-YOLO: A re-\\nport on real-time object detection design. arXiv preprint\\narXiv:2211.15444 , 2022. 3, 7, 2, 4\\n[76] Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui, Yu\\nQiao, Hongsheng Li, and Peng Gao. MonoDETR: Depth-\\nguided transformer for monocular 3D object detection. In\\n13', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 12}),\n",
       " Document(page_content='Proceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV) , pages 9155–9166, 2023. 1, 3\\n[77] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\\nYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-\\nter learning for bounding box regression. In Proceedings of\\nthe AAAI Conference on Artificial Intelligence (AAAI) , vol-\\nume 34, pages 12993–13000, 2020. 1\\n[78] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\\nYin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D\\nobject detection. In International Conference on 3D Vision\\n(3DV) , pages 85–94, 2019. 1\\n[79] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,\\nSongtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-\\nentiable label assignment for dense object detection. arXiv\\npreprint arXiv:2007.03496 , 2020. 1\\n[80] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,\\nWenyu Liu, and Xinggang Wang. Vision mamba: Efficient\\nvisual representation learning with bidirectional state space\\nmodel. arXiv preprint arXiv:2401.09417 , 2024. 1\\n[81] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng\\nLi, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-\\ntraining unified architecture for generic perception for zero-\\nshot and few-shot tasks. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 16804–16815, 2022. 1\\n[82] Zhuofan Zong, Guanglu Song, and Yu Liu. DETRs with\\ncollaborative hybrid assignments training. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 6748–6758, 2023. 3\\n14', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 13}),\n",
       " Document(page_content='Appendix\\nA. Implementation Details\\nTable 1. Hyper parameter settings of YOLOv9.\\nhyper parameter value\\nepochs 500\\noptimizer SGD\\ninitial learning rate 0.01\\nfinish learning rate 0.0001\\nlearning rate decay linear\\nmomentum 0.937\\nweight decay 0.0005\\nwarm-up epochs 3\\nwarm-up momentum 0.8\\nwarm-up bias learning rate 0.1\\nbox loss gain 7.5\\nclass loss gain 0.5\\nDFL loss gain 1.5\\nHSV saturation augmentation 0.7\\nHSV value augmentation 0.4\\ntranslation augmentation 0.1\\nscale augmentation 0.9\\nmosaic augmentation 1.0\\nMixUp augmentation 0.15\\ncopy & paste augmentation 0.3\\nclose mosaic epochs 15\\nThe training parameters of YOLOv9 are shown in Ta-\\nble 1. We fully follow the settings of YOLOv7 AF [63],\\nwhich is to use SGD optimizer to train 500 epochs. We first\\nwarm-up for 3 epochs and only update the bias during the\\nwarm-up stage. Next we step down from the initial learning\\nrate 0.01 to 0.0001 in linear decay manner, and the data aug-\\nmentation settings are listed in the bottom part of Table 1.\\nWe shut down mosaic data augmentation operations on the\\nlast 15 epochs.Table 2. Network configurations of YOLOv9.\\nIndex Module Route Filters Depth Size Stride\\n0 Conv – 64 – 3 2\\n1 Conv 0 128 – 3 2\\n2 CSP-ELAN 1 256, 128, 64 2, 1 – 1\\n3 DOWN 2 256 – 3 2\\n4 CSP-ELAN 3 512, 256, 128 2, 1 – 1\\n5 DOWN 4 512 – 3 2\\n6 CSP-ELAN 5 512, 512, 256 2, 1 – 1\\n7 DOWN 6 512 – 3 2\\n8 CSP-ELAN 7 512, 512, 256 2, 1 – 1\\n9 SPP-ELAN 8 512, 256, 256 3, 1 – 1\\n10 Up 9 512 – – 2\\n11 Concat 10, 6 1024 – – 1\\n12 CSP-ELAN 11 512, 512, 256 2, 1 – 1\\n13 Up 12 512 – – 2\\n14 Concat 13, 4 1024 – – 1\\n15 CSP-ELAN 14 256, 256, 128 2, 1 – 1\\n16 DOWN 15 256 – 3 2\\n17 Concat 16, 12 768 – – 1\\n18 CSP-ELAN 17 512, 512, 256 2, 1 – 1\\n19 DOWN 18 512 – 3 2\\n20 Concat 19, 9 1024 – – 1\\n21 CSP-ELAN 20 512, 512, 256 2, 1 – 1\\n22 Predict 15, 18, 21 – – – –\\nThe network topology of YOLOv9 completely follows\\nYOLOv7 AF [63], that is, we replace ELAN with the pro-\\nposed CSP-ELAN block. As listed in Table 2, the depth\\nparameters of CSP-ELAN are represented as ELAN depth\\nand CSP depth, respectively. As for the parameters of CSP-\\nELAN filters, they are represented as ELAN output fil-\\nter, CSP output filter, and CSP inside filter. In the down-\\nsampling module part, we simplify CSP-DOWN module to\\nDOWN module. DOWN module is composed of a pooling\\nlayer with size 2 and stride 1, and a Conv layer with size 3\\nand stride 2. Finally, we optimized the prediction layer and\\nreplaced top, left, bottom, and right in the regression branch\\nwith decoupled branch.\\n1', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 14}),\n",
       " Document(page_content='Table 3. Comparison of state-of-the-art object detectors with different training settings.\\nModel #Param. (M) FLOPs (G) AP 50:95 (%) AP 50(%) AP 75(%) AP S(%) AP M(%) AP L(%)Train-from-scratchDy-YOLOv7 [36] – 181.7 53.9 72.2 58.7 35.3 57.6 66.4\\nDy-YOLOv7-X [36] – 307.9 55.0 73.2 60.0 36.6 58.7 68.5\\nYOLOv9-S (Ours) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nYOLOv9-M (Ours) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nYOLOv9-C (Ours) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nYOLOv9-E (Ours) 35.0 148.1 54.5 71.7 59.2 38.1 59.9 70.3\\nYOLOv9-E (Ours) 44.8 187.0 55.1 72.3 60.7 38.7 60.6 71.4\\nYOLOv9-E (Ours) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4ImageNet PretrainedRTMDet-T [44] 4.8 12.6 41.1 57.9 – – – –\\nRTMDet-S [44] 9.0 25.6 44.6 61.9 – – – –\\nRTMDet-M [44] 24.7 78.6 49.4 66.8 – – – –\\nRTMDet-L [44] 52.3 160.4 51.5 68.8 – – – –\\nRTMDet-X [44] 94.9 283.4 52.8 70.4 – – – –\\nPPYOLOE-S [74] 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nPPYOLOE-M [74] 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nPPYOLOE-L [74] 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nPPYOLOE-X [74] 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\nRT DETR-L [43] 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nRT DETR-X [43] 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nRT DETR-R18 [43] 20 60 46.5 63.8 – – – –\\nRT DETR-R34 [43] 31 92 48.9 66.8 – – – –\\nRT DETR-R50M [43] 36 100 51.3 69.6 – – – –\\nRT DETR-R50 [43] 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nRT DETR-R101 [43] 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nGold YOLO-S [61] 21.5 46.0 45.5 62.2 – – – –\\nGold YOLO-M [61] 41.3 57.5 50.2 67.5 – – – –\\nGold YOLO-L [61] 75.1 151.7 52.3 69.6 – – – –Knowledge DistillationYOLOv6-N v3.0 [30] 4.7 11.4 37.5 53.1 – – – –\\nYOLOv6-S v3.0 [30] 18.5 45.3 45.0 61.8 – – – –\\nYOLOv6-M v3.0 [30] 34.9 85.8 50.0 66.9 – – – –\\nYOLOv6-L v3.0 [30] 59.6 150.7 52.8 70.3 – – – –\\nDAMO YOLO-T [75] 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nDAMO YOLO-S [75] 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nDAMO YOLO-M [75] 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nDAMO YOLO-L [75] 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nGold YOLO-N [61] 5.6 12.1 39.9 55.9 – – – –\\nGold YOLO-S [61] 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-M [61] 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-L [61] 75.1 151.7 53.2 70.5 – – – –Complex SettingGold YOLO-S [61] 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-M [61] 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-L [61] 75.1 151.7 53.3 70.9 – – – –\\nYOLOR-CSP [66] 52.9 120.4 52.8 71.2 57.6 – – –\\nYOLOR-CSP-X [66] 96.9 226.8 54.8 73.1 59.7 – – –\\nPPYOLOE+-S [74] 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE+-M [74] 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE+-L [74] 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE+-X [74] 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nB. More Comparison\\nWe compare YOLOv9 to state-of-the-art real-time object\\ndetectors trained with different methods. It mainly includes\\nfour different training methods: (1) train-from-scratch: we\\nhave completed most of the comparisons in the text. Here\\nare only list of additional data of DynamicDet [36] for com-\\nparisons; (2) Pretrained by ImageNet: this includes two\\nmethods of using ImageNet for supervised pretrain and self-\\nsupervised pretrain; (3) knowledge distillation: a method\\nto perform additional self-distillation after training is com-pleted; and (4) a more complex training process: a combi-\\nnation of steps including pretrained by ImageNet, knowl-\\nedge distillation, DAMO-YOLO and even additional pre-\\ntrained large object detection dataset. We show the results\\nin Table 3. From this table, we can see that our proposed\\nYOLOv9 performed better than all other methods. Com-\\npared with PPYOLOE+-X trained using ImageNet and Ob-\\njects365, our method still reduces the number of parameters\\nby 54% and the amount of computation by 9%, and improv-\\ning 0.4% AP.\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 15}),\n",
       " Document(page_content='Table 4. Comparison of state-of-the-art object detectors with different training settings (sorted by number of parameters).\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv6-N v3.0 [30] (D) 4.7 11.4 37.5 53.1 – – – –\\nRTMDet-T [44] (I) 4.8 12.6 41.1 57.9 – – – –\\nGold YOLO-N [61] (D) 5.6 12.1 39.9 55.9 – – – –\\nYOLOv9-S (S) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nPPYOLOE+-S [74] (C) 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE-S [74] (I) 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nDAMO YOLO-T [75] (D) 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nRTMDet-S [44] (I) 9.0 25.6 44.6 61.9 – – – –\\nDAMO YOLO-S [75] (D) 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nYOLOv6-S v3.0 [30] (D) 18.5 45.3 45.0 61.8 – – – –\\nRT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\\nYOLOv9-M (S) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nGold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\\nPPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nRTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\\nYOLOv9-C (S) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nDAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nRT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\\nRT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nYOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\\nYOLOv9-E (S) 35.0 148.1 54.5 71.7 59.2 38.1 59.9 70.3\\nRT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\\nGold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\\nRT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nDAMO YOLO-L [75] (D) 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nYOLOv9-E (S) 44.8 187.0 55.1 72.3 60.7 38.7 60.6 71.4\\nPPYOLOE+-L [74] (C) 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE-L [74] (I) 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nRTMDet-L [44] (I) 52.3 160.4 51.5 68.8 – – – –\\nYOLOR-CSP [66] (C) 52.9 120.4 52.8 71.2 57.6 – – –\\nYOLOv9-E (S) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4\\nYOLOv6-L v3.0 [30] (D) 59.6 150.7 52.8 70.3 – – – –\\nRT DETR-X [43] (I) 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nGold YOLO-L [61] (C) 75.1 151.7 53.3 70.9 – – – –\\nGold YOLO-L [61] (D) 75.1 151.7 53.2 70.5 – – – –\\nGold YOLO-L [61] (I) 75.1 151.7 52.3 69.6 – – – –\\nRT DETR-R101 [43] (I) 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nRTMDet-X [44] (I) 94.9 283.4 52.8 70.4 – – – –\\nYOLOR-CSP-X [66] (C) 96.9 226.8 54.8 73.1 59.7 – – –\\nPPYOLOE+-X [74] (C) 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nPPYOLOE-X [74] (I) 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\n1(S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.\\nTable 4 shows the performance of all models sorted by\\nparameter size. Our proposed YOLOv9 is Pareto optimal\\nin all models of different sizes. Among them, we found no\\nother method for Pareto optimal in models with more than\\n20M parameters. The above experimental data shows that\\nour YOLOv9 has excellent parameter usage efficiency.Shown in Table 5 is the performance of all participat-\\ning models sorted by the amount of computation. Our pro-\\nposed YOLOv9 is Pareto optimal in all models with differ-\\nent scales. Among models with more than 60 GFLOPs, only\\nELAN-based DAMO-YOLO and DETR-based RT DETR\\ncan rival the proposed YOLOv9. The above comparison\\nresults show that YOLOv9 has the most outstanding per-\\nformance in the trade-off between computation complexity\\nand accuracy.\\n3', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 16}),\n",
       " Document(page_content='Table 5. Comparison of state-of-the-art object detectors with different training settings (sorted by amount of computation).\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv6-N v3.0 [30] (D) 4.7 11.4 37.5 53.1 – – – –\\nGold YOLO-N [61] (D) 5.6 12.1 39.9 55.9 – – – –\\nRTMDet-T [44] (I) 4.8 12.6 41.1 57.9 – – – –\\nPPYOLOE+-S [74] (C) 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE-S [74] (I) 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nDAMO YOLO-T [75] (D) 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nRTMDet-S [44] (I) 9.0 25.6 44.6 61.9 – – – –\\nYOLOv9-S (S) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nDAMO YOLO-S [75] (D) 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nYOLOv6-S v3.0 [30] (D) 18.5 45.3 45.0 61.8 – – – –\\nGold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\\nPPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nGold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\\nRT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\\nDAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nYOLOv9-M (S) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nRTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\\nYOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\\nRT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\\nDAMO YOLO-L [75] (D) 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nRT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\\nYOLOv9-C (S) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nRT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nPPYOLOE+-L [74] (C) 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE-L [74] (I) 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nYOLOR-CSP [66] (C) 52.9 120.4 52.8 71.2 57.6 – – –\\nRT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nYOLOv9-E (S) 35.0 148.1 54.5 71.7 59.2 38.1 59.9 70.3\\nYOLOv6-L v3.0 [30] (D) 59.6 150.7 52.8 70.3 – – – –\\nGold YOLO-L [61] (C) 75.1 151.7 53.3 70.9 – – – –\\nGold YOLO-L [61] (D) 75.1 151.7 53.2 70.5 – – – –\\nGold YOLO-L [61] (I) 75.1 151.7 52.3 69.6 – – – –\\nRTMDet-L [44] (I) 52.3 160.4 51.5 68.8 – – – –\\nDy-YOLOv7 [36] (S) – 181.7 53.9 72.2 58.7 35.3 57.6 66.4\\nYOLOv9-E (S) 44.8 187.0 55.1 72.3 60.7 38.7 60.6 71.4\\nYOLOv9-E (S) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4\\nPPYOLOE+-X [74] (C) 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nPPYOLOE-X [74] (I) 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\nYOLOR-CSP-X [66] (C) 96.9 226.8 54.8 73.1 59.7 – – –\\nRT DETR-X [43] (I) 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nRT DETR-R101 [43] (I) 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nRTMDet-X [44] (I) 94.9 283.4 52.8 70.4 – – – –\\nDy-YOLOv7-X [36] (S) – 307.9 55.0 73.2 60.0 36.6 58.7 68.5\\n1(S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.\\n4', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 17})]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "\n",
    "for link in links:\n",
    "    if link != None:\n",
    "\n",
    "        loader = PyPDFLoader(link)\n",
    "        # all_documents.extend(loader.load_and_split())\n",
    "        # all_documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='YOLOv9: Learning What You Want to Learn\\nUsing Programmable Gradient Information\\nChien-Yao Wang1,2, I-Hau Yeh2, and Hong-Yuan Mark Liao1,2,3\\n1Institute of Information Science, Academia Sinica, Taiwan\\n2National Taipei University of Technology, Taiwan\\n3Department of Information and Computer Engineering, Chung Yuan Christian University, Taiwan\\nkinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, and liao@iis.sinica.edu.tw\\nAbstract\\nToday’s deep learning methods focus on how to design\\nthe most appropriate objective functions so that the pre-\\ndiction results of the model can be closest to the ground\\ntruth. Meanwhile, an appropriate architecture that can\\nfacilitate acquisition of enough information for prediction\\nhas to be designed. Existing methods ignore a fact that\\nwhen input data undergoes layer-by-layer feature extrac-\\ntion and spatial transformation, large amount of informa-\\ntion will be lost. This paper will delve into the important is-\\nsues of data loss when data is transmitted through deep net-\\nworks, namely information bottleneck and reversible func-\\ntions. We proposed the concept of programmable gradi-\\nent information (PGI) to cope with the various changes\\nrequired by deep networks to achieve multiple objectives.\\nPGI can provide complete input information for the tar-\\nget task to calculate objective function, so that reliable\\ngradient information can be obtained to update network\\nweights. In addition, a new lightweight network architec-\\nture – Generalized Efficient Layer Aggregation Network\\n(GELAN), based on gradient path planning is designed.\\nGELAN’s architecture confirms that PGI has gained su-\\nperior results on lightweight models. We verified the pro-\\nposed GELAN and PGI on MS COCO dataset based ob-\\nject detection. The results show that GELAN only uses\\nconventional convolution operators to achieve better pa-\\nrameter utilization than the state-of-the-art methods devel-\\noped based on depth-wise convolution. PGI can be used\\nfor variety of models from lightweight to large. It can be\\nused to obtain complete information, so that train-from-\\nscratch models can achieve better results than state-of-the-\\nart models pre-trained using large datasets, the compari-\\nson results are shown in Figure 1. The source codes are at:\\nhttps://github.com/WongKinYiu/yolov9 .\\n1. Introduction\\nDeep learning-based models have demonstrated far bet-\\nter performance than past artificial intelligence systems in\\nvarious fields, such as computer vision, language process-\\ning, and speech recognition. In recent years, researchers\\nFigure 1. Comparisons of the real-time object detecors on MS\\nCOCO dataset. The GELAN and PGI-based object detection\\nmethod surpassed all previous train-from-scratch methods in terms\\nof object detection performance. In terms of accuracy, the new\\nmethod outperforms RT DETR [43] pre-trained with a large\\ndataset, and it also outperforms depth-wise convolution-based de-\\nsign YOLO MS [7] in terms of parameters utilization.\\nin the field of deep learning have mainly focused on how\\nto develop more powerful system architectures and learn-\\ning methods, such as CNNs [21–23, 42, 55, 71, 72], Trans-\\nformers [8, 9, 40, 41, 60, 69, 70], Perceivers [26, 26, 32, 52,\\n56, 81, 81], and Mambas [17, 38, 80]. In addition, some\\nresearchers have tried to develop more general objective\\nfunctions, such as loss function [5, 45, 46, 50, 77, 78], la-\\nbel assignment [10, 12, 33, 67, 79] and auxiliary supervi-\\nsion [18, 20, 24, 28, 29, 51, 54, 68, 76]. The above studies\\nall try to precisely find the mapping between input and tar-\\nget tasks. However, most past approaches have ignored that\\ninput data may have a non-negligible amount of informa-\\ntion loss during the feedforward process. This loss of in-\\nformation can lead to biased gradient flows, which are sub-\\nsequently used to update the model. The above problems\\ncan result in deep networks to establish incorrect associa-\\ntions between targets and inputs, causing the trained model\\nto produce incorrect predictions.\\n1arXiv:2402.13616v1  [cs.CV]  21 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 0}),\n",
       " Document(page_content='Figure 2. Visualization results of random initial weight output feature maps for different network architectures: (a) input image, (b)\\nPlainNet, (c) ResNet, (d) CSPNet, and (e) proposed GELAN. From the figure, we can see that in different architectures, the information\\nprovided to the objective function to calculate the loss is lost to varying degrees, and our architecture can retain the most complete\\ninformation and provide the most reliable gradient information for calculating the objective function.\\nIn deep networks, the phenomenon of input data losing\\ninformation during the feedforward process is commonly\\nknown as information bottleneck [59], and its schematic di-\\nagram is as shown in Figure 2. At present, the main meth-\\nods that can alleviate this phenomenon are as follows: (1)\\nThe use of reversible architectures [3, 16, 19]: this method\\nmainly uses repeated input data and maintains the informa-\\ntion of the input data in an explicit way; (2) The use of\\nmasked modeling [1, 6, 9, 27, 71, 73]: it mainly uses recon-\\nstruction loss and adopts an implicit way to maximize the\\nextracted features and retain the input information; and (3)\\nIntroduction of the deep supervision concept [28,51,54,68]:\\nit uses shallow features that have not lost too much impor-\\ntant information to pre-establish a mapping from features\\nto targets to ensure that important information can be trans-\\nferred to deeper layers. However, the above methods have\\ndifferent drawbacks in the training process and inference\\nprocess. For example, a reversible architecture requires ad-\\nditional layers to combine repeatedly fed input data, which\\nwill significantly increase the inference cost. In addition,\\nsince the input data layer to the output layer cannot have a\\ntoo deep path, this limitation will make it difficult to model\\nhigh-order semantic information during the training pro-\\ncess. As for masked modeling, its reconstruction loss some-\\ntimes conflicts with the target loss. In addition, most mask\\nmechanisms also produce incorrect associations with data.\\nFor the deep supervision mechanism, it will produce error\\naccumulation, and if the shallow supervision loses informa-\\ntion during the training process, the subsequent layers will\\nnot be able to retrieve the required information. The above\\nphenomenon will be more significant on difficult tasks and\\nsmall models.\\nTo address the above-mentioned issues, we propose a\\nnew concept, which is programmable gradient information\\n(PGI). The concept is to generate reliable gradients through\\nauxiliary reversible branch, so that the deep features can\\nstill maintain key characteristics for executing target task.\\nThe design of auxiliary reversible branch can avoid the se-\\nmantic loss that may be caused by a traditional deep super-\\nvision process that integrates multi-path features. In other\\nwords, we are programming gradient information propaga-\\ntion at different semantic levels, and thereby achieving the\\nbest training results. The reversible architecture of PGI isbuilt on auxiliary branch, so there is no additional cost.\\nSince PGI can freely select loss function suitable for the\\ntarget task, it also overcomes the problems encountered by\\nmask modeling. The proposed PGI mechanism can be ap-\\nplied to deep neural networks of various sizes and is more\\ngeneral than the deep supervision mechanism, which is only\\nsuitable for very deep neural networks.\\nIn this paper, we also designed generalized ELAN\\n(GELAN) based on ELAN [65], the design of GELAN si-\\nmultaneously takes into account the number of parameters,\\ncomputational complexity, accuracy and inference speed.\\nThis design allows users to arbitrarily choose appropriate\\ncomputational blocks for different inference devices. We\\ncombined the proposed PGI and GELAN, and then de-\\nsigned a new generation of YOLO series object detection\\nsystem, which we call YOLOv9. We used the MS COCO\\ndataset to conduct experiments, and the experimental results\\nverified that our proposed YOLOv9 achieved the top perfor-\\nmance in all comparisons.\\nWe summarize the contributions of this paper as follows:\\n1. We theoretically analyzed the existing deep neural net-\\nwork architecture from the perspective of reversible\\nfunction, and through this process we successfully ex-\\nplained many phenomena that were difficult to explain\\nin the past. We also designed PGI and auxiliary re-\\nversible branch based on this analysis and achieved ex-\\ncellent results.\\n2. The PGI we designed solves the problem that deep su-\\npervision can only be used for extremely deep neu-\\nral network architectures, and therefore allows new\\nlightweight architectures to be truly applied in daily\\nlife.\\n3. The GELAN we designed only uses conventional con-\\nvolution to achieve a higher parameter usage than the\\ndepth-wise convolution design that based on the most\\nadvanced technology, while showing great advantages\\nof being light, fast, and accurate.\\n4. Combining the proposed PGI and GELAN, the object\\ndetection performance of the YOLOv9 on MS COCO\\ndataset greatly surpasses the existing real-time object\\ndetectors in all aspects.\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 1}),\n",
       " Document(page_content='2. Related work\\n2.1. Real-time Object Detectors\\nThe current mainstream real-time object detectors are the\\nYOLO series [2, 7, 13–15, 25, 30, 31, 47–49, 61–63, 74, 75],\\nand most of these models use CSPNet [64] or ELAN [65]\\nand their variants as the main computing units. In terms of\\nfeature integration, improved PAN [37] or FPN [35] is of-\\nten used as a tool, and then improved YOLOv3 head [49] or\\nFCOS head [57, 58] is used as prediction head. Recently\\nsome real-time object detectors, such as RT DETR [43],\\nwhich puts its fundation on DETR [4], have also been pro-\\nposed. However, since it is extremely difficult for DETR\\nseries object detector to be applied to new domains without\\na corresponding domain pre-trained model, the most widely\\nused real-time object detector at present is still YOLO se-\\nries. This paper chooses YOLOv7 [63], which has been\\nproven effective in a variety of computer vision tasks and\\nvarious scenarios, as a base to develop the proposed method.\\nWe use GELAN to improve the architecture and the training\\nprocess with the proposed PGI. The above novel approach\\nmakes the proposed YOLOv9 the top real-time object de-\\ntector of the new generation.\\n2.2. Reversible Architectures\\nThe operation unit of reversible architectures [3, 16, 19]\\nmust maintain the characteristics of reversible conversion,\\nso it can be ensured that the output feature map of each\\nlayer of operation unit can retain complete original informa-\\ntion. Before, RevCol [3] generalizes traditional reversible\\nunit to multiple levels, and in doing so can expand the se-\\nmantic levels expressed by different layer units. Through\\na literature review of various neural network architectures,\\nwe found that there are many high-performing architectures\\nwith varying degree of reversible properties. For exam-\\nple, Res2Net module [11] combines different input parti-\\ntions with the next partition in a hierarchical manner, and\\nconcatenates all converted partitions before passing them\\nbackwards. CBNet [34, 39] re-introduces the original in-\\nput data through composite backbone to obtain complete\\noriginal information, and obtains different levels of multi-\\nlevel reversible information through various composition\\nmethods. These network architectures generally have ex-\\ncellent parameter utilization, but the extra composite layers\\ncause slow inference speeds. DynamicDet [36] combines\\nCBNet [34] and the high-efficiency real-time object detec-\\ntor YOLOv7 [63] to achieve a very good trade-off among\\nspeed, number of parameters, and accuracy. This paper in-\\ntroduces the DynamicDet architecture as the basis for de-\\nsigning reversible branches. In addition, reversible infor-\\nmation is further introduced into the proposed PGI. The\\nproposed new architecture does not require additional con-\\nnections during the inference process, so it can fully retain\\nthe advantages of speed, parameter amount, and accuracy.2.3. Auxiliary Supervision\\nDeep supervision [28,54,68] is the most common auxil-\\niary supervision method, which performs training by insert-\\ning additional prediction layers in the middle layers. Es-\\npecially the application of multi-layer decoders introduced\\nin the transformer-based methods is the most common one.\\nAnother common auxiliary supervision method is to utilize\\nthe relevant meta information to guide the feature maps pro-\\nduced by the intermediate layers and make them have the\\nproperties required by the target tasks [18, 20, 24, 29, 76].\\nExamples of this type include using segmentation loss or\\ndepth loss to enhance the accuracy of object detectors. Re-\\ncently, there are many reports in the literature [53, 67, 82]\\nthat use different label assignment methods to generate dif-\\nferent auxiliary supervision mechanisms to speed up the\\nconvergence speed of the model and improve the robustness\\nat the same time. However, the auxiliary supervision mech-\\nanism is usually only applicable to large models, so when\\nit is applied to lightweight models, it is easy to cause an\\nunder parameterization phenomenon, which makes the per-\\nformance worse. The PGI we proposed designed a way to\\nreprogram multi-level semantic information, and this design\\nallows lightweight models to also benefit from the auxiliary\\nsupervision mechanism.\\n3. Problem Statement\\nUsually, people attribute the difficulty of deep neural net-\\nwork convergence problem due to factors such as gradient\\nvanish or gradient saturation, and these phenomena do ex-\\nist in traditional deep neural networks. However, modern\\ndeep neural networks have already fundamentally solved\\nthe above problem by designing various normalization and\\nactivation functions. Nevertheless, deep neural networks\\nstill have the problem of slow convergence or poor conver-\\ngence results.\\nIn this paper, we explore the nature of the above issue\\nfurther. Through in-depth analysis of information bottle-\\nneck, we deduced that the root cause of this problem is that\\nthe initial gradient originally coming from a very deep net-\\nwork has lost a lot of information needed to achieve the\\ngoal soon after it is transmitted. In order to confirm this\\ninference, we feedforward deep networks of different archi-\\ntectures with initial weights, and then visualize and illus-\\ntrate them in Figure 2. Obviously, PlainNet has lost a lot of\\nimportant information required for object detection in deep\\nlayers. As for the proportion of important information that\\nResNet, CSPNet, and GELAN can retain, it is indeed posi-\\ntively related to the accuracy that can be obtained after train-\\ning. We further design reversible network-based methods to\\nsolve the causes of the above problems. In this section we\\nshall elaborate our analysis of information bottleneck prin-\\nciple and reversible functions.\\n3', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 2}),\n",
       " Document(page_content='3.1. Information Bottleneck Principle\\nAccording to information bottleneck principle, we know\\nthat data Xmay cause information loss when going through\\ntransformation, as shown in Eq. 1 below:\\nI(X, X )≥I(X, f θ(X))≥I(X, gϕ(fθ(X))),(1)\\nwhere Iindicates mutual information, fandgare transfor-\\nmation functions, and θandϕare parameters of fandg,\\nrespectively.\\nIn deep neural networks, fθ(·)andgϕ(·)respectively\\nrepresent the operations of two consecutive layers in deep\\nneural network. From Eq. 1, we can predict that as the num-\\nber of network layer becomes deeper, the original data will\\nbe more likely to be lost. However, the parameters of the\\ndeep neural network are based on the output of the network\\nas well as the given target, and then update the network after\\ngenerating new gradients by calculating the loss function.\\nAs one can imagine, the output of a deeper neural network\\nis less able to retain complete information about the pre-\\ndiction target. This will make it possible to use incomplete\\ninformation during network training, resulting in unreliable\\ngradients and poor convergence.\\nOne way to solve the above problem is to directly in-\\ncrease the size of the model. When we use a large number\\nof parameters to construct a model, it is more capable of\\nperforming a more complete transformation of the data. The\\nabove approach allows even if information is lost during the\\ndata feedforward process, there is still a chance to retain\\nenough information to perform the mapping to the target.\\nThe above phenomenon explains why the width is more im-\\nportant than the depth in most modern models. However,\\nthe above conclusion cannot fundamentally solve the prob-\\nlem of unreliable gradients in very deep neural network.\\nBelow, we will introduce how to use reversible functions\\nto solve problems and conduct relative analysis.\\n3.2. Reversible Functions\\nWhen a function rhas an inverse transformation func-\\ntionv, we call this function reversible function, as shown in\\nEq. 2.\\nX=vζ(rψ(X)), (2)\\nwhere ψandζare parameters of randv, respectively. Data\\nXis converted by reversible function without losing infor-\\nmation, as shown in Eq. 3.\\nI(X, X ) =I(X, rψ(X)) =I(X, v ζ(rψ(X))).(3)\\nWhen the network’s transformation function is composed\\nof reversible functions, more reliable gradients can be ob-\\ntained to update the model. Almost all of today’s populardeep learning methods are architectures that conform to the\\nreversible property, such as Eq. 4.\\nXl+1=Xl+fl+1\\nθ(Xl), (4)\\nwhere lindicates the l-th layer of a PreAct ResNet and\\nfis the transformation function of the l-th layer. PreAct\\nResNet [22] repeatedly passes the original data Xto sub-\\nsequent layers in an explicit way. Although such a design\\ncan make a deep neural network with more than a thousand\\nlayers converge very well, it destroys an important reason\\nwhy we need deep neural networks. That is, for difficult\\nproblems, it is difficult for us to directly find simple map-\\nping functions to map data to targets. This also explains\\nwhy PreAct ResNet performs worse than ResNet [21] when\\nthe number of layers is small.\\nIn addition, we tried to use masked modeling that al-\\nlowed the transformer model to achieve significant break-\\nthroughs. We use approximation methods, such as Eq. 5,\\nto try to find the inverse transformation vofr, so that the\\ntransformed features can retain enough information using\\nsparse features. The form of Eq. 5 is as follows:\\nX=vζ(rψ(X)·M), (5)\\nwhere Mis a dynamic binary mask. Other methods that\\nare commonly used to perform the above tasks are diffusion\\nmodel and variational autoencoder, and they both have the\\nfunction of finding the inverse function. However, when\\nwe apply the above approach to a lightweight model, there\\nwill be defects because the lightweight model will be under\\nparameterized to a large amount of raw data. Because of\\nthe above reason, important information I(Y, X)that maps\\ndataXto target Ywill also face the same problem. For this\\nissue, we will explore it using the concept of information\\nbottleneck [59]. The formula for information bottleneck is\\nas follows:\\nI(X, X )≥I(Y, X)≥I(Y, fθ(X))≥...≥I(Y,ˆY).(6)\\nGenerally speaking, I(Y, X)will only occupy a very small\\npart of I(X, X ). However, it is critical to the target mis-\\nsion. Therefore, even if the amount of information lost in\\nthe feedforward stage is not significant, as long as I(Y, X)\\nis covered, the training effect will be greatly affected. The\\nlightweight model itself is in an under parameterized state,\\nso it is easy to lose a lot of important information in the\\nfeedforward stage. Therefore, our goal for the lightweight\\nmodel is how to accurately filter I(Y, X)fromI(X, X ). As\\nfor fully preserving the information of X, that is difficult to\\nachieve. Based on the above analysis, we hope to propose a\\nnew deep neural network training method that can not only\\ngenerate reliable gradients to update the model, but also be\\nsuitable for shallow and lightweight neural networks.\\n4', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 3. PGI and related network architectures and methods. (a) Path Aggregation Network (PAN)) [37], (b) Reversible Columns\\n(RevCol) [3], (c) conventional deep supervision, and (d) our proposed Programmable Gradient Information (PGI). PGI is mainly composed\\nof three components: (1) main branch: architecture used for inference, (2) auxiliary reversible branch: generate reliable gradients to supply\\nmain branch for backward transmission, and (3) multi-level auxiliary information: control main branch learning plannable multi-level of\\nsemantic information.\\n4. Methodology\\n4.1. Programmable Gradient Information\\nIn order to solve the aforementioned problems, we pro-\\npose a new auxiliary supervision framework called Pro-\\ngrammable Gradient Information (PGI), as shown in Fig-\\nure 3 (d). PGI mainly includes three components, namely\\n(1) main branch, (2) auxiliary reversible branch, and (3)\\nmulti-level auxiliary information. From Figure 3 (d) we\\nsee that the inference process of PGI only uses main branch\\nand therefore does not require any additional inference cost.\\nAs for the other two components, they are used to solve or\\nslow down several important issues in deep learning meth-\\nods. Among them, auxiliary reversible branch is designed\\nto deal with the problems caused by the deepening of neural\\nnetworks. Network deepening will cause information bot-\\ntleneck, which will make the loss function unable to gener-\\nate reliable gradients. As for multi-level auxiliary informa-\\ntion, it is designed to handle the error accumulation problem\\ncaused by deep supervision, especially for the architecture\\nand lightweight model of multiple prediction branch. Next,\\nwe will introduce these two components step by step.\\n4.1.1 Auxiliary Reversible Branch\\nIn PGI, we propose auxiliary reversible branch to gener-\\nate reliable gradients and update network parameters. By\\nproviding information that maps from data to targets, the\\nloss function can provide guidance and avoid the possibil-\\nity of finding false correlations from incomplete feedfor-\\nward features that are less relevant to the target. We pro-pose the maintenance of complete information by introduc-\\ning reversible architecture, but adding main branch to re-\\nversible architecture will consume a lot of inference costs.\\nWe analyzed the architecture of Figure 3 (b) and found that\\nwhen additional connections from deep to shallow layers\\nare added, the inference time will increase by 20%. When\\nwe repeatedly add the input data to the high-resolution com-\\nputing layer of the network (yellow box), the inference time\\neven exceeds twice the time.\\nSince our goal is to use reversible architecture to ob-\\ntain reliable gradients, “reversible” is not the only neces-\\nsary condition in the inference stage. In view of this, we\\nregard reversible branch as an expansion of deep supervi-\\nsion branch, and then design auxiliary reversible branch, as\\nshown in Figure 3 (d). As for the main branch deep fea-\\ntures that would have lost important information due to in-\\nformation bottleneck, they will be able to receive reliable\\ngradient information from the auxiliary reversible branch.\\nThese gradient information will drive parameter learning to\\nassist in extracting correct and important information, and\\nthe above actions can enable the main branch to obtain fea-\\ntures that are more effective for the target task. Moreover,\\nthe reversible architecture performs worse on shallow net-\\nworks than on general networks because complex tasks re-\\nquire conversion in deeper networks. Our proposed method\\ndoes not force the main branch to retain complete origi-\\nnal information but updates it by generating useful gradient\\nthrough the auxiliary supervision mechanism. The advan-\\ntage of this design is that the proposed method can also be\\napplied to shallower networks.\\n5', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 4}),\n",
       " Document(page_content='Figure 4. The architecture of GELAN: (a) CSPNet [64], (b) ELAN [65], and (c) proposed GELAN. We imitate CSPNet and extend ELAN\\ninto GELAN that can support any computational blocks.\\nFinally, since auxiliary reversible branch can be removed\\nduring the inference phase, the inference capabilities of the\\noriginal network can be retained. We can also choose any\\nreversible architectures in PGI to play the role of auxiliary\\nreversible branch.\\n4.1.2 Multi-level Auxiliary Information\\nIn this section we will discuss how multi-level auxiliary in-\\nformation works. The deep supervision architecture includ-\\ning multiple prediction branch is shown in Figure 3 (c). For\\nobject detection, different feature pyramids can be used to\\nperform different tasks, for example together they can de-\\ntect objects of different sizes. Therefore, after connecting\\nto the deep supervision branch, the shallow features will be\\nguided to learn the features required for small object detec-\\ntion, and at this time the system will regard the positions\\nof objects of other sizes as the background. However, the\\nabove deed will cause the deep feature pyramids to lose a lot\\nof information needed to predict the target object. Regard-\\ning this issue, we believe that each feature pyramid needs\\nto receive information about all target objects so that subse-\\nquent main branch can retain complete information to learn\\npredictions for various targets.\\nThe concept of multi-level auxiliary information is to in-\\nsert an integration network between the feature pyramid hi-\\nerarchy layers of auxiliary supervision and the main branch,\\nand then uses it to combine returned gradients from differ-\\nent prediction heads, as shown in Figure 3 (d). Multi-level\\nauxiliary information is then to aggregate the gradient infor-\\nmation containing all target objects, and pass it to the main\\nbranch and then update parameters. At this time, the charac-\\nteristics of the main branch’s feature pyramid hierarchy will\\nnot be dominated by some specific object’s information. As\\na result, our method can alleviate the broken information\\nproblem in deep supervision. In addition, any integrated\\nnetwork can be used in multi-level auxiliary information.\\nTherefore, we can plan the required semantic levels to guide\\nthe learning of network architectures of different sizes.4.2. Generalized ELAN\\nIn this Section we describe the proposed new network\\narchitecture – GELAN. By combining two neural network\\narchitectures, CSPNet [64] and ELAN [65], which are de-\\nsigned with gradient path planning, we designed gener-\\nalized efficient layer aggregation network (GELAN) that\\ntakes into account lighweight, inference speed, and accu-\\nracy. Its overall architecture is shown in Figure 4. We gen-\\neralized the capability of ELAN [65], which originally only\\nused stacking of convolutional layers, to a new architecture\\nthat can use any computational blocks.\\n5. Experiments\\n5.1. Experimental Setup\\nWe verify the proposed method with MS COCO dataset.\\nAll experimental setups follow YOLOv7 AF [63], while the\\ndataset is MS COCO 2017 splitting. All models we men-\\ntioned are trained using the train-from-scratch strategy, and\\nthe total number of training times is 500 epochs. In setting\\nthe learning rate, we use linear warm-up in the first three\\nepochs, and the subsequent epochs set the corresponding\\ndecay manner according to the model scale. As for the last\\n15 epochs, we turn mosaic data augmentation off. For more\\nsettings, please refer to Appendix.\\n5.2. Implimentation Details\\nWe built general and extended version of YOLOv9 based\\non YOLOv7 [63] and Dynamic YOLOv7 [36] respectively.\\nIn the design of the network architecture, we replaced\\nELAN [65] with GELAN using CSPNet blocks [64] with\\nplanned RepConv [63] as computational blocks. We also\\nsimplified downsampling module and optimized anchor-\\nfree prediction head. As for the auxiliary loss part of PGI,\\nwe completely follow YOLOv7’s auxiliary head setting.\\nPlease see Appendix for more details.\\n6', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 5}),\n",
       " Document(page_content='Table 1. Comparison of state-of-the-art real-time object detectors.\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv5-N r7.0 [14] 1.9 4.5 28.0 45.7 – – – –\\nYOLOv5-S r7.0 [14] 7.2 16.5 37.4 56.8 – – – –\\nYOLOv5-M r7.0 [14] 21.2 49.0 45.4 64.1 – – – –\\nYOLOv5-L r7.0 [14] 46.5 109.1 49.0 67.3 – – – –\\nYOLOv5-X r7.0 [14] 86.7 205.7 50.7 68.9 – – – –\\nYOLOv6-N v3.0 [30] 4.7 11.4 37.0 52.7 – – – –\\nYOLOv6-S v3.0 [30] 18.5 45.3 44.3 61.2 – – – –\\nYOLOv6-M v3.0 [30] 34.9 85.8 49.1 66.1 – – – –\\nYOLOv6-L v3.0 [30] 59.6 150.7 51.8 69.2 – – – –\\nYOLOv7 [63] 36.9 104.7 51.2 69.7 55.9 31.8 55.5 65.0\\nYOLOv7-X [63] 71.3 189.9 52.9 71.1 51.4 36.9 57.7 68.6\\nYOLOv7-N AF [63] 3.1 8.7 37.6 53.3 40.6 18.7 41.7 52.8\\nYOLOv7-S AF [63] 11.0 28.1 45.1 61.8 48.9 25.7 50.2 61.2\\nYOLOv7 AF [63] 43.6 130.5 53.0 70.2 57.5 35.8 58.7 68.9\\nYOLOv8-N [15] 3.2 8.7 37.3 52.6 – – – –\\nYOLOv8-S [15] 11.2 28.6 44.9 61.8 – – – –\\nYOLOv8-M [15] 25.9 78.9 50.2 67.2 – – – –\\nYOLOv8-L [15] 43.7 165.2 52.9 69.8 57.5 35.3 58.3 69.8\\nYOLOv8-X [15] 68.2 257.8 53.9 71.0 58.7 35.7 59.3 70.7\\nDAMO YOLO-T [75] 8.5 18.1 42.0 58.0 45.2 23.0 46.1 58.5\\nDAMO YOLO-S [75] 12.3 37.8 46.0 61.9 49.5 25.9 50.6 62.5\\nDAMO YOLO-M [75] 28.2 61.8 49.2 65.5 53.0 29.7 53.1 66.1\\nDAMO YOLO-L [75] 42.1 97.3 50.8 67.5 55.5 33.2 55.7 66.6\\nGold YOLO-N [61] 5.6 12.1 39.6 55.7 – 19.7 44.1 57.0\\nGold YOLO-S [61] 21.5 46.0 45.4 62.5 – 25.3 50.2 62.6\\nGold YOLO-M [61] 41.3 87.5 49.8 67.0 – 32.3 55.3 66.3\\nGold YOLO-L [61] 75.1 151.7 51.8 68.9 – 34.1 57.4 68.2\\nYOLO MS-N [7] 4.5 17.4 43.4 60.4 47.6 23.7 48.3 60.3\\nYOLO MS-S [7] 8.1 31.2 46.2 63.7 50.5 26.9 50.5 63.0\\nYOLO MS [7] 22.2 80.2 51.0 68.6 55.7 33.1 56.1 66.5\\nGELAN-S (Ours) 7.2 26.7 46.7 63.0 50.7 25.9 51.5 64.0\\nGELAN-M (Ours) 20.1 76.8 51.1 67.9 55.7 33.6 56.4 67.3\\nGELAN-C (Ours) 25.5 102.8 52.5 69.5 57.3 35.8 57.6 69.4\\nGELAN-E (Ours) 58.1 192.5 55.0 71.9 60.0 38.0 60.6 70.9\\nYOLOv9-S (Ours) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nYOLOv9-M (Ours) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nYOLOv9-C (Ours) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nYOLOv9-E (Ours) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4\\n5.3. Comparison with state-of-the-arts\\nTable 1 lists comparison of our proposed YOLOv9 with\\nother train-from-scratch real-time object detectors. Over-\\nall, the best performing methods among existing methods\\nare YOLO MS-S [7] for lightweight models, YOLO MS [7]\\nfor medium models, YOLOv7 AF [63] for general mod-\\nels, and YOLOv8-X [15] for large models. Compared with\\nlightweight and medium model YOLO MS [7], YOLOv9\\nhas about 10% less parameters and 5 ∼15% less calcula-\\ntions, but still has a 0.4 ∼0.6% improvement in AP. Com-\\npared with YOLOv7 AF, YOLOv9-C has 42% less pa-\\nrameters and 21% less calculations, but achieves the same\\nAP (53%). Compared with YOLOv8-X, YOLOv9-X has\\n15% less parameters, 25% less calculations, and has sig-\\nnificant improvement of 1.7% AP. The above comparison\\nresults show that our proposed YOLOv9 has significantlyimproved in all aspects compared with existing methods.\\nOn the other hand, we also include ImageNet pretrained\\nmodel in the comparison, and the results are shown in Fig-\\nure 5. We compare them based on the parameters and the\\namount of computation respectively. In terms of the num-\\nber of parameters, the best performing large model is RT\\nDETR [43]. From Figure 5, we can see that YOLOv9 using\\nconventional convolution is even better than YOLO MS us-\\ning depth-wise convolution in parameter utilization. As for\\nthe parameter utilization of large models, it also greatly sur-\\npasses RT DETR using ImageNet pretrained model. Even\\nbetter is that in the deep model, YOLOv9 shows the huge\\nadvantages of using PGI. By accurately retaining and ex-\\ntracting the information needed to map the data to the tar-\\nget, our method requires only 64% of the parameters while\\nmaintaining the accuracy as RT DETR-X.\\n7', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 6}),\n",
       " Document(page_content='Figure 5. Comparison of state-of-the-art real-time object detectors. The methods participating in the comparison all use ImageNet as\\npre-trained weights, including RT DETR [43], RTMDet [44], and PP-YOLOE [74], etc. The YOLOv9 that uses train-from-scratch method\\nclearly surpasses the performance of other methods.\\nAs for the amount of computation, the best existing mod-\\nels from the smallest to the largest are YOLO MS [7], PP\\nYOLOE [74], and RT DETR [43]. From Figure 5, we can\\nsee that YOLOv9 is far superior to the train-from-scratch\\nmethods in terms of computational complexity. In addi-\\ntion, if compared with those based on depth-wise convo-\\nlution and ImageNet-based pretrained models, YOLOv9 is\\nalso very competitive.\\n5.4. Ablation Studies\\n5.4.1 Generalized ELAN\\nFor GELAN, we first do ablation studies for computational\\nblocks. We used Res blocks [21], Dark blocks [49], and\\nCSP blocks [64] to conduct experiments, respectively. Ta-\\nble 2 shows that after replacing convolutional layers in\\nELAN with different computational blocks, the system can\\nmaintain good performance. Users are indeed free to re-\\nplace computational blocks and use them on their respective\\ninference devices. Among different computational block re-\\nplacements, CSP blocks perform particularly well. They\\nnot only reduce the amount of parameters and computation,\\nbut also improve AP by 0.7%. Therefore, we choose CSP-\\nELAN as the component unit of GELAN in YOLOv9.\\nTable 2. Ablation study on various computational blocks.\\nModel CB type #Param. FLOPs APval\\n50:95\\nGELAN-S Conv 6.3M 24.0G 44.8%\\nGELAN-S Res [21] 5.5M 21.1G 44.3%\\nGELAN-S Dark [49] 5.7M 21.8G 44.5%\\nGELAN-S CSP [64] 5.9M 22.5G 45.5%\\n1CB type nedotes as computational block type.\\n2-S nedotes small size model.Next, we conduct ELAN block-depth and CSP block-\\ndepth experiments on GELAN of different sizes, and dis-\\nplay the results in Table 3. We can see that when the depth\\nof ELAN is increased from 1 to 2, the accuracy is signif-\\nicantly improved. But when the depth is greater than or\\nequal to 2, no matter it is improving the ELAN depth or the\\nCSP depth, the number of parameters, the amount of com-\\nputation, and the accuracy will always show a linear rela-\\ntionship. This means GELAN is not sensitive to the depth.\\nIn other words, users can arbitrarily combine the compo-\\nnents in GELAN to design the network architecture, and\\nhave a model with stable performance without special de-\\nsign. In Table 3, for YOLOv9- {S,M,C}, we set the pairing\\nof the ELAN depth and the CSP depth to {{2, 3},{2, 1},\\n{2, 1}}.\\nTable 3. Ablation study on ELAN and CSP depth.\\nModel D ELAN DCSP #Param. FLOPs APval\\n50:95\\nGELAN-S 2 1 5.9M 22.5G 45.5%\\nGELAN-S 2 2 6.6M 24.6G 46.0%\\nGELAN-S 3 1 7.1M 26.4G 46.5%\\nGELAN-S 2 3 7.2M 26.7G 46.7%\\nGELAN-M 2 1 20.1M 76.8G 51.1%\\nGELAN-M 2 2 22.4M 86.1G 51.7%\\nGELAN-M 3 1 24.5M 94.2G 51.8%\\nGELAN-M 2 3 24.8M 95.5G 52.3%\\nGELAN-C 1 1 19.0M 77.8G 50.7%\\nGELAN-C 2 1 25.5M 102.8G 52.5%\\nGELAN-C 2 2 28.9M 115.8G 53.0%\\nGELAN-C 3 1 32.0M 127.9G 53.2%\\nGELAN-C 2 3 32.4M 128.7G 53.3%\\n1DELAN andDCSP respectively nedotes depth of ELAN and CSP.\\n2-{S, M, C }indicate small, medium, and compact models.\\n8', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 7}),\n",
       " Document(page_content='5.4.2 Programmable Gradient Information\\nIn terms of PGI, we performed ablation studies on auxiliary\\nreversible branch and multi-level auxiliary information on\\nthe backbone and neck, respectively. We designed auxiliary\\nreversible branch ICN to use DHLC [34] linkage to obtain\\nmulti-level reversible information. As for multi-level aux-\\niliary information, we use FPN and PAN for ablation stud-\\nies and the role of PFH is equivalent to the traditional deep\\nsupervision. The results of all experiments are listed in Ta-\\nble 4. From Table 4, we can see that PFH is only effective in\\ndeep models, while our proposed PGI can improve accuracy\\nunder different combinations. Especially when using ICN,\\nwe get stable and better results. We also tried to apply the\\nlead-head guided assignment proposed in YOLOv7 [63] to\\nthe PGI’s auxiliary supervision, and achieved much better\\nperformance.\\nTable 4. Ablation study on PGI of backbone and neck.\\nModel G backbone Gneck APval\\n50:95APval\\nSAPval\\nMAPval\\nL\\nGELAN-C – – 52.5% 35.8% 57.6% 69.4%\\nGELAN-C PFH – 52.5% 35.3% 58.1% 68.9%\\nGELAN-C FPN – 52.6% 35.3% 58.1% 68.9%\\nGELAN-C – ICN 52.7% 35.3% 58.4% 68.9%\\nGELAN-C FPN ICN 52.8% 35.8% 58.2% 69.1%\\nGELAN-C ICN – 52.9% 35.2% 58.7% 68.6%\\nGELAN-C LHG-ICN – 53.0% 36.3% 58.5% 69.1%\\nGELAN-E – – 55.0% 38.0% 60.6% 70.9%\\nGELAN-E PFH – 55.3% 38.3% 60.3% 71.6%\\nGELAN-E FPN – 55.6% 40.2% 61.0% 71.4%\\nGELAN-E PAN – 55.5% 39.0% 61.1% 71.5%\\nGELAN-E FPN ICN 55.6% 39.8% 60.9% 71.9%\\n1DELAN andDCSP respectively nedotes depth of ELAN and CSP.\\n2LHG indicates lead head guided training proposed by YOLOv7 [63].\\nWe further implemented the concepts of PGI and deep\\nsupervision on models of various sizes and compared the\\nresults, these results are shown in Table 5. As analyzed at\\nthe beginning, introduction of deep supervision will cause\\na loss of accuracy for shallow models. As for general mod-\\nels, introducing deep supervision will cause unstable perfor-\\nmance, and the design concept of deep supervision can only\\nbring gains in extremely deep models. The proposed PGI\\ncan effectively handle problems such as information bottle-\\nneck and information broken, and can comprehensively im-\\nprove the accuracy of models of different sizes. The concept\\nof PGI brings two valuable contributions. The first one is to\\nmake the auxiliary supervision method applicable to shal-\\nlow models, while the second one is to make the deep model\\ntraining process obtain more reliable gradients. These gra-\\ndients enable deep models to use more accurate information\\nto establish correct correlations between data and targets.Table 5. Ablation study on PGI.\\nModel APval\\n50:95APval\\n50APval\\n75\\nGELAN-S 46.7% 63.0% 50.7%\\n+ DS 46.5% -0.2 62.9% -0.1 50.5% -0.2\\n+ PGI 46.8% +0.1 63.4% +0.4 50.7% =\\nGELAN-M 51.1% 67.9% 55.7%\\n+ DS 51.2% +0.1 68.2% +0.3 55.7% =\\n+ PGI 51.4% +0.3 68.1% +0.2 56.1% +0.4\\nGELAN-C 52.5% 69.5% 57.3%\\n+ DS 52.5% = 69.9% +0.4 57.1% -0.2\\n+ PGI 53.0% +0.5 70.3% +0.8 57.8% +0.5\\nGELAN-E 55.0% 71.9% 60.0%\\n+ DS 55.3% +0.3 72.3% +0.4 60.2% +0.2\\n+ PGI 55.6% +0.6 72.8% +0.9 60.6% +0.6\\n1DS indicates deep supervision.\\nFinally, we show in the table the results of gradually in-\\ncreasing components from baseline YOLOv7 to YOLOv9-\\nE. The GELAN and PGI we proposed have brought all-\\nround improvement to the model.\\nTable 6. Ablation study on GELAN and PGI.\\nModel #Param. FLOPs APval\\n50:95APval\\nSAPval\\nMAPval\\nL\\nYOLOv7 [63] 36.9 104.7 51.2% 31.8% 55.5% 65.0%\\n+ AF [63] 43.6 130.5 53.0% 35.8% 58.7% 68.9%\\n+ GELAN 41.7 127.9 53.2% 36.2% 58.5% 69.9%\\n+ DHLC [34] 58.1 192.5 55.0% 38.0% 60.6% 70.9%\\n+ PGI 58.1 192.5 55.6% 40.2% 61.0% 71.4%\\n5.5. Visualization\\nThis section will explore the information bottleneck is-\\nsues and visualize them. In addition, we will also visualize\\nhow the proposed PGI uses reliable gradients to find the\\ncorrect correlations between data and targets. In Figure 6\\nwe show the visualization results of feature maps obtained\\nby using random initial weights as feedforward under dif-\\nferent architectures. We can see that as the number of lay-\\ners increases, the original information of all architectures\\ngradually decreases. For example, at the 50thlayer of the\\nPlainNet, it is difficult to see the location of objects, and all\\ndistinguishable features will be lost at the 100thlayer. As\\nfor ResNet, although the position of object can still be seen\\nat the 50thlayer, the boundary information has been lost.\\nWhen the depth reached to the 100thlayer, the whole image\\nbecomes blurry. Both CSPNet and the proposed GELAN\\nperform very well, and they both can maintain features that\\nsupport clear identification of objects until the 200thlayer.\\nAmong the comparisons, GELAN has more stable results\\nand clearer boundary information.\\n9', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 8}),\n",
       " Document(page_content='Figure 6. Feature maps (visualization results) output by random initial weights of PlainNet, ResNet, CSPNet, and GELAN at different\\ndepths. After 100 layers, ResNet begins to produce feedforward output that is enough to obfuscate object information. Our proposed\\nGELAN can still retain quite complete information up to the 150thlayer, and is still sufficiently discriminative up to the 200thlayer.\\nFigure 7. PAN feature maps (visualization results) of GELAN\\nand YOLOv9 (GELAN + PGI) after one epoch of bias warm-up.\\nGELAN originally had some divergence, but after adding PGI’s\\nreversible branch, it is more capable of focusing on the target ob-\\nject.\\nFigure 7 is used to show whether PGI can provide more\\nreliable gradients during the training process, so that the\\nparameters used for updating can effectively capture the\\nrelationship between the input data and the target. Fig-\\nure 7 shows the visualization results of the feature map of\\nGELAN and YOLOv9 (GELAN + PGI) in PAN bias warm-\\nup. From the comparison of Figure 7(b) and (c), we can\\nclearly see that PGI accurately and concisely captures the\\narea containing objects. As for GELAN that does not use\\nPGI, we found that it had divergence when detecting ob-ject boundaries, and it also produced unexpected responses\\nin some background areas. This experiment confirms that\\nPGI can indeed provide better gradients to update parame-\\nters and enable the feedforward stage of the main branch to\\nretain more important features.\\n6. Conclusions\\nIn this paper, we propose to use PGI to solve the infor-\\nmation bottleneck problem and the problem that the deep\\nsupervision mechanism is not suitable for lightweight neu-\\nral networks. We designed GELAN, a highly efficient\\nand lightweight neural network. In terms of object detec-\\ntion, GELAN has strong and stable performance at different\\ncomputational blocks and depth settings. It can indeed be\\nwidely expanded into a model suitable for various inference\\ndevices. For the above two issues, the introduction of PGI\\nallows both lightweight models and deep models to achieve\\nsignificant improvements in accuracy. The YOLOv9, de-\\nsigned by combining PGI and GELAN, has shown strong\\ncompetitiveness. Its excellent design allows the deep model\\nto reduce the number of parameters by 49% and the amount\\nof calculations by 43% compared with YOLOv8, but it still\\nhas a 0.6% AP improvement on MS COCO dataset.\\n7. Acknowledgements\\nThe authors wish to thank National Center for High-\\nperformance Computing (NCHC) for providing computa-\\ntional and storage resources.\\n10', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 9}),\n",
       " Document(page_content='References\\n[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT:\\nBERT pre-training of image transformers. In International\\nConference on Learning Representations (ICLR) , 2022. 2\\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\\nYuan Mark Liao. YOLOv4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934 , 2020. 3\\n[3] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiang-\\nwen Kong, Jun Li, and Xiangyu Zhang. Reversible column\\nnetworks. In International Conference on Learning Repre-\\nsentations (ICLR) , 2023. 2, 3, 5\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\\nto-end object detection with transformers. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\npages 213–229, 2020. 3\\n[5] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\\nJunni Zou. AP-loss for accurate one-stage object detection.\\nIEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence (TPAMI) , 43(11):3782–3798, 2020. 1\\n[6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang,\\nWenrui Dai, Hongkai Xiong, and Qi Tian. SdAE: Self-\\ndistillated masked autoencoder. In Proceedings of the Euro-\\npean Conference on Computer Vision (ECCV) , pages 108–\\n124, 2022. 2\\n[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin\\nHou, and Ming-Ming Cheng. YOLO-MS: rethinking multi-\\nscale representation learning for real-time object detection.\\narXiv preprint arXiv:2308.05480 , 2023. 1, 3, 7, 8\\n[8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong\\nWang, and Lu Yuan. DaVIT: Dual attention vision trans-\\nformers. In Proceedings of the European Conference on\\nComputer Vision (ECCV) , pages 74–92, 2022. 1\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. In International Con-\\nference on Learning Representations (ICLR) , 2021. 1, 2\\n[10] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,\\nand Weilin Huang. TOOD: Task-aligned one-stage object\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV) , pages 3490–3499,\\n2021. 1\\n[11] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu\\nZhang, Ming-Hsuan Yang, and Philip Torr. Res2Net: A\\nnew multi-scale backbone architecture. IEEE Transac-\\ntions on Pattern Analysis and Machine Intelligence (TPAMI) ,\\n43(2):652–662, 2019. 3\\n[12] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian\\nSun. OTA: Optimal transport assignment for object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 303–\\n312, 2021. 1\\n[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: Exceeding YOLO series in 2021. arXiv\\npreprint arXiv:2107.08430 , 2021. 3[14] Jocher Glenn. YOLOv5 release v7.0. https://github.\\ncom/ultralytics/yolov5/releases/tag/v7.\\n0, 2022. 3, 7\\n[15] Jocher Glenn. YOLOv8 release v8.1.0. https :\\n/ / github . com / ultralytics / ultralytics /\\nreleases/tag/v8.1.0 , 2024. 3, 7\\n[16] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B\\nGrosse. The reversible residual network: Backpropagation\\nwithout storing activations. Advances in Neural Information\\nProcessing Systems (NeurIPS) , 2017. 2, 3\\n[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence\\nmodeling with selective state spaces. arXiv preprint\\narXiv:2312.00752 , 2023. 1\\n[18] Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, and\\nChunhong Pan. AugFPN: Improving multi-scale fea-\\nture learning for object detection. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 12595–12604, 2020. 1, 3\\n[19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Ex-\\nploring disentangled representations in masked image mod-\\neling. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 2, 3\\n[20] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.\\nBoundary-aware instance segmentation. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 5696–5704, 2017. 1, 3\\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 770–778, 2016. 1, 4, 8\\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nIdentity mappings in deep residual networks. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\npages 630–645. Springer, 2016. 1, 4\\n[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4700–4708, 2017. 1\\n[24] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Win-\\nston H Hsu. MonoDTR: Monocular 3D object detection with\\ndepth-aware transformer. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 4012–4021, 2022. 1, 3\\n[25] Lin Huang, Weisheng Li, Linlin Shen, Haojie Fu, Xue Xiao,\\nand Suihan Xiao. YOLOCS: Object detection based on dense\\nchannel compression for feature spatial solidification. arXiv\\npreprint arXiv:2305.04170 , 2023. 3\\n[26] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,\\nAndrew Zisserman, and Joao Carreira. Perceiver: General\\nperception with iterative attention. In International Confer-\\nence on Machine Learning (ICML) , pages 4651–4664, 2021.\\n1\\n[27] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\\nToutanova. BERT: Pre-training of deep bidirectional trans-\\nformers for language understanding. In Proceedings of\\nNAACL-HLT , volume 1, page 2, 2019. 2\\n11', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 10}),\n",
       " Document(page_content='[28] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\\nZhang, and Zhuowen Tu. Deeply-supervised nets. In Ar-\\ntificial Intelligence and Statistics , pages 562–570, 2015. 1,\\n2, 3\\n[29] Alex Levinshtein, Alborz Rezazadeh Sereshkeh, and Kon-\\nstantinos Derpanis. DATNet: Dense auxiliary tasks for ob-\\nject detection. In Proceedings of the IEEE/CVF Winter Con-\\nference on Applications of Computer Vision (WACV) , pages\\n1419–1427, 2020. 1, 3\\n[30] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng\\nCheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-\\niang Chu. YOLOv6 v3.0: A full-scale reloading. arXiv\\npreprint arXiv:2301.05586 , 2023. 3, 7, 2, 4\\n[31] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei\\nGeng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, et al. YOLOv6: A single-stage object de-\\ntection framework for industrial applications. arXiv preprint\\narXiv:2209.02976 , 2022. 3\\n[32] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng\\nLi, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang,\\nWenhai Wang, et al. Uni-perceiver v2: A generalist model\\nfor large-scale vision and vision-language tasks. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 2691–2700, 2023. 1\\n[33] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A\\ndual weighting label assignment scheme for object detection.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 9387–9396,\\n2022. 1\\n[34] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\\nZhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-\\nNet: A composite backbone network architecture for object\\ndetection. IEEE Transactions on Image Processing (TIP) ,\\n2022. 3, 9\\n[35] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyra-\\nmid networks for object detection. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 2117–2125, 2017. 3\\n[36] Zhihao Lin, Yongtao Wang, Jinhe Zhang, and Xiaojie Chu.\\nDynamicDet: A unified dynamic architecture for object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n6282–6291, 2023. 3, 6, 2, 4\\n[37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\\nPath aggregation network for instance segmentation. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR) , pages 8759–8768, 2018.\\n3, 5\\n[38] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi\\nXie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba:\\nVisual state space model. arXiv preprint arXiv:2401.10166 ,\\n2024. 1\\n[39] Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang,\\nQijie Zhao, Zhi Tang, and Haibin Ling. CBNet: A novel\\ncomposite backbone network architecture for object detec-\\ntion. In Proceedings of the AAAI Conference on Artificial\\nIntelligence (AAAI) , pages 11653–11660, 2020. 3[40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\\nSwin transformer v2: Scaling up capacity and resolution. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR) , 2022. 1\\n[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV) , pages 10012–10022, 2021. 1\\n[42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A ConvNet for the\\n2020s. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 11976–\\n11986, 2022. 1\\n[43] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang,\\nJinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and\\nYi Liu. DETRs beat YOLOs on real-time object detection.\\narXiv preprint arXiv:2304.08069 , 2023. 1, 3, 7, 8, 2, 4\\n[44] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou,\\nYudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.\\nRTMDet: An empirical study of designing real-time object\\ndetectors. arXiv preprint arXiv:2212.07784 , 2022. 8, 2, 3, 4\\n[45] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. A ranking-based, balanced loss function unify-\\ning classification and localisation in object detection. Ad-\\nvances in Neural Information Processing Systems (NeurIPS) ,\\n33:15534–15545, 2020. 1\\n[46] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. Rank & sort loss for object detection and instance\\nsegmentation. In Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision (ICCV) , pages 3009–\\n3018, 2021. 1\\n[47] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Unified, real-time object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 779–\\n788, 2016. 3\\n[48] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7263–7271, 2017. 3\\n[49] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental\\nimprovement. arXiv preprint arXiv:1804.02767 , 2018. 3, 8\\n[50] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\\ntersection over union: A metric and a loss for bounding box\\nregression. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n658–666, 2019. 1\\n[51] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,\\nYurong Chen, and Xiangyang Xue. Object detection from\\nscratch with deep supervision. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence (TPAMI) , 42(2):398–412,\\n2019. 1, 2\\n[52] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-\\nactor: A multi-task transformer for robotic manipulation.\\n12', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 11}),\n",
       " Document(page_content='InConference on Robot Learning (CoRL) , pages 785–799,\\n2023. 1\\n[53] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan,\\nChanghu Wang, and Ping Luo. What makes for end-to-end\\nobject detection? In International Conference on Machine\\nLearning (ICML) , pages 9934–9944, 2021. 3\\n[54] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n1–9, 2015. 1, 2, 3\\n[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\\ntecture for computer vision. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 2818–2826, 2016. 1\\n[56] Zineng Tang, Jaemin Cho, Jie Lei, and Mohit Bansal.\\nPerceiver-VL: Efficient vision-and-language modeling with\\niterative latent attention. In Proceedings of the IEEE/CVF\\nWinter Conference on Applications of Computer Vision\\n(WACV) , pages 4410–4420, 2023. 1\\n[57] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nFully convolutional one-stage object detection. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV) , pages 9627–9636, 2019. 3\\n[58] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nA simple and strong anchor-free object detector. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(TPAMI) , 44(4):1922–1933, 2022. 3\\n[59] Naftali Tishby and Noga Zaslavsky. Deep learning and the\\ninformation bottleneck principle. In IEEE Information The-\\nory Workshop (ITW) , pages 1–5, 2015. 2, 4\\n[60] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\\nPeyman Milanfar, Alan Bovik, and Yinxiao Li. MaxVIT:\\nMulti-axis vision transformer. In Proceedings of the Euro-\\npean Conference on Computer Vision (ECCV) , pages 459–\\n479, 2022. 1\\n[61] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo,\\nChuanjian Liu, Kai Han, and Yunhe Wang. Gold-YOLO:\\nEfficient object detector via gather-and-distribute mecha-\\nnism. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 3, 7, 2, 4\\n[62] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Scaled-YOLOv4: Scaling cross stage\\npartial network. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 13029–13038, 2021. 3\\n[63] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. YOLOv7: Trainable bag-of-freebies\\nsets new state-of-the-art for real-time object detectors. In\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 7464–7475,\\n2023. 3, 6, 7, 9, 1\\n[64] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet: A\\nnew backbone that can enhance learning capability of CNN.InProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition Workshops (CVPRW) , pages\\n390–391, 2020. 3, 6, 8\\n[65] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh.\\nDesigning network design strategies through gradient path\\nanalysis. Journal of Information Science and Engineering\\n(JISE) , 39(4):975–995, 2023. 2, 3, 6\\n[66] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\\nYou only learn one representation: Unified network for mul-\\ntiple tasks. Journal of Information Science & Engineering\\n(JISE) , 39(3):691–709, 2023. 2, 3, 4\\n[67] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\\nSun, and Nanning Zheng. End-to-end object detection\\nwith fully convolutional network. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 15849–15858, 2021. 1, 3\\n[68] Liwei Wang, Chen-Yu Lee, Zhuowen Tu, and Svetlana\\nLazebnik. Training deeper convolutional networks with deep\\nsupervision. arXiv preprint arXiv:1505.02496 , 2015. 1, 2, 3\\n[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\\nPyramid vision transformer: A versatile backbone for dense\\nprediction without convolutions. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision\\n(ICCV) , pages 568–578, 2021. 1\\n[70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. PVT\\nv2: Improved baselines with pyramid vision transformer.\\nComputational Visual Media , 8(3):415–424, 2022. 1\\n[71] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\\nChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\\nvNeXt v2: Co-designing and scaling convnets with masked\\nautoencoders. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n16133–16142, 2023. 1, 2\\n[72] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\\nKaiming He. Aggregated residual transformations for deep\\nneural networks. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 1492–1500, 2017. 1\\n[73] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\\nBao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: A simple\\nframework for masked image modeling. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 9653–9663, 2022. 2\\n[74] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,\\nCheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing\\nDang, Shengyu Wei, Yuning Du, et al. PP-YOLOE: An\\nevolved version of YOLO. arXiv preprint arXiv:2203.16250 ,\\n2022. 3, 8, 2, 4\\n[75] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang,\\nYuan Zhang, and Xiuyu Sun. DAMO-YOLO: A re-\\nport on real-time object detection design. arXiv preprint\\narXiv:2211.15444 , 2022. 3, 7, 2, 4\\n[76] Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui, Yu\\nQiao, Hongsheng Li, and Peng Gao. MonoDETR: Depth-\\nguided transformer for monocular 3D object detection. In\\n13', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 12}),\n",
       " Document(page_content='Proceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV) , pages 9155–9166, 2023. 1, 3\\n[77] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\\nYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-\\nter learning for bounding box regression. In Proceedings of\\nthe AAAI Conference on Artificial Intelligence (AAAI) , vol-\\nume 34, pages 12993–13000, 2020. 1\\n[78] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\\nYin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D\\nobject detection. In International Conference on 3D Vision\\n(3DV) , pages 85–94, 2019. 1\\n[79] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,\\nSongtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-\\nentiable label assignment for dense object detection. arXiv\\npreprint arXiv:2007.03496 , 2020. 1\\n[80] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,\\nWenyu Liu, and Xinggang Wang. Vision mamba: Efficient\\nvisual representation learning with bidirectional state space\\nmodel. arXiv preprint arXiv:2401.09417 , 2024. 1\\n[81] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng\\nLi, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-\\ntraining unified architecture for generic perception for zero-\\nshot and few-shot tasks. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 16804–16815, 2022. 1\\n[82] Zhuofan Zong, Guanglu Song, and Yu Liu. DETRs with\\ncollaborative hybrid assignments training. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 6748–6758, 2023. 3\\n14', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 13}),\n",
       " Document(page_content='Appendix\\nA. Implementation Details\\nTable 1. Hyper parameter settings of YOLOv9.\\nhyper parameter value\\nepochs 500\\noptimizer SGD\\ninitial learning rate 0.01\\nfinish learning rate 0.0001\\nlearning rate decay linear\\nmomentum 0.937\\nweight decay 0.0005\\nwarm-up epochs 3\\nwarm-up momentum 0.8\\nwarm-up bias learning rate 0.1\\nbox loss gain 7.5\\nclass loss gain 0.5\\nDFL loss gain 1.5\\nHSV saturation augmentation 0.7\\nHSV value augmentation 0.4\\ntranslation augmentation 0.1\\nscale augmentation 0.9\\nmosaic augmentation 1.0\\nMixUp augmentation 0.15\\ncopy & paste augmentation 0.3\\nclose mosaic epochs 15\\nThe training parameters of YOLOv9 are shown in Ta-\\nble 1. We fully follow the settings of YOLOv7 AF [63],\\nwhich is to use SGD optimizer to train 500 epochs. We first\\nwarm-up for 3 epochs and only update the bias during the\\nwarm-up stage. Next we step down from the initial learning\\nrate 0.01 to 0.0001 in linear decay manner, and the data aug-\\nmentation settings are listed in the bottom part of Table 1.\\nWe shut down mosaic data augmentation operations on the\\nlast 15 epochs.Table 2. Network configurations of YOLOv9.\\nIndex Module Route Filters Depth Size Stride\\n0 Conv – 64 – 3 2\\n1 Conv 0 128 – 3 2\\n2 CSP-ELAN 1 256, 128, 64 2, 1 – 1\\n3 DOWN 2 256 – 3 2\\n4 CSP-ELAN 3 512, 256, 128 2, 1 – 1\\n5 DOWN 4 512 – 3 2\\n6 CSP-ELAN 5 512, 512, 256 2, 1 – 1\\n7 DOWN 6 512 – 3 2\\n8 CSP-ELAN 7 512, 512, 256 2, 1 – 1\\n9 SPP-ELAN 8 512, 256, 256 3, 1 – 1\\n10 Up 9 512 – – 2\\n11 Concat 10, 6 1024 – – 1\\n12 CSP-ELAN 11 512, 512, 256 2, 1 – 1\\n13 Up 12 512 – – 2\\n14 Concat 13, 4 1024 – – 1\\n15 CSP-ELAN 14 256, 256, 128 2, 1 – 1\\n16 DOWN 15 256 – 3 2\\n17 Concat 16, 12 768 – – 1\\n18 CSP-ELAN 17 512, 512, 256 2, 1 – 1\\n19 DOWN 18 512 – 3 2\\n20 Concat 19, 9 1024 – – 1\\n21 CSP-ELAN 20 512, 512, 256 2, 1 – 1\\n22 Predict 15, 18, 21 – – – –\\nThe network topology of YOLOv9 completely follows\\nYOLOv7 AF [63], that is, we replace ELAN with the pro-\\nposed CSP-ELAN block. As listed in Table 2, the depth\\nparameters of CSP-ELAN are represented as ELAN depth\\nand CSP depth, respectively. As for the parameters of CSP-\\nELAN filters, they are represented as ELAN output fil-\\nter, CSP output filter, and CSP inside filter. In the down-\\nsampling module part, we simplify CSP-DOWN module to\\nDOWN module. DOWN module is composed of a pooling\\nlayer with size 2 and stride 1, and a Conv layer with size 3\\nand stride 2. Finally, we optimized the prediction layer and\\nreplaced top, left, bottom, and right in the regression branch\\nwith decoupled branch.\\n1', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 14}),\n",
       " Document(page_content='Table 3. Comparison of state-of-the-art object detectors with different training settings.\\nModel #Param. (M) FLOPs (G) AP 50:95 (%) AP 50(%) AP 75(%) AP S(%) AP M(%) AP L(%)Train-from-scratchDy-YOLOv7 [36] – 181.7 53.9 72.2 58.7 35.3 57.6 66.4\\nDy-YOLOv7-X [36] – 307.9 55.0 73.2 60.0 36.6 58.7 68.5\\nYOLOv9-S (Ours) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nYOLOv9-M (Ours) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nYOLOv9-C (Ours) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nYOLOv9-E (Ours) 35.0 148.1 54.5 71.7 59.2 38.1 59.9 70.3\\nYOLOv9-E (Ours) 44.8 187.0 55.1 72.3 60.7 38.7 60.6 71.4\\nYOLOv9-E (Ours) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4ImageNet PretrainedRTMDet-T [44] 4.8 12.6 41.1 57.9 – – – –\\nRTMDet-S [44] 9.0 25.6 44.6 61.9 – – – –\\nRTMDet-M [44] 24.7 78.6 49.4 66.8 – – – –\\nRTMDet-L [44] 52.3 160.4 51.5 68.8 – – – –\\nRTMDet-X [44] 94.9 283.4 52.8 70.4 – – – –\\nPPYOLOE-S [74] 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nPPYOLOE-M [74] 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nPPYOLOE-L [74] 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nPPYOLOE-X [74] 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\nRT DETR-L [43] 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nRT DETR-X [43] 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nRT DETR-R18 [43] 20 60 46.5 63.8 – – – –\\nRT DETR-R34 [43] 31 92 48.9 66.8 – – – –\\nRT DETR-R50M [43] 36 100 51.3 69.6 – – – –\\nRT DETR-R50 [43] 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nRT DETR-R101 [43] 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nGold YOLO-S [61] 21.5 46.0 45.5 62.2 – – – –\\nGold YOLO-M [61] 41.3 57.5 50.2 67.5 – – – –\\nGold YOLO-L [61] 75.1 151.7 52.3 69.6 – – – –Knowledge DistillationYOLOv6-N v3.0 [30] 4.7 11.4 37.5 53.1 – – – –\\nYOLOv6-S v3.0 [30] 18.5 45.3 45.0 61.8 – – – –\\nYOLOv6-M v3.0 [30] 34.9 85.8 50.0 66.9 – – – –\\nYOLOv6-L v3.0 [30] 59.6 150.7 52.8 70.3 – – – –\\nDAMO YOLO-T [75] 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nDAMO YOLO-S [75] 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nDAMO YOLO-M [75] 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nDAMO YOLO-L [75] 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nGold YOLO-N [61] 5.6 12.1 39.9 55.9 – – – –\\nGold YOLO-S [61] 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-M [61] 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-L [61] 75.1 151.7 53.2 70.5 – – – –Complex SettingGold YOLO-S [61] 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-M [61] 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-L [61] 75.1 151.7 53.3 70.9 – – – –\\nYOLOR-CSP [66] 52.9 120.4 52.8 71.2 57.6 – – –\\nYOLOR-CSP-X [66] 96.9 226.8 54.8 73.1 59.7 – – –\\nPPYOLOE+-S [74] 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE+-M [74] 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE+-L [74] 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE+-X [74] 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nB. More Comparison\\nWe compare YOLOv9 to state-of-the-art real-time object\\ndetectors trained with different methods. It mainly includes\\nfour different training methods: (1) train-from-scratch: we\\nhave completed most of the comparisons in the text. Here\\nare only list of additional data of DynamicDet [36] for com-\\nparisons; (2) Pretrained by ImageNet: this includes two\\nmethods of using ImageNet for supervised pretrain and self-\\nsupervised pretrain; (3) knowledge distillation: a method\\nto perform additional self-distillation after training is com-pleted; and (4) a more complex training process: a combi-\\nnation of steps including pretrained by ImageNet, knowl-\\nedge distillation, DAMO-YOLO and even additional pre-\\ntrained large object detection dataset. We show the results\\nin Table 3. From this table, we can see that our proposed\\nYOLOv9 performed better than all other methods. Com-\\npared with PPYOLOE+-X trained using ImageNet and Ob-\\njects365, our method still reduces the number of parameters\\nby 54% and the amount of computation by 9%, and improv-\\ning 0.4% AP.\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 15}),\n",
       " Document(page_content='Table 4. Comparison of state-of-the-art object detectors with different training settings (sorted by number of parameters).\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv6-N v3.0 [30] (D) 4.7 11.4 37.5 53.1 – – – –\\nRTMDet-T [44] (I) 4.8 12.6 41.1 57.9 – – – –\\nGold YOLO-N [61] (D) 5.6 12.1 39.9 55.9 – – – –\\nYOLOv9-S (S) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nPPYOLOE+-S [74] (C) 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE-S [74] (I) 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nDAMO YOLO-T [75] (D) 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nRTMDet-S [44] (I) 9.0 25.6 44.6 61.9 – – – –\\nDAMO YOLO-S [75] (D) 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nYOLOv6-S v3.0 [30] (D) 18.5 45.3 45.0 61.8 – – – –\\nRT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\\nYOLOv9-M (S) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nGold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\\nPPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nRTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\\nYOLOv9-C (S) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nDAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nRT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\\nRT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nYOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\\nYOLOv9-E (S) 35.0 148.1 54.5 71.7 59.2 38.1 59.9 70.3\\nRT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\\nGold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\\nRT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nDAMO YOLO-L [75] (D) 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nYOLOv9-E (S) 44.8 187.0 55.1 72.3 60.7 38.7 60.6 71.4\\nPPYOLOE+-L [74] (C) 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE-L [74] (I) 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nRTMDet-L [44] (I) 52.3 160.4 51.5 68.8 – – – –\\nYOLOR-CSP [66] (C) 52.9 120.4 52.8 71.2 57.6 – – –\\nYOLOv9-E (S) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4\\nYOLOv6-L v3.0 [30] (D) 59.6 150.7 52.8 70.3 – – – –\\nRT DETR-X [43] (I) 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nGold YOLO-L [61] (C) 75.1 151.7 53.3 70.9 – – – –\\nGold YOLO-L [61] (D) 75.1 151.7 53.2 70.5 – – – –\\nGold YOLO-L [61] (I) 75.1 151.7 52.3 69.6 – – – –\\nRT DETR-R101 [43] (I) 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nRTMDet-X [44] (I) 94.9 283.4 52.8 70.4 – – – –\\nYOLOR-CSP-X [66] (C) 96.9 226.8 54.8 73.1 59.7 – – –\\nPPYOLOE+-X [74] (C) 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nPPYOLOE-X [74] (I) 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\n1(S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.\\nTable 4 shows the performance of all models sorted by\\nparameter size. Our proposed YOLOv9 is Pareto optimal\\nin all models of different sizes. Among them, we found no\\nother method for Pareto optimal in models with more than\\n20M parameters. The above experimental data shows that\\nour YOLOv9 has excellent parameter usage efficiency.Shown in Table 5 is the performance of all participat-\\ning models sorted by the amount of computation. Our pro-\\nposed YOLOv9 is Pareto optimal in all models with differ-\\nent scales. Among models with more than 60 GFLOPs, only\\nELAN-based DAMO-YOLO and DETR-based RT DETR\\ncan rival the proposed YOLOv9. The above comparison\\nresults show that YOLOv9 has the most outstanding per-\\nformance in the trade-off between computation complexity\\nand accuracy.\\n3', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 16}),\n",
       " Document(page_content='Table 5. Comparison of state-of-the-art object detectors with different training settings (sorted by amount of computation).\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv6-N v3.0 [30] (D) 4.7 11.4 37.5 53.1 – – – –\\nGold YOLO-N [61] (D) 5.6 12.1 39.9 55.9 – – – –\\nRTMDet-T [44] (I) 4.8 12.6 41.1 57.9 – – – –\\nPPYOLOE+-S [74] (C) 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE-S [74] (I) 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nDAMO YOLO-T [75] (D) 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nRTMDet-S [44] (I) 9.0 25.6 44.6 61.9 – – – –\\nYOLOv9-S (S) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nDAMO YOLO-S [75] (D) 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nYOLOv6-S v3.0 [30] (D) 18.5 45.3 45.0 61.8 – – – –\\nGold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\\nPPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nGold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\\nRT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\\nDAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nYOLOv9-M (S) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nRTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\\nYOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\\nRT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\\nDAMO YOLO-L [75] (D) 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nRT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\\nYOLOv9-C (S) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nRT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nPPYOLOE+-L [74] (C) 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE-L [74] (I) 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nYOLOR-CSP [66] (C) 52.9 120.4 52.8 71.2 57.6 – – –\\nRT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nYOLOv9-E (S) 35.0 148.1 54.5 71.7 59.2 38.1 59.9 70.3\\nYOLOv6-L v3.0 [30] (D) 59.6 150.7 52.8 70.3 – – – –\\nGold YOLO-L [61] (C) 75.1 151.7 53.3 70.9 – – – –\\nGold YOLO-L [61] (D) 75.1 151.7 53.2 70.5 – – – –\\nGold YOLO-L [61] (I) 75.1 151.7 52.3 69.6 – – – –\\nRTMDet-L [44] (I) 52.3 160.4 51.5 68.8 – – – –\\nDy-YOLOv7 [36] (S) – 181.7 53.9 72.2 58.7 35.3 57.6 66.4\\nYOLOv9-E (S) 44.8 187.0 55.1 72.3 60.7 38.7 60.6 71.4\\nYOLOv9-E (S) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4\\nPPYOLOE+-X [74] (C) 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nPPYOLOE-X [74] (I) 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\nYOLOR-CSP-X [66] (C) 96.9 226.8 54.8 73.1 59.7 – – –\\nRT DETR-X [43] (I) 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nRT DETR-R101 [43] (I) 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nRTMDet-X [44] (I) 94.9 283.4 52.8 70.4 – – – –\\nDy-YOLOv7-X [36] (S) – 307.9 55.0 73.2 60.0 36.6 58.7 68.5\\n1(S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.\\n4', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 17}),\n",
       " Document(page_content='Scalable Diffusion Models with Transformers\\nWilliam Peebles*\\nUC BerkeleySaining Xie\\nNew York University\\nFigure 1. Diffusion models with transformer backbones achieve state-of-the-art image quality. We show selected samples from two\\nof our class-conditional DiT-XL/2 models trained on ImageNet at 512 ×512 and 256×256 resolution, respectively.\\nAbstract\\nWe explore a new class of diffusion models based on the\\ntransformer architecture. We train latent diffusion models\\nof images, replacing the commonly-used U-Net backbone\\nwith a transformer that operates on latent patches. We an-\\nalyze the scalability of our Diffusion Transformers (DiTs)\\nthrough the lens of forward pass complexity as measured by\\nGﬂops. We ﬁnd that DiTs with higher Gﬂops—through in-\\ncreased transformer depth/width or increased number of in-\\nput tokens—consistently have lower FID. In addition to pos-\\nsessing good scalability properties, our largest DiT-XL/2\\nmodels outperform all prior diffusion models on the class-\\nconditional ImageNet 512 ×512 and 256×256 benchmarks,\\nachieving a state-of-the-art FID of 2.27 on the latter.1. Introduction\\nMachine learning is experiencing a renaissance powered\\nby transformers. Over the past ﬁve years, neural architec-\\ntures for natural language processing [8, 42], vision [10]\\nand several other domains have largely been subsumed by\\ntransformers [60]. Many classes of image-level genera-\\ntive models remain holdouts to the trend, though—while\\ntransformers see widespread use in autoregressive mod-\\nels [3,6,43,47], they have seen less adoption in other gener-\\native modeling frameworks. For example, diffusion models\\nhave been at the forefront of recent advances in image-level\\ngenerative models [9,46]; yet, they all adopt a convolutional\\nU-Net architecture as the de-facto choice of backbone.\\n*Work done during an internship at Meta AI, FAIR Team.\\nCode and project page available here.\\n1arXiv:2212.09748v2  [cs.CV]  2 Mar 2023', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 0}),\n",
       " Document(page_content='520 80 320GflopsDiameterFigure 2. ImageNet generation with Diffusion Transformers (DiTs). Bubble area indicates the ﬂops of the diffusion model. Left:\\nFID-50K (lower is better) of our DiT models at 400K training iterations. Performance steadily improves in FID as model ﬂops increase.\\nRight: Our best model, DiT-XL/2, is compute-efﬁcient and outperforms all prior U-Net-based diffusion models, like ADM and LDM.\\nThe seminal work of Ho et al. [19] ﬁrst introduced the\\nU-Net backbone for diffusion models. Having initially seen\\nsuccess within pixel-level autoregressive models and con-\\nditional GANs [23], the U-Net was inherited from Pixel-\\nCNN++ [52, 58] with a few changes. The model is con-\\nvolutional, comprised primarily of ResNet [15] blocks. In\\ncontrast to the standard U-Net [49], additional spatial self-\\nattention blocks, which are essential components in trans-\\nformers, are interspersed at lower resolutions. Dhariwal and\\nNichol [9] ablated several architecture choices for the U-\\nNet, such as the use of adaptive normalization layers [40] to\\ninject conditional information and channel counts for con-\\nvolutional layers. However, the high-level design of the U-\\nNet from Ho et al. has largely remained intact.\\nWith this work, we aim to demystify the signiﬁcance of\\narchitectural choices in diffusion models and offer empiri-\\ncal baselines for future generative modeling research. We\\nshow that the U-Net inductive bias is notcrucial to the per-\\nformance of diffusion models, and they can be readily re-\\nplaced with standard designs such as transformers. As a\\nresult, diffusion models are well-poised to beneﬁt from the\\nrecent trend of architecture uniﬁcation—e.g., by inheriting\\nbest practices and training recipes from other domains, as\\nwell as retaining favorable properties like scalability, ro-\\nbustness and efﬁciency. A standardized architecture would\\nalso open up new possibilities for cross-domain research.\\nIn this paper, we focus on a new class of diffusion models\\nbased on transformers. We call them Diffusion Transform-\\ners, or DiTs for short. DiTs adhere to the best practices of\\nVision Transformers (ViTs) [10], which have been shown to\\nscale more effectively for visual recognition than traditional\\nconvolutional networks (e.g., ResNet [15]).More speciﬁcally, we study the scaling behavior of trans-\\nformers with respect to network complexity vs. sample\\nquality . We show that by constructing and benchmark-\\ning the DiT design space under the Latent Diffusion Mod-\\nels(LDMs) [48] framework, where diffusion models are\\ntrained within a V AE’s latent space, we can successfully\\nreplace the U-Net backbone with a transformer. We further\\nshow that DiTs are scalable architectures for diffusion mod-\\nels: there is a strong correlation between the network com-\\nplexity (measured by Gﬂops) vs. sample quality (measured\\nby FID). By simply scaling-up DiT and training an LDM\\nwith a high-capacity backbone (118.6 Gﬂops), we are able\\nto achieve a state-of-the-art result of 2.27 FID on the class-\\nconditional 256×256ImageNet generation benchmark.\\n2. Related Work\\nTransformers. Transformers [60] have replaced domain-\\nspeciﬁc architectures across language, vision [10], rein-\\nforcement learning [5, 25] and meta-learning [39]. They\\nhave shown remarkable scaling properties under increas-\\ning model size, training compute and data in the language\\ndomain [26], as generic autoregressive models [17] and\\nas ViTs [63]. Beyond language, transformers have been\\ntrained to autoregressively predict pixels [6, 7, 38]. They\\nhave also been trained on discrete codebooks [59] as both\\nautoregressive models [11,47] and masked generative mod-\\nels [4, 14]; the former has shown excellent scaling behavior\\nup to 20B parameters [62]. Finally, transformers have been\\nexplored in DDPMs to synthesize non-spatial data; e.g., to\\ngenerate CLIP image embeddings in DALL ·E 2 [41,46]. In\\nthis paper, we study the scaling properties of transformers\\nwhen used as the backbone of diffusion models of images.\\n2', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 1}),\n",
       " Document(page_content='Multi-Head Self-AttentionLayer NormScale, ShiftMLPPointwise FeedforwardLayer NormScale, ShiftScale++\\nScale𝛾!,𝛽!𝛼!𝛾\",𝛽\"𝛼\"\\nInput TokensConditioningDiTBlock with adaLN-ZeroLatent Diffusion TransformerTimestep 𝑡Label 𝑦DiTBlockN xPatchifyLayer NormLinear and ReshapeEmbedNoiseΣ32 x 32 x 432 x 32 x 4\\nNoised Latent32 x 32 x 4Multi-Head Self-AttentionLayer NormPointwise Feedforward\\nLayer Norm++\\nInput TokensConditioningDiTBlock with Cross-AttentionMulti-Head Cross-AttentionLayer Norm+Multi-Head Self-AttentionLayer NormPointwise Feedforward++\\nInput TokensConditioningDiTBlock with In-Context ConditioningLayer Norm\\nConcatenate on Sequence DimensionFigure 3. The Diffusion Transformer (DiT) architecture. Left: We train conditional latent DiT models. The input latent is decomposed\\ninto patches and processed by several DiT blocks. Right: Details of our DiT blocks. We experiment with variants of standard transformer\\nblocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Adaptive layer norm works best.\\nDenoising diffusion probabilistic models (DDPMs).\\nDiffusion [19, 54] and score-based generative models [22,\\n56] have been particularly successful as generative models\\nof images [35,46,48,50], in many cases outperforming gen-\\nerative adversarial networks (GANs) [12] which had previ-\\nously been state-of-the-art. Improvements in DDPMs over\\nthe past two years have largely been driven by improved\\nsampling techniques [19, 27, 55], most notably classiﬁer-\\nfree guidance [21], reformulating diffusion models to pre-\\ndict noise instead of pixels [19] and using cascaded DDPM\\npipelines where low-resolution base diffusion models are\\ntrained in parallel with upsamplers [9, 20]. For all the dif-\\nfusion models listed above, convolutional U-Nets [49] are\\nthe de-facto choice of backbone architecture. Concurrent\\nwork [24] introduced a novel, efﬁcient architecture based\\non attention for DDPMs; we explore pure transformers.\\nArchitecture complexity. When evaluating architecture\\ncomplexity in the image generation literature, it is fairly\\ncommon practice to use parameter counts. In general, pa-\\nrameter counts can be poor proxies for the complexity of\\nimage models since they do not account for, e.g., image res-\\nolution which signiﬁcantly impacts performance [44, 45].\\nInstead, much of the model complexity analysis in this pa-\\nper is through the lens of theoretical Gﬂops. This brings us\\nin-line with the architecture design literature where Gﬂops\\nare widely-used to gauge complexity. In practice, the\\ngolden complexity metric is still up for debate as it fre-\\nquently depends on particular application scenarios. Nichol\\nand Dhariwal’s seminal work improving diffusion mod-\\nels [9, 36] is most related to us—there, they analyzed the\\nscalability and Gﬂop properties of the U-Net architecture\\nclass. In this paper, we focus on the transformer class.3. Diffusion Transformers\\n3.1. Preliminaries\\nDiffusion formulation. Before introducing our architec-\\nture, we brieﬂy review some basic concepts needed to\\nunderstand diffusion models (DDPMs) [19, 54]. Gaus-\\nsian diffusion models assume a forward noising process\\nwhich gradually applies noise to real data x0:q(xt|x0) =\\nN(xt;√¯αtx0,(1−¯αt)I), where constants ¯αtare hyperpa-\\nrameters. By applying the reparameterization trick, we can\\nsamplext=√¯αtx0+√1−¯αtϵt, whereϵt∼N(0,I).\\nDiffusion models are trained to learn the reverse process\\nthat inverts forward process corruptions: pθ(xt−1|xt) =\\nN(µθ(xt),Σθ(xt)), where neural networks are used to pre-\\ndict the statistics of pθ. The reverse process model is\\ntrained with the variational lower bound [30] of the log-\\nlikelihood of x0, which reduces to L(θ) =−p(x0|x1) +∑\\ntDKL(q∗(xt−1|xt,x0)||pθ(xt−1|xt)), excluding an ad-\\nditional term irrelevant for training. Since both q∗andpθ\\nare Gaussian,DKLcan be evaluated with the mean and co-\\nvariance of the two distributions. By reparameterizing µθas\\na noise prediction network ϵθ, the model can be trained us-\\ning simple mean-squared error between the predicted noise\\nϵθ(xt)and the ground truth sampled Gaussian noise ϵt:\\nLsimple (θ) =||ϵθ(xt)−ϵt||2\\n2. But, in order to train diffu-\\nsion models with a learned reverse process covariance Σθ,\\nthe fullDKLterm needs to be optimized. We follow Nichol\\nand Dhariwal’s approach [36]: train ϵθwithLsimple , and\\ntrainΣθwith the fullL. Oncepθis trained, new images can\\nbe sampled by initializing xtmax∼ N (0,I)and sampling\\nxt−1∼pθ(xt−1|xt)via the reparameterization trick.\\n3', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 2}),\n",
       " Document(page_content='Classiﬁer-free guidance. Conditional diffusion models\\ntake extra information as input, such as a class label c.\\nIn this case, the reverse process becomes pθ(xt−1|xt,c),\\nwhereϵθandΣθare conditioned on c. In this setting,\\nclassiﬁer-free guidance can be used to encourage the sam-\\npling procedure to ﬁnd xsuch that logp(c|x)is high [21].\\nBy Bayes Rule, logp(c|x)∝logp(x|c)−logp(x), and\\nhence∇xlogp(c|x)∝∇xlogp(x|c)−∇xlogp(x). By in-\\nterpreting the output of diffusion models as the score func-\\ntion, the DDPM sampling procedure can be guided to sam-\\nplexwith highp(x|c)by:ˆϵθ(xt,c) =ϵθ(xt,∅) +s·\\n∇xlogp(x|c)∝ϵθ(xt,∅)+s·(ϵθ(xt,c)−ϵθ(xt,∅)), where\\ns>1indicates the scale of the guidance (note that s= 1re-\\ncovers standard sampling). Evaluating the diffusion model\\nwithc=∅is done by randomly dropping out cduring\\ntraining and replacing it with a learned “null” embedding\\n∅. Classiﬁer-free guidance is widely-known to yield sig-\\nniﬁcantly improved samples over generic sampling tech-\\nniques [21, 35, 46], and the trend holds for our DiT models.\\nLatent diffusion models. Training diffusion models di-\\nrectly in high-resolution pixel space can be computationally\\nprohibitive. Latent diffusion models (LDMs) [48] tackle this\\nissue with a two-stage approach: (1) learn an autoencoder\\nthat compresses images into smaller spatial representations\\nwith a learned encoder E; (2) train a diffusion model of\\nrepresentations z=E(x)instead of a diffusion model of\\nimagesx(Eis frozen). New images can then be generated\\nby sampling a representation zfrom the diffusion model\\nand subsequently decoding it to an image with the learned\\ndecoderx=D(z).\\nAs shown in Figure 2, LDMs achieve good performance\\nwhile using a fraction of the Gﬂops of pixel space diffusion\\nmodels like ADM. Since we are concerned with compute\\nefﬁciency, this makes them an appealing starting point for\\narchitecture exploration. In this paper, we apply DiTs to\\nlatent space, although they could be applied to pixel space\\nwithout modiﬁcation as well. This makes our image genera-\\ntion pipeline a hybrid-based approach; we use off-the-shelf\\nconvolutional V AEs and transformer-based DDPMs.\\n3.2. Diffusion Transformer Design Space\\nWe introduce Diffusion Transformers (DiTs), a new ar-\\nchitecture for diffusion models. We aim to be as faithful to\\nthe standard transformer architecture as possible to retain\\nits scaling properties. Since our focus is training DDPMs of\\nimages (speciﬁcally, spatial representations of images), DiT\\nis based on the Vision Transformer (ViT) architecture which\\noperates on sequences of patches [10]. DiT retains many of\\nthe best practices of ViTs. Figure 3 shows an overview of\\nthe complete DiT architecture. In this section, we describe\\nthe forward pass of DiT, as well as the components of the\\ndesign space of the DiT class.\\n𝑝𝐼𝑝𝐼𝑇=𝐼/𝑝!Noised LatentI×I×CInput Tokens T×dDiTBlockFigure 4. Input speciﬁcations for DiT. Given patch size p×p,\\na spatial representation (the noised latent from the V AE) of shape\\nI×I×Cis “patchiﬁed” into a sequence of length T= (I/p)2\\nwith hidden dimension d. A smaller patch size presults in a longer\\nsequence length and thus more Gﬂops.\\nPatchify. The input to DiT is a spatial representation z\\n(for256×256×3images,zhas shape 32×32×4). The\\nﬁrst layer of DiT is “patchify,” which converts the spatial\\ninput into a sequence of Ttokens, each of dimension d,\\nby linearly embedding each patch in the input. Following\\npatchify, we apply standard ViT frequency-based positional\\nembeddings (the sine-cosine version) to all input tokens.\\nThe number of tokens Tcreated by patchify is determined\\nby the patch size hyperparameter p. As shown in Figure 4,\\nhalvingpwill quadruple T, and thus at least quadruple total\\ntransformer Gﬂops. Although it has a signiﬁcant impact on\\nGﬂops, note that changing phas no meaningful impact on\\ndownstream parameter counts.\\nWe addp= 2,4,8to the DiT design space.\\nDiT block design. Following patchify, the input tokens\\nare processed by a sequence of transformer blocks. In ad-\\ndition to noised image inputs, diffusion models sometimes\\nprocess additional conditional information such as noise\\ntimestepst, class labels c, natural language, etc. We explore\\nfour variants of transformer blocks that process conditional\\ninputs differently. The designs introduce small, but impor-\\ntant, modiﬁcations to the standard ViT block design. The\\ndesigns of all blocks are shown in Figure 3.\\n–In-context conditioning. We simply append the vec-\\ntor embeddings of tandcas two additional tokens in\\nthe input sequence, treating them no differently from\\nthe image tokens. This is similar to cls tokens in\\nViTs, and it allows us to use standard ViT blocks with-\\nout modiﬁcation. After the ﬁnal block, we remove the\\nconditioning tokens from the sequence. This approach\\nintroduces negligible new Gﬂops to the model.\\n4', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 3}),\n",
       " Document(page_content='100K 200K 300K 400K\\nTraining Steps20406080100FID-50K\\nXL/2 In-Context\\nXL/2 Cross-Attention\\nXL/2 adaLN\\nXL/2 adaLN-ZeroFigure 5. Comparing different conditioning strategies. adaLN-\\nZero outperforms cross-attention and in-context conditioning at all\\nstages of training.\\n–Cross-attention block. We concatenate the embeddings\\noftandcinto a length-two sequence, separate from\\nthe image token sequence. The transformer block is\\nmodiﬁed to include an additional multi-head cross-\\nattention layer following the multi-head self-attention\\nblock, similar to the original design from Vaswani et\\nal.[60], and also similar to the one used by LDM for\\nconditioning on class labels. Cross-attention adds the\\nmost Gﬂops to the model, roughly a 15% overhead.\\n–Adaptive layer norm (adaLN) block. Following\\nthe widespread usage of adaptive normalization lay-\\ners [40] in GANs [2, 28] and diffusion models with U-\\nNet backbones [9], we explore replacing standard layer\\nnorm layers in transformer blocks with adaptive layer\\nnorm (adaLN). Rather than directly learn dimension-\\nwise scale and shift parameters γandβ, we regress\\nthem from the sum of the embedding vectors of tand\\nc. Of the three block designs we explore, adaLN adds\\nthe least Gﬂops and is thus the most compute-efﬁcient.\\nIt is also the only conditioning mechanism that is re-\\nstricted to apply the same function to all tokens.\\n–adaLN-Zero block. Prior work on ResNets has found\\nthat initializing each residual block as the identity\\nfunction is beneﬁcial. For example, Goyal et al. found\\nthat zero-initializing the ﬁnal batch norm scale factor γ\\nin each block accelerates large-scale training in the su-\\npervised learning setting [13]. Diffusion U-Net mod-\\nels use a similar initialization strategy, zero-initializing\\nthe ﬁnal convolutional layer in each block prior to any\\nresidual connections. We explore a modiﬁcation of\\nthe adaLN DiT block which does the same. In addi-\\ntion to regressing γandβ, we also regress dimension-\\nwise scaling parameters αthat are applied immediately\\nprior to any residual connections within the DiT block.Model Layers N Hidden size dHeads Gﬂops (I=32,p=4)\\nDiT-S 12 384 6 1.4\\nDiT-B 12 768 12 5.6\\nDiT-L 24 1024 16 19.7\\nDiT-XL 28 1152 16 29.1\\nTable 1. Details of DiT models. We follow ViT [10] model con-\\nﬁgurations for the Small (S), Base (B) and Large (L) variants; we\\nalso introduce an XLarge (XL) conﬁg as our largest model.\\nWe initialize the MLP to output the zero-vector for all\\nα; this initializes the full DiT block as the identity\\nfunction. As with the vanilla adaLN block, adaLN-\\nZero adds negligible Gﬂops to the model.\\nWe include the in-context, cross-attention, adaptive layer\\nnorm and adaLN-Zero blocks in the DiT design space.\\nModel size. We apply a sequence of NDiT blocks, each\\noperating at the hidden dimension size d. Following ViT,\\nwe use standard transformer conﬁgs that jointly scale N,\\ndand attention heads [10, 63]. Speciﬁcally, we use four\\nconﬁgs: DiT-S, DiT-B, DiT-L and DiT-XL. They cover a\\nwide range of model sizes and ﬂop allocations, from 0.3\\nto 118.6 Gﬂops, allowing us to gauge scaling performance.\\nTable 1 gives details of the conﬁgs.\\nWe add B, S, L and XL conﬁgs to the DiT design space.\\nTransformer decoder. After the ﬁnal DiT block, we need\\nto decode our sequence of image tokens into an output noise\\nprediction and an output diagonal covariance prediction.\\nBoth of these outputs have shape equal to the original spa-\\ntial input. We use a standard linear decoder to do this; we\\napply the ﬁnal layer norm (adaptive if using adaLN) and lin-\\nearly decode each token into a p×p×2Ctensor, where Cis\\nthe number of channels in the spatial input to DiT. Finally,\\nwe rearrange the decoded tokens into their original spatial\\nlayout to get the predicted noise and covariance.\\nThe complete DiT design space we explore is patch size,\\ntransformer block architecture and model size.\\n4. Experimental Setup\\nWe explore the DiT design space and study the scaling\\nproperties of our model class. Our models are named ac-\\ncording to their conﬁgs and latent patch sizes p; for exam-\\nple, DiT-XL/2 refers to the XLarge conﬁg and p= 2.\\nTraining. We train class-conditional latent DiT models at\\n256×256 and512×512 image resolution on the Ima-\\ngeNet dataset [31], a highly-competitive generative mod-\\neling benchmark. We initialize the ﬁnal linear layer with\\nzeros and otherwise use standard weight initialization tech-\\nniques from ViT. We train all models with AdamW [29,33].\\n5', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 4}),\n",
       " Document(page_content='Figure 6. Scaling the DiT model improves FID at all stages of training. We show FID-50K over training iterations for 12 of our DiT\\nmodels. Top row: We compare FID holding patch size constant. Bottom row: We compare FID holding model size constant. Scaling the\\ntransformer backbone yields better generative models across all model sizes and patch sizes.\\nWe use a constant learning rate of 1×10−4, no weight de-\\ncay and a batch size of 256. The only data augmentation\\nwe use is horizontal ﬂips. Unlike much prior work with\\nViTs [57, 61], we did not ﬁnd learning rate warmup nor\\nregularization necessary to train DiTs to high performance.\\nEven without these techniques, training was highly stable\\nacross all model conﬁgs and we did not observe any loss\\nspikes commonly seen when training transformers. Follow-\\ning common practice in the generative modeling literature,\\nwe maintain an exponential moving average (EMA) of DiT\\nweights over training with a decay of 0.9999. All results\\nreported use the EMA model. We use identical training hy-\\nperparameters across all DiT model sizes and patch sizes.\\nOur training hyperparameters are almost entirely retained\\nfrom ADM. We did not tune learning rates, decay/warm-up\\nschedules, Adam β1/β2or weight decays.\\nDiffusion. We use an off-the-shelf pre-trained variational\\nautoencoder (V AE) model [30] from Stable Diffusion [48].\\nThe V AE encoder has a downsample factor of 8—given an\\nRGB image xwith shape 256×256×3,z=E(x)has\\nshape 32×32×4. Across all experiments in this section,\\nour diffusion models operate in this Z-space. After sam-\\npling a new latent from our diffusion model, we decode it\\nto pixels using the V AE decoder x=D(z). We retain diffu-\\nsion hyperparameters from ADM [9]; speciﬁcally, we use a\\ntmax= 1000 linear variance schedule ranging from 1×10−4\\nto2×10−2, ADM’s parameterization of the covariance Σθ\\nand their method for embedding input timesteps and labels.\\nEvaluation metrics. We measure scaling performance\\nwith Fr ´echet Inception Distance (FID) [18], the standard\\nmetric for evaluating generative models of images.We follow convention when comparing against prior works\\nand report FID-50K using 250 DDPM sampling steps.\\nFID is known to be sensitive to small implementation de-\\ntails [37]; to ensure accurate comparisons, all values re-\\nported in this paper are obtained by exporting samples and\\nusing ADM’s TensorFlow evaluation suite [9]. FID num-\\nbers reported in this section do notuse classiﬁer-free guid-\\nance except where otherwise stated. We additionally report\\nInception Score [51], sFID [34] and Precision/Recall [32]\\nas secondary metrics.\\nCompute. We implement all models in JAX [1] and train\\nthem using TPU-v3 pods. DiT-XL/2, our most compute-\\nintensive model, trains at roughly 5.7 iterations/second on a\\nTPU v3-256 pod with a global batch size of 256.\\n5. Experiments\\nDiT block design. We train four of our highest Gﬂop\\nDiT-XL/2 models, each using a different block design—\\nin-context (119.4 Gﬂops), cross-attention (137.6 Gﬂops),\\nadaptive layer norm (adaLN, 118.6 Gﬂops) or adaLN-zero\\n(118.6 Gﬂops). We measure FID over the course of training.\\nFigure 5 shows the results. The adaLN-Zero block yields\\nlower FID than both cross-attention and in-context condi-\\ntioning while being the most compute-efﬁcient. At 400K\\ntraining iterations, the FID achieved with the adaLN-Zero\\nmodel is nearly half that of the in-context model, demon-\\nstrating that the conditioning mechanism critically affects\\nmodel quality. Initialization is also important—adaLN-\\nZero, which initializes each DiT block as the identity func-\\ntion, signiﬁcantly outperforms vanilla adaLN. For the rest\\nof the paper, all models will use adaLN-Zero DiT blocks.\\n6', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 5}),\n",
       " Document(page_content='Increasing transformer size \\nDecreasing patch size\\nFigure 7. Increasing transformer forward pass Gﬂops increases sample quality. Best viewed zoomed-in. We sample from all 12 of\\nour DiT models after 400K training steps using the same input latent noise and class label. Increasing the Gﬂops in the model—either by\\nincreasing transformer depth/width or increasing the number of input tokens—yields signiﬁcant improvements in visual ﬁdelity.\\n7', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 6}),\n",
       " Document(page_content='100101102\\nTransformer Gflops20406080100120140160FID-50K\\nCorrelation: -0.93S/8\\nS/4\\nS/2B/8\\nB/4\\nB/2L/8\\nL/4\\nL/2XL/8\\nXL/4\\nXL/2Figure 8. Transformer Gﬂops are strongly correlated with FID.\\nWe plot the Gﬂops of each of our DiT models and each model’s\\nFID-50K after 400K training steps.\\nScaling model size and patch size. We train 12 DiT mod-\\nels, sweeping over model conﬁgs (S, B, L, XL) and patch\\nsizes (8, 4, 2). Note that DiT-L and DiT-XL are signiﬁcantly\\ncloser to each other in terms of relative Gﬂops than other\\nconﬁgs. Figure 2 (left) gives an overview of the Gﬂops of\\neach model and their FID at 400K training iterations. In\\nall cases, we ﬁnd that increasing model size and decreasing\\npatch size yields considerably improved diffusion models.\\nFigure 6 (top) demonstrates how FID changes as model\\nsize is increased and patch size is held constant. Across all\\nfour conﬁgs, signiﬁcant improvements in FID are obtained\\nover all stages of training by making the transformer deeper\\nand wider. Similarly, Figure 6 (bottom) shows FID as patch\\nsize is decreased and model size is held constant. We again\\nobserve considerable FID improvements throughout train-\\ning by simply scaling the number of tokens processed by\\nDiT, holding parameters approximately ﬁxed.\\nDiT Gﬂops are critical to improving performance. The\\nresults of Figure 6 suggest that parameter counts do not\\nuniquely determine the quality of a DiT model. As model\\nsize is held constant and patch size is decreased, the trans-\\nformer’s total parameters are effectively unchanged (actu-\\nally, total parameters slightly decrease ), and only Gﬂops are\\nincreased. These results indicate that scaling model Gﬂops\\nis actually the key to improved performance. To investi-\\ngate this further, we plot the FID-50K at 400K training steps\\nagainst model Gﬂops in Figure 8. The results demonstrate\\nthat different DiT conﬁgs obtain similar FID values when\\ntheir total Gﬂops are similar (e.g., DiT-S/2 and DiT-B/4).\\nWe ﬁnd a strong negative correlation between model Gﬂops\\nand FID-50K, suggesting that additional model compute is\\nthe critical ingredient for improved DiT models. In Fig-\\nure 12 (appendix), we ﬁnd that this trend holds for other\\nmetrics such as Inception Score.\\n107108109101010111012\\nTraining Compute (Gflops)0255075100125150175200FID-50K\\n1015202530\\nS/8\\nS/4\\nS/2B/8\\nB/4\\nB/2L/8\\nL/4\\nL/2XL/8\\nXL/4\\nXL/2Figure 9. Larger DiT models use large compute more efﬁ-\\nciently. We plot FID as a function of total training compute.\\nLarger DiT models are more compute-efﬁcient. In\\nFigure 9, we plot FID as a function of total training compute\\nfor all DiT models. We estimate training compute as model\\nGﬂops·batch size·training steps·3, where the factor of\\n3 roughly approximates the backwards pass as being twice\\nas compute-heavy as the forward pass. We ﬁnd that small\\nDiT models, even when trained longer, eventually become\\ncompute-inefﬁcient relative to larger DiT models trained for\\nfewer steps. Similarly, we ﬁnd that models that are identi-\\ncal except for patch size have different performance proﬁles\\neven when controlling for training Gﬂops. For example,\\nXL/4 is outperformed by XL/2 after roughly 1010Gﬂops.\\nVisualizing scaling. We visualize the effect of scaling on\\nsample quality in Figure 7. At 400K training steps, we sam-\\nple an image from each of our 12 DiT models using iden-\\ntical starting noise xtmax, sampling noise and class labels.\\nThis lets us visually interpret how scaling affects DiT sam-\\nple quality. Indeed, scaling both model size and the number\\nof tokens yields notable improvements in visual quality.\\n5.1. State-of-the-Art Diffusion Models\\n256×256 ImageNet. Following our scaling analysis, we\\ncontinue training our highest Gﬂop model, DiT-XL/2, for\\n7M steps. We show samples from the model in Figures 1,\\nand we compare against state-of-the-art class-conditional\\ngenerative models. We report results in Table 2. When us-\\ning classiﬁer-free guidance, DiT-XL/2 outperforms all prior\\ndiffusion models, decreasing the previous best FID-50K of\\n3.60 achieved by LDM to 2.27. Figure 2 (right) shows that\\nDiT-XL/2 (118.6 Gﬂops) is compute-efﬁcient relative to la-\\ntent space U-Net models like LDM-4 (103.6 Gﬂops) and\\nsubstantially more efﬁcient than pixel space U-Net mod-\\nels such as ADM (1120 Gﬂops) or ADM-U (742 Gﬂops).\\n8', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 7}),\n",
       " Document(page_content='Class-Conditional ImageNet 256 ×256\\nModel FID ↓ sFID↓ IS↑ Precision↑Recall↑\\nBigGAN-deep [2] 6.95 7.36 171.4 0.87 0.28\\nStyleGAN-XL [53] 2.30 4.02 265.12 0.78 0.53\\nADM [9] 10.94 6.02 100.98 0.69 0.63\\nADM-U 7.49 5.13 127.49 0.72 0.63\\nADM-G 4.59 5.25 186.70 0.82 0.52\\nADM-G, ADM-U 3.94 6.14 215.84 0.83 0.53\\nCDM [20] 4.88 - 158.71 - -\\nLDM-8 [48] 15.51 - 79.03 0.65 0.63\\nLDM-8-G 7.76 - 209.52 0.84 0.35\\nLDM-4 10.56 - 103.49 0.71 0.62\\nLDM-4-G (cfg=1.25) 3.95 - 178.22 0.81 0.55\\nLDM-4-G (cfg=1.50) 3.60 - 247.67 0.87 0.48\\nDiT-XL/2 9.62 6.85 121.50 0.67 0.67\\nDiT-XL/2-G (cfg=1.25) 3.22 5.28 201.77 0.76 0.62\\nDiT-XL/2-G (cfg=1.50) 2.27 4.60 278.24 0.83 0.57\\nTable 2. Benchmarking class-conditional image generation on\\nImageNet 256×256. DiT-XL/2 achieves state-of-the-art FID.\\nClass-Conditional ImageNet 512 ×512\\nModel FID ↓ sFID↓ IS↑ Precision↑Recall↑\\nBigGAN-deep [2] 8.43 8.13 177.90 0.88 0.29\\nStyleGAN-XL [53] 2.41 4.06 267.75 0.77 0.52\\nADM [9] 23.24 10.19 58.06 0.73 0.60\\nADM-U 9.96 5.62 121.78 0.75 0.64\\nADM-G 7.72 6.57 172.71 0.87 0.42\\nADM-G, ADM-U 3.85 5.86 221.72 0.84 0.53\\nDiT-XL/2 12.03 7.12 105.25 0.75 0.64\\nDiT-XL/2-G (cfg=1.25) 4.64 5.77 174.77 0.81 0.57\\nDiT-XL/2-G (cfg=1.50) 3.04 5.02 240.82 0.84 0.54\\nTable 3. Benchmarking class-conditional image generation on\\nImageNet 512×512. Note that prior work [9] measures Precision\\nand Recall using 1000 real samples for 512×512resolution; for\\nconsistency, we do the same.\\nOur method achieves the lowest FID of all prior generative\\nmodels, including the previous state-of-the-art StyleGAN-\\nXL [53]. Finally, we also observe that DiT-XL/2 achieves\\nhigher recall values at all tested classiﬁer-free guidance\\nscales compared to LDM-4 and LDM-8. When trained for\\nonly 2.35M steps (similar to ADM), XL/2 still outperforms\\nall prior diffusion models with an FID of 2.55.\\n512×512 ImageNet. We train a new DiT-XL/2 model on\\nImageNet at 512×512resolution for 3M iterations with\\nidentical hyperparameters as the 256×256model. With a\\npatch size of 2, this XL/2 model processes a total of 1024\\ntokens after patchifying the 64×64×4input latent (524.6\\nGﬂops). Table 3 shows comparisons against state-of-the-art\\nmethods. XL/2 again outperforms all prior diffusion models\\nat this resolution, improving the previous best FID of 3.85\\nachieved by ADM to 3.04. Even with the increased num-\\nber of tokens, XL/2 remains compute-efﬁcient. For exam-\\nple, ADM uses 1983 Gﬂops and ADM-U uses 2813 Gﬂops;\\nXL/2 uses 524.6 Gﬂops. We show samples from the high-\\nresolution XL/2 model in Figure 1 and the appendix.\\n101\\n102\\n103\\n104\\n105\\nSampling Compute (Gflops)20406080100120140160180FID-10K\\nS/8\\nS/4\\nS/2B/8\\nB/4\\nB/2L/8\\nL/4\\nL/2XL/8\\nXL/4\\nXL/2Figure 10. Scaling-up sampling compute does not compensate\\nfor a lack of model compute. For each of our DiT models trained\\nfor 400K iterations, we compute FID-10K using [16, 32, 64, 128,\\n256, 1000] sampling steps. For each number of steps, we plot the\\nFID as well as the Gﬂops used to sample each image. Small mod-\\nels cannot close the performance gap with our large models, even\\nif they sample with more test-time Gﬂops than the large models.\\n5.2. Scaling Model vs. Sampling Compute\\nDiffusion models are unique in that they can use addi-\\ntional compute after training by increasing the number of\\nsampling steps when generating an image. Given the im-\\npact of model Gﬂops on sample quality, in this section we\\nstudy if smaller- model compute DiTs can outperform larger\\nones by using more sampling compute . We compute FID\\nfor all 12 of our DiT models after 400K training steps, us-\\ning [16, 32, 64, 128, 256, 1000] sampling steps per-image.\\nThe main results are in Figure 10. Consider DiT-L/2 us-\\ning 1000 sampling steps versus DiT-XL/2 using 128 steps.\\nIn this case, L/2 uses 80.7Tﬂops to sample each image;\\nXL/2 uses 5×less compute— 15.2Tﬂops—to sample each\\nimage. Nonetheless, XL/2 has the better FID-10K (23.7\\nvs 25.9). In general, scaling-up sampling compute cannot\\ncompensate for a lack of model compute.\\n6. Conclusion\\nWe introduce Diffusion Transformers (DiTs), a simple\\ntransformer-based backbone for diffusion models that out-\\nperforms prior U-Net models and inherits the excellent scal-\\ning properties of the transformer model class. Given the\\npromising scaling results in this paper, future work should\\ncontinue to scale DiTs to larger models and token counts.\\nDiT could also be explored as a drop-in backbone for text-\\nto-image models like DALL ·E 2 and Stable Diffusion.\\nAcknowledgements. We thank Kaiming He, Ronghang\\nHu, Alexander Berg, Shoubhik Debnath, Tim Brooks, Ilija\\nRadosavovic and Tete Xiao for helpful discussions. William\\nPeebles is supported by the NSF GRFP.\\n9', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 8}),\n",
       " Document(page_content='References\\n[1] James Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal Maclau-\\nrin, George Necula, Adam Paszke, Jake VanderPlas, Skye\\nWanderman-Milne, and Qiao Zhang. JAX: composable\\ntransformations of Python+NumPy programs, 2018. 6\\n[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\\nscale GAN training for high ﬁdelity natural image synthesis.\\nInICLR , 2019. 5, 9\\n[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\\nguage models are few-shot learners. In NeurIPS , 2020. 1\\n[4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\\nFreeman. Maskgit: Masked generative image transformer. In\\nCVPR , pages 11315–11325, 2022. 2\\n[5] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,\\nAditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srini-\\nvas, and Igor Mordatch. Decision transformer: Reinforce-\\nment learning via sequence modeling. In NeurIPS , 2021. 2\\n[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\\ntraining from pixels. In ICML , 2020. 1, 2\\n[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\\nGenerating long sequences with sparse transformers. arXiv\\npreprint arXiv:1904.10509 , 2019. 2\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\\nToutanova. Bert: Pre-training of deep bidirectional trans-\\nformers for language understanding. In NAACL-HCT , 2019.\\n1\\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models\\nbeat gans on image synthesis. In NeurIPS , 2021. 1, 2, 3, 5,\\n6, 9, 12\\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. In ICLR , 2020. 1, 2,\\n4, 5\\n[11] Patrick Esser, Robin Rombach, and Bj ¨orn Ommer. Taming\\ntransformers for high-resolution image synthesis, 2020. 2\\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\\nYoshua Bengio. Generative adversarial nets. In NIPS , 2014.\\n3\\n[13] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noord-\\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\\nYangqing Jia, and Kaiming He. Accurate, large minibatch\\nsgd: Training imagenet in 1 hour. arXiv:1706.02677 , 2017.\\n5\\n[14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\\ntor quantized diffusion model for text-to-image synthesis. In\\nCVPR , pages 10696–10706, 2022. 2\\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In CVPR ,\\n2016. 2[16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 12\\n[17] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\\nChristopher Hesse, Jacob Jackson, Heewoo Jun, Tom B\\nBrown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws\\nfor autoregressive generative modeling. arXiv preprint\\narXiv:2010.14701 , 2020. 2\\n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\\ntwo time-scale update rule converge to a local nash equilib-\\nrium. 2017. 6\\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\\nsion probabilistic models. In NeurIPS , 2020. 2, 3\\n[20] Jonathan Ho, Chitwan Saharia, William Chan, David J\\nFleet, Mohammad Norouzi, and Tim Salimans. Cas-\\ncaded diffusion models for high ﬁdelity image generation.\\narXiv:2106.15282 , 2021. 3, 9\\n[21] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion\\nguidance. In NeurIPS 2021 Workshop on Deep Generative\\nModels and Downstream Applications , 2021. 3, 4\\n[22] Aapo Hyv ¨arinen and Peter Dayan. Estimation of non-\\nnormalized statistical models by score matching. Journal\\nof Machine Learning Research , 6(4), 2005. 3\\n[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\\nEfros. Image-to-image translation with conditional adver-\\nsarial networks. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 1125–1134,\\n2017. 2\\n[24] Allan Jabri, David Fleet, and Ting Chen. Scalable adap-\\ntive computation for iterative generation. arXiv preprint\\narXiv:2212.11972 , 2022. 3\\n[25] Michael Janner, Qiyang Li, and Sergey Levine. Ofﬂine rein-\\nforcement learning as one big sequence modeling problem.\\nInNeurIPS , 2021. 2\\n[26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\\nneural language models. arXiv:2001.08361 , 2020. 2, 13\\n[27] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\\nElucidating the design space of diffusion-based generative\\nmodels. In Proc. NeurIPS , 2022. 3\\n[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based\\ngenerator architecture for generative adversarial networks. In\\nCVPR , 2019. 5\\n[29] Diederik Kingma and Jimmy Ba. Adam: A method for\\nstochastic optimization. In ICLR , 2015. 5\\n[30] Diederik P Kingma and Max Welling. Auto-encoding varia-\\ntional bayes. arXiv preprint arXiv:1312.6114 , 2013. 3, 6\\n[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\\nImagenet classiﬁcation with deep convolutional neural net-\\nworks. In NeurIPS , 2012. 5\\n[32] Tuomas Kynk ¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko\\nLehtinen, and Timo Aila. Improved precision and recall met-\\nric for assessing generative models. In NeurIPS , 2019. 6\\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv:1711.05101 , 2017. 5\\n10', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 9}),\n",
       " Document(page_content='[34] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W\\nBattaglia. Generating images with sparse representations.\\narXiv preprint arXiv:2103.03841 , 2021. 6\\n[35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\\nand Mark Chen. Glide: Towards photorealistic image\\ngeneration and editing with text-guided diffusion models.\\narXiv:2112.10741 , 2021. 3, 4\\n[36] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\\ndenoising diffusion probabilistic models. In ICML , 2021. 3\\n[37] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On\\naliased resizing and surprising subtleties in gan evaluation.\\nInCVPR , 2022. 6\\n[38] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\\nage transformer. In International conference on machine\\nlearning , pages 4055–4064. PMLR, 2018. 2\\n[39] William Peebles, Ilija Radosavovic, Tim Brooks, Alexei\\nEfros, and Jitendra Malik. Learning to learn with genera-\\ntive models of neural network checkpoints. arXiv preprint\\narXiv:2209.12892 , 2022. 2\\n[40] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-\\nmoulin, and Aaron Courville. Film: Visual reasoning with a\\ngeneral conditioning layer. In AAAI , 2018. 2, 5\\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\\ning transferable visual models from natural language super-\\nvision. In ICML , 2021. 2\\n[42] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\\nSutskever. Improving language understanding by generative\\npre-training. 2018. 1\\n[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\\nAmodei, Ilya Sutskever, et al. Language models are unsu-\\npervised multitask learners. 2019. 1\\n[44] Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo,\\nand Piotr Doll ´ar. On network design spaces for visual recog-\\nnition. In ICCV , 2019. 3\\n[45] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\\nKaiming He, and Piotr Doll ´ar. Designing network design\\nspaces. In CVPR , 2020. 3\\n[46] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\\nand Mark Chen. Hierarchical text-conditional image gener-\\nation with clip latents. arXiv:2204.06125 , 2022. 1, 2, 3, 4\\n[47] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\\nChelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.\\nZero-shot text-to-image generation. In ICML , 2021. 1, 2\\n[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image syn-\\nthesis with latent diffusion models. In CVPR , 2022. 2, 3, 4,\\n6, 9\\n[49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\\nnet: Convolutional networks for biomedical image segmen-\\ntation. In International Conference on Medical image com-\\nputing and computer-assisted intervention , pages 234–241.\\nSpringer, 2015. 2, 3[50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\\nFleet, and Mohammad Norouzi. Photorealistic text-to-\\nimage diffusion models with deep language understanding.\\narXiv:2205.11487 , 2022. 3\\n[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\\nCheung, Alec Radford, Xi Chen, and Xi Chen. Improved\\ntechniques for training GANs. In NeurIPS , 2016. 6\\n[52] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P\\nKingma. PixelCNN++: Improving the pixelcnn with dis-\\ncretized logistic mixture likelihood and other modiﬁcations.\\narXiv preprint arXiv:1701.05517 , 2017. 2\\n[53] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-\\nxl: Scaling stylegan to large diverse datasets. In SIGGRAPH ,\\n2022. 9\\n[54] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\\nand Surya Ganguli. Deep unsupervised learning using\\nnonequilibrium thermodynamics. In ICML , 2015. 3\\n[55] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\\ning diffusion implicit models. arXiv:2010.02502 , 2020. 3\\n[56] Yang Song and Stefano Ermon. Generative modeling by es-\\ntimating gradients of the data distribution. In NeurIPS , 2019.\\n3\\n[57] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross\\nWightman, Jakob Uszkoreit, and Lucas Beyer. How to train\\nyour ViT? data, augmentation, and regularization in vision\\ntransformers. TMLR , 2022. 6\\n[58] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt,\\nOriol Vinyals, Alex Graves, et al. Conditional image genera-\\ntion with pixelcnn decoders. Advances in neural information\\nprocessing systems , 29, 2016. 2\\n[59] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\\nrepresentation learning. Advances in neural information pro-\\ncessing systems , 30, 2017. 2\\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 1,\\n2, 5\\n[61] Tete Xiao, Piotr Dollar, Mannat Singh, Eric Mintun, Trevor\\nDarrell, and Ross Girshick. Early convolutions help trans-\\nformers see better. In NeurIPS , 2021. 6\\n[62] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,\\nGunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku,\\nYinfei Yang, Burcu Karagol Ayan, et al. Scaling autore-\\ngressive models for content-rich text-to-image generation.\\narXiv:2206.10789 , 2022. 2\\n[63] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-\\ncas Beyer. Scaling vision transformers. In CVPR , 2022. 2,\\n5\\n11', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 10}),\n",
       " Document(page_content='Figure 11. Additional selected samples from our 512 ×512 and 256×256 resolution DiT-XL/2 models. We use a classiﬁer-free guidance\\nscale of 6.0 for the 512×512model and 4.0 for the 256×256model. Both models use the ft-EMA V AE decoder.\\nA. Additional Implementation Details\\nWe include detailed information about all of our DiT\\nmodels in Table 4, including both 256×256and512×512\\nmodels. In Figure 13, we report DiT training loss curves.\\nFinally, we also include Gﬂop counts for DDPM U-Net\\nmodels from ADM and LDM in Table 6.\\nDiT model details. To embed input timesteps, we use\\na 256-dimensional frequency embedding [9] followed by\\na two-layer MLP with dimensionality equal to the trans-\\nformer’s hidden size and SiLU activations. Each adaLN\\nlayer feeds the sum of the timestep and class embeddings\\ninto a SiLU nonlinearity and a linear layer with output neu-\\nrons equal to either 4×(adaLN) or 6×(adaLN-Zero) the\\ntransformer’s hidden size. We use GELU nonlinearities (ap-\\nproximated with tanh) in the core transformer [16].\\nClassiﬁer-free guidance on a subset of channels. In our\\nexperiments using classiﬁer-free guidance, we applied guid-\\nance only to the ﬁrst three channels of the latents instead of\\nall four channels. Upon investigating, we found that three-\\nchannel guidance and four-channel guidance give similarresults (in terms of FID) when simply adjusting the scale\\nfactor. Speciﬁcally, three-channel guidance with a scale\\nof(1 +x)appears reasonably well-approximated by four-\\nchannel guidance with a scale of (1 +3\\n4x)(e.g., three-\\nchannel guidance with a scale of 1.5gives an FID-50K of\\n2.27, and four-channel guidance with a scale of 1.375gives\\nan FID-50K of 2.20). It is somewhat interesting that ap-\\nplying guidance to a subset of elements can still yield good\\nperformance, and we leave it to future work to explore this\\nphenomenon further.\\nB. Model Samples\\nWe show samples from our two DiT-XL/2 models at\\n512×512and256×256resolution trained for 3M and 7M\\nsteps, respectively. Figures 1 and 11 show selected samples\\nfrom both models. Figures 14 through 33 show uncurated\\nsamples from the two models across a range of classiﬁer-\\nfree guidance scales and input class labels (generated with\\n250 DDPM sampling steps and the ft-EMA V AE decoder).\\nAs with prior work using guidance, we observe that larger\\nscales increase visual ﬁdelity and decrease sample diversity.\\n12', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 11}),\n",
       " Document(page_content='Model Image Resolution Flops (G) Params (M) Training Steps (K) Batch Size Learning Rate DiT Block FID-50K (no guidance)\\nDiT-S/8 256×256 0.36 33 400 256 1×10−4adaLN-Zero 153.60\\nDiT-S/4 256×256 1.41 33 400 256 1×10−4adaLN-Zero 100.41\\nDiT-S/2 256×256 6.06 33 400 256 1×10−4adaLN-Zero 68.40\\nDiT-B/8 256×256 1.42 131 400 256 1×10−4adaLN-Zero 122.74\\nDiT-B/4 256×256 5.56 130 400 256 1×10−4adaLN-Zero 68.38\\nDiT-B/2 256×256 23.01 130 400 256 1×10−4adaLN-Zero 43.47\\nDiT-L/8 256×256 5.01 459 400 256 1×10−4adaLN-Zero 118.87\\nDiT-L/4 256×256 19.70 458 400 256 1×10−4adaLN-Zero 45.64\\nDiT-L/2 256×256 80.71 458 400 256 1×10−4adaLN-Zero 23.33\\nDiT-XL/8 256×256 7.39 676 400 256 1×10−4adaLN-Zero 106.41\\nDiT-XL/4 256×256 29.05 675 400 256 1×10−4adaLN-Zero 43.01\\nDiT-XL/2 256×256 118.64 675 400 256 1×10−4adaLN-Zero 19.47\\nDiT-XL/2 256×256 119.37 449 400 256 1×10−4in-context 35.24\\nDiT-XL/2 256×256 137.62 598 400 256 1×10−4cross-attention 26.14\\nDiT-XL/2 256×256 118.56 600 400 256 1×10−4adaLN 25.21\\nDiT-XL/2 256×256 118.64 675 2352 256 1×10−4adaLN-Zero 10.67\\nDiT-XL/2 256×256 118.64 675 7000 256 1×10−4adaLN-Zero 9.62\\nDiT-XL/2 512×512 524.60 675 1301 256 1×10−4adaLN-Zero 13.78\\nDiT-XL/2 512×512 524.60 675 3000 256 1×10−4adaLN-Zero 11.93\\nTable 4. Details of all DiT models. We report detailed information about every DiT model in our paper. Note that FID-50K here is\\ncomputed without classiﬁer-free guidance. Parameter and ﬂop counts exclude the V AE model which contains 84M parameters across the\\nencoder and decoder. For both the 256×256and512×512DiT-XL/2 models, we never observed FID saturate and continued training\\nthem as long as possible. Numbers reported in this table use the ft-MSE V AE decoder.\\nC. Additional Scaling Results\\nImpact of scaling on metrics beyond FID. In Figure 12,\\nwe show the effects of DiT scale on a suite of evaluation\\nmetrics—FID, sFID, Inception Score, Precision and Recall.\\nWe ﬁnd that our FID-driven analysis in the main paper gen-\\neralizes to the other metrics—across every metric, scaled-up\\nDiT models are more compute-efﬁcient and model Gﬂops\\nare highly-correlated with performance. In particular, In-\\nception Score and Precision beneﬁt heavily from increased\\nmodel scale.\\nImpact of scaling on training loss. We also examine the\\nimpact of scale on training loss in Figure 13. Increasing\\nDiT model Gﬂops (via transformer size or number of input\\ntokens) causes the training loss to decrease more rapidly and\\nsaturate at a lower value. This phenomenon is consistent\\nwith trends observed with language models, where scaled-\\nup transformers demonstrate both improved loss curves as\\nwell as improved performance on downstream evaluation\\nsuites [26].\\nD. V AE Decoder Ablations\\nWe used off-the-shelf, pre-trained V AEs across our ex-\\nperiments. The V AE models (ft-MSE and ft-EMA) are ﬁne-\\ntuned versions of the original LDM “f8” model (only the\\ndecoder weights are ﬁne-tuned). We monitored metrics for\\nour scaling analysis in Section 5 using the ft-MSE decoder,\\nand we used the ft-EMA decoder for our ﬁnal metrics re-\\nported in Tables 2 and 3. In this section, we ablate threeClass-Conditional ImageNet 256 ×256, DiT-XL/2-G (cfg=1.5)\\nDecoder FID↓sFID↓ IS↑ Precision↑ Recall↑\\noriginal 2.46 5.18 271.56 0.82 0.57\\nft-MSE 2.30 4.73 276.09 0.83 0.57\\nft-EMA 2.27 4.60 278.24 0.83 0.57\\nTable 5. Decoder ablation. We tested different pre-trained V AE\\ndecoder weights available at https://huggingface.co/\\nstabilityai/sd-vae-ft-mse . Different pre-trained de-\\ncoder weights yield comparable results on ImageNet 256×256.\\nDiffusion U-Net Model Complexities\\nModel Image Resolution Base Flops (G) Upsampler Flops (G) Total Flops (G)\\nADM 128×128 307 - 307\\nADM 256×256 1120 - 1120\\nADM 512×512 1983 - 1983\\nADM-U 256×256 110 632 742\\nADM-U 512×512 307 2506 2813\\nLDM-4 256×256 104 - 104\\nLDM-8 256×256 57 - 57\\nTable 6. Gﬂop counts for baseline diffusion models that use U-\\nNet backbones. Note that we only count Flops for DDPM com-\\nponents.\\ndifferent choices of the V AE decoder; the original one used\\nby LDM and the two ﬁne-tuned decoders used by Stable\\nDiffusion. Because the encoders are identical across mod-\\nels, the decoders can be swapped-in without retraining the\\ndiffusion model. Table 5 shows results; XL/2 continues to\\noutperform all prior diffusion models when using the LDM\\ndecoder.\\n13', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 12}),\n",
       " Document(page_content='Figure 12. DiT scaling behavior on several generative modeling metrics. Left: We plot model performance as a function of total training\\ncompute for FID, sFID, Inception Score, Precision and Recall. Right: We plot model performance at 400K training steps for all 12 DiT\\nvariants against transformer Gﬂops, ﬁnding strong correlations across metrics. All values were computed using the ft-MSE V AE decoder.\\n14', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 13}),\n",
       " Document(page_content='0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M\\nTraining Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20S/8 S/4 S/2\\n0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M\\nTraining Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20B/8 B/4 B/2\\n0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M\\nTraining Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20L/8 L/4 L/2\\n0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M\\nTraining Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20XL/8 XL/4 XL/2\\n0 0.25M 0.50M 0.75M 1.00M 1.25M 1.50M 1.75M 2.00M 2.25M 2.50M 2.75M 3.00M 3.25M 3.50M 3.75M 4.00M 4.25M 4.50M 4.75M 5.00M 5.25M 5.50M 5.75M 6.00M 6.25M 6.50M 6.75M 7.00M\\nTraining Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20XL/2 (256x256) XL/2 (512x512)Figure 13. Training loss curves for all DiT models. We plot the loss over training for all DiT models (the sum of the noise prediction\\nmean-squared error and DKL). We also highlight early training behavior. Note that scaled-up DiT models exhibit lower training losses.\\n15', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 14}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 4.0\\nFigure 14. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “arctic wolf” (270)\\nFigure 15. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “volcano” (980)\\n16', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 15}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 4.0\\nFigure 16. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “husky” (250)\\nFigure 17. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “sulphur-crested cockatoo” (89)\\n17', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 16}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 4.0\\nFigure 18. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “cliff drop-off” (972)\\nFigure 19. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “balloon” (417)\\n18', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 17}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 4.0\\nFigure 20. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “lion” (291)\\nFigure 21. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “otter” (360)\\n19', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 18}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 2.0\\nFigure 22. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 2.0\\nClass label = “red panda” (387)\\nFigure 23. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 2.0\\nClass label = “panda” (388)\\n20', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 19}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 1.5\\nFigure 24. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 1.5\\nClass label = “coral reef” (973)\\nFigure 25. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 1.5\\nClass label = “macaw” (88)\\n21', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 20}),\n",
       " Document(page_content='DiT-XL/2 256×256samples, classiﬁer-free guidance scale = 4.0\\nFigure 26. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “macaw” (88)\\nFigure 27. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “dog sled” (537)\\n22', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 21}),\n",
       " Document(page_content='DiT-XL/2 256×256samples, classiﬁer-free guidance scale = 4.0\\nFigure 28. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “arctic fox” (279)\\nFigure 29. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “loggerhead sea turtle” (33)\\n23', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 22}),\n",
       " Document(page_content='DiT-XL/2 256×256samples, classiﬁer-free guidance scale = 2.0\\nFigure 30. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 2.0\\nClass label = “golden retriever” (207)\\nFigure 31. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 2.0\\nClass label = “lake shore” (975)\\n24', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 23}),\n",
       " Document(page_content='DiT-XL/2 256×256samples, classiﬁer-free guidance scale = 1.5\\nFigure 32. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 1.5\\nClass label = “space shuttle” (812)\\nFigure 33. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 1.5\\nClass label = “ice cream” (928)\\n25', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 24}),\n",
       " Document(page_content='Neural Network Diffusion\\nKai Wang1Zhaopan Xu1Yukun Zhou1Zelin Zang1Trevor Darrell2Zhuang Liu* 3Yang You* 1\\nCode: https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion\\nAbstract\\nDiffusion models have achieved remarkable suc-\\ncess in image and video generation. In this work,\\nwe demonstrate that diffusion models can also\\ngenerate high-performing neural network param-\\neters . Our approach is simple, utilizing an au-\\ntoencoder and a standard latent diffusion model.\\nThe autoencoder extracts latent representations\\nof a subset of the trained network parameters.\\nA diffusion model is then trained to synthesize\\nthese latent parameter representations from ran-\\ndom noise. It then generates new representations\\nthat are passed through the autoencoder’s decoder,\\nwhose outputs are ready to use as new subsets of\\nnetwork parameters. Across various architectures\\nand datasets, our diffusion process consistently\\ngenerates models of comparable or improved per-\\nformance over trained networks, with minimal\\nadditional cost. Notably, we empirically find that\\nthe generated models perform differently with the\\ntrained networks. Our results encourage more ex-\\nploration on the versatile use of diffusion models.\\n1. Introduction\\nThe origin of diffusion models can be traced back to\\nnon-equilibrium thermodynamics (Jarzynski, 1997; Sohl-\\nDickstein et al., 2015). Diffusion processes were first uti-\\nlized to progressively remove noise from inputs and generate\\nclear images in (Sohl-Dickstein et al., 2015). Later works,\\nsuch as DDPM (Ho et al., 2020) and DDIM (Song et al.,\\n2021), refine diffusion models, with a training paradigm\\ncharacterized by forward and reverse processes.\\nAt that time, the quality of images generated by diffu-\\nsion models had not yet reached a desired level. Guided-\\nDiffusion (Dhariwal & Nichol, 2021) conducts sufficient\\nablations and finds a better architecture, which represents\\nthe pioneering effort to elevate diffusion models beyond\\nGAN-based methods (Zhu et al., 2017; Isola et al., 2017) in\\n*Equal advising,1National University of Singapore2University\\nof California, Berkeley3Meta AI Research.\\narXiv preprint\\nImage\\nNoiseForward ProcessReverse ProcessModel\\nInitial.Adding NoiseSGD Optimization\\nAcc:76.6Acc:64.0Acc:42.1Acc:1.4\\nmin.maxFigure 1. The top: illustrates the standard diffusion process in\\nimage generation. The bottom: denotes the parameter distribution\\nof batch normalization (BN) during the training CIFAR-100 with\\nResNet-18. The upper half of the bracket: BN weights. The lower\\nhalf of the bracket: BN biases.\\nterms of image quality. Subsequently, GLIDE (Nichol et al.,\\n2021), Imagen (Saharia et al., 2022), DALL ·E 2 (Ramesh\\net al., 2022), and Stable Diffusion (Rombach et al., 2022)\\nachieve photorealistic images adopted by artists.\\nDespite the great success of diffusion models in visual gener-\\nation, their potential in other domains remains relatively un-\\nderexplored. In this work, we demonstrate the surprising ca-\\npability of diffusion models in generating high-performing\\nmodel parameters , a task fundamentally distinct from tradi-\\ntional visual generation. Parameter generation focuses on\\ncreating neural network parameters that can perform well on\\ngiven tasks. It has been explored from prior and probability\\nmodeling aspects, i.e.stochastic neural network (Sompolin-\\nsky et al., 1988; Bottou et al., 1991; Wong, 1991; Schmidt\\net al., 1992; Murata et al., 1994) and Bayesian neural net-\\nwork (Neal, 2012; Kingma & Welling, 2013; Rezende et al.,\\n2014; Kingma et al., 2015; Gal & Ghahramani, 2016). How-\\never, using a diffusion model in parameter generation has\\nnot been well-explored yet.\\nTaking a closer look at the neural network training and dif-\\nfusion models, the diffusion-based image generation shares\\ncommonalities with the stochastic gradient descent (SGD)\\nlearning process in the following aspects (illustrated in\\n1arXiv:2402.13144v1  [cs.LG]  20 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 0}),\n",
       " Document(page_content='Neural Network Diffusion\\nFig. 1). i) Both neural network training and the reverse\\nprocess of diffusion models can be regarded as transitions\\nfrom random noise/initialization to specific distributions. ii)\\nHigh-quality images and high-performing parameters can\\nalso be degraded into simple distributions, such as Gaussian\\ndistribution, through multiple noise additions.\\nBased on the observations above, we introduce a novel ap-\\nproach for parameter generation, named neural network\\ndiffusion ( p-diff , p stands for parameter), which employs a\\nstandard latent diffusion model to synthesize a new set of\\nparameters. That is motivated by the fact that the diffusion\\nmodel has the capability to transform a given random distri-\\nbution to a specific one. Our method is simple, comprising\\nan autoencoder and a standard latent diffusion model to\\nlearn the distribution of high-performing parameters. First,\\nfor a subset of parameters of models trained by the SGD\\noptimizer, the autoencoder is trained to extract the latent\\nrepresentations for these parameters. Then, we leverage a\\nstandard latent diffusion model to synthesize latent represen-\\ntations from random noise. Finally, the synthesized latent\\nrepresentations are passed through the trained autoencoder’s\\ndecoder to yield new high-performing model parameters.\\nOur approach has the following characteristics: i) It consis-\\ntently achieves similar, even enhanced performance than its\\ntraining data, i.e.,models trained by SGD optimizer, across\\nmultiple datasets and architectures within seconds. ii) Our\\ngenerated models have great differences from the trained\\nmodels, which illustrates our approach can synthesize new\\nparameters instead of memorizing the training samples. We\\nhope our research can provide fresh insights into expanding\\nthe applications of diffusion models to other domains.\\n2. Nerual Network Diffusion\\n2.1. Preliminaries of diffusion models\\nDiffusion models typically consist of forward and reverse\\nprocesses in a multi-step chain indexed by timesteps. We\\nintroduce these two processes in the following.\\nForward process. Given a sample x0∼q(x), the forward\\nprocess progressively adds Gaussian noise for Tsteps and\\nobtain x1, x2,···, xT. The formulation of this process can\\nbe written as follows,\\nq(xt|xt−1) =N(xt;p\\n1−βtxt−1, βtI),\\nq(x1:T|x0) =TY\\nt=1q(xt|xt−1),(1)\\nwhere qandNrepresent forward process and adding Gaus-\\nsian noise parameterized by βt, and Iis the identity matrix.\\nReverse process. Different from the forward process, thereverse process aims to train a denoising network to recur-\\nsively remove the noise from xt. It moves backward on the\\nmulti-step chain as t decreases from Tto 0. Mathematically,\\nthe reverse process can be formulated as follows,\\npθ(xt−1|xt) =N(xt−1;µθ(xt, t),Σθ(xt, t)),\\npθ(x0:T) =p(xT)TY\\nt=1pθ(xt−1|xt),(2)\\nwhere prepresents the reverse process, µθ(xt, t)and\\nΣθ(xt, t))are the Gaussian mean and variance that esti-\\nmated by the denoising network parameter θ. The denoising\\nnetwork in the reverse process is optimized by the standard\\nnegative log-likelihood:\\nLdm=DKL(q(xt−1|xt, x0)||pθ(xt−1|xt)), (3)\\nwhere the DKL(·||·)denotes the Kullback–Leibler (KL)\\ndivergence that is normally used to compute the difference\\nbetween two distributions.\\nTraining and inference procedures. The goal of the train-\\ning diffusion model is to find the reverse transitions that\\nmaximize the likelihood of the forward transitions in each\\ntime step t. In practice, training equivalently consists of\\nminimizing the variational upper bound. The inference pro-\\ncedure aims to generate novel samples from random noise\\nvia the optimized denoising parameters θ∗and the multi-\\nstep chains in the reverse process.\\n2.2. Overview\\nWe propose neural network diffusion (p-diff), which aims\\nto generate high-performing parameters from random noise.\\nAs illustrated in Fig. 2, our method consists of two processes,\\nnamed parameter autoencoder and generation. Given a set\\nof trained high-performing models, we first select a subset\\nof these parameters and flatten them into 1-dimensional\\nvectors. Subsequently, we introduce an encoder to extract\\nlatent representations from these vectors, accompanied by a\\ndecoder responsible for reconstructing the parameters from\\nlatent representations. Then, a standard latent diffusion\\nmodel is trained to synthesize latent representations from\\nrandom noise. After training, we utilize p-diff to generate\\nnew parameters via the following chain: random noise →\\nreverse process →trained decoder →generated parameters.\\n2.3. Parameter autoencoder\\nPreparing the data for training the autoencoder. In our\\npaper, we default to synthesizing a subset of model pa-\\nrameters. Therefore, to collect the training data for the\\nautoencoder, we train a model from scratch and densely\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 1}),\n",
       " Document(page_content='Neural Network Diffusion\\nEncoderInputParametersLatent RepresentationsParameterAutoencoder\\nDecoderLDMRandomNoiseLatent RepresentationsParameterGeneration (standard latent diffusion)\\nGenerated ParametersLDMInference: Forward/Reverse Process/: FrozenRandomNoise\\nDecoder\\nFigure 2. Our approach consists of two processes, named parameter autoencoder and generation. Parameter autoencoder aims to extract\\nthe latent representations and reconstruct model parameters via the decoder. The extracted representations are used to train a standard\\nlatent diffusion model (LDM). In the inference, the random noise is fed into LDM and trained decoder to obtain the generated parameters.\\nsave checkpoints in the last epoch. It is worth noting that\\nwe only update the selected subset of parameters via SGD\\noptimizer and fix the remained parameters of the model.\\nThe saved subsets of parameters S= [s1, . . . , s k, . . . , s K]\\nis utilized to train the autoencoder, where Kis the number\\nof the training samples. For some large architectures that\\nhave been trained on large-scale datasets, considering the\\ncost of training them from scratch, we fine-tune a subset of\\nthe parameters of the pre-trained model and densely save\\nthe fine-tuned parameters as training samples.\\nTraining parameter autoencoder. We then flatten\\nthese parameters Sinto 1-dimensional vectors V=\\n[v1, . . . , v k, . . . , v K], where V∈RK×DandDis the size\\nof the subset parameters. After that, an autoencoder is\\ntrained to reconstruct these parameters V. To enhance the\\nrobustness and generalization of the autoencoder, we intro-\\nduce random noise augmentation in input parameters and\\nlatent representations simultaneously. The encoding and\\ndecoding processes can be formulated as,\\nZ= [z0\\n1, . . . , z0\\nk, . . . , z0\\nK] =fencoder (V+ξV, σ)| {z }\\nencoding;\\nV′= [v′\\n1,···, v′\\nk,···, v′\\nK] =fdecoder (Z+ξZ, ρ)| {z }\\ndecoding,(4)\\nwhere fencoder (·, σ)andfdecoder (·, ρ)denote the encoder\\nand decoder parameterized by σandρ, respectively. Zrepresents the latent representations, ξVandξZdenote ran-\\ndom noise that are added into input parameters Vand latent\\nrepresentations Z, and V′is the reconstructed parameters.\\nWe default to using an autoencoder with a 4-layer encoder\\nand decoder. Same as the normal autoencoder training, we\\nminimize the mean square error (MSE) loss between V′and\\nVas follows,\\nLMSE=1\\nKXK\\n1∥vk−v′\\nk∥2, (5)\\nwhere v′\\nkis the reconstructed parameters of k-th model.\\n2.4. Parameter generation\\nOne of the most direct strategies is to synthesize the novel\\nparameters via a diffusion model. However, the memory\\ncost of this operation is too heavy, especially when the di-\\nmension of Vis ultra-large. Based on this consideration, we\\napply the diffusion process to the latent representations by\\ndefault. For Z= [z0\\n1,···, z0\\nk,···, z0\\nK]extracted from pa-\\nrameter autoencoder, we use the optimization of DDPM (Ho\\net al., 2020) as follows,\\nθ←θ− ∇ θ||ϵ−ϵθ(√αtz0\\nk+√\\n1−αtϵ, t)||2,(6)\\nwhere tis uniform between 1andT, the sequence of hy-\\nperparameters αtindicates the noise strength at each step,\\nϵis the added Gaussian noise, ϵθ(·)denotes the denoising\\nnetwork that parameterized by θ. After finishing the training\\nof the parameter generation, we directly fed random noise\\n3', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 2}),\n",
       " Document(page_content='Neural Network Diffusion\\nTable 1. We present results in the format of ‘original / ensemble / p-diff’. Our method obtains similar or even higher performance than\\nbaselines. The results of p-diff is average in three runs. Bold entries are best results.\\nNetwork \\\\Dataset MNIST CIFAR-10 CIFAR-100 STL-10 Flowers Pets F-101 ImageNet-1K\\nResNet-18 99.2 / 99.2 / 99.3 92.5 / 92.5 / 92.7 76.7 / 76.7 / 76.9 75.5 / 75.5 / 75.4 49.1 / 49.1 / 49.7 60.9 / 60.8 / 61.1 71.2 / 71.3 / 71.3 78.7 / 78.5 / 78.7\\nResNet-50 99.4 / 99.3 / 99.4 91.3 / 91.4 / 91.3 71.6 / 71.6 / 71.7 69.2 / 69.1 / 69.2 33.7 / 33.9 / 38.1 58.0 / 58.0 / 58.0 68.6 / 68.5 / 68.6 79.2 / 79.2 / 79.3\\nViT-Tiny 99.5 / 99.5 / 99.5 96.8 / 96.8 / 96.8 86.7 / 86.8 / 86.7 97.3 / 97.3 / 97.3 87.5 / 87.5 / 87.5 89.3 / 89.3 / 89.3 78.5 / 78.4 / 78.5 73.7 / 73.7 / 74.1\\nViT-Base 99.5 / 99.4 / 99.5 98.7 / 98.7 / 98.7 91.5 / 91.4 / 91.7 99.1 / 99.0 / 99.2 98.3 / 98.3 / 98.3 91.6 / 91.5 / 91.7 83.4 / 83.4 / 83.4 84.5 / 84.5 / 84.7\\nConvNeXt-T 99.3 / 99.4 / 99.3 97.6 / 97.6 / 97.7 87.0 / 87.0 / 87.1 98.2 / 98.0 / 98.2 70.0 / 70.0 / 70.5 92.9 / 92.8 / 93.0 76.1 / 76.1 / 76.2 82.1 / 82.1 / 82.3\\nConvNeXt-B 99.3 / 99.3 / 99.4 98.1 / 98.1 / 98.1 88.3 / 88.4 / 88.4 98.8 / 98.8 / 98.9 88.4 / 88.4 / 88.5 94.1 / 94.0 / 94.1 81.4 / 81.4 / 81.6 83.8 / 83.7 / 83.9\\ninto the reverse process and the trained decoder to generate\\na new set of high-performing parameters. These generated\\nparameters are concatenated with the remained model pa-\\nrameters to form new models for evaluation. Neural network\\nparameters and image pixels exhibit significant disparities in\\nseveral key aspects, including data type, dimensions, range,\\nand physical interpretation. Different from images, neural\\nnetwork parameters mostly have no spatial relevance, so\\nwe replace 2D convolutions with 1D convolutions in our\\nparameter autoencoder and parameter generation processes.\\n3. Experiments\\nIn this section, We first introduce the setup for reproducing.\\nThen, we report the result comparisons and ablation studies.\\n3.1. Setup\\nDatasets and architectures. We evaluate our approach\\nacross a wide range of datasets, including MNIST (Le-\\nCun et al., 1998), CIFAR-10/100 (Krizhevsky et al., 2009),\\nImageNet-1K (Deng et al., 2009), STL-10 (Coates et al.,\\n2011), Flowers (Nilsback & Zisserman, 2008), Pets (Parkhi\\net al., 2012), and F-101 (Bossard et al., 2014) to study the ef-\\nfectiveness of our method. We mainly conduct experiments\\non ResNet-18/50 (He et al., 2016), ViT-Tiny/Base (Dosovit-\\nskiy et al., 2020), and ConvNeXt-T/B (Liu et al., 2022).\\nTraining details. The autoencoder and latent diffusion\\nmodel both include a 4-layer 1D CNNs-based encoder and\\ndecoder. We default to collecting 200 training data for all\\narchitectures. For ResNet-18/50, we train the models from\\nscratch. In the last epoch, we continue to train the last two\\nnormalization layers and fix the other parameters. We save\\n200 checkpoints in the last epoch, i.e.,original models. For\\nViT-Tiny/Base and ConvNeXt-T/B, we fine-tune the last two\\nnormalization parameters of the released model in the timm\\nlibrary (Wightman, 2019). The ξVandξZare Gaussian\\nnoise with amplitude of 0.001 and 0.1. In most cases, the\\nautoencoder and latent diffusion training can be completed\\nwithin 1 to 3 hours on a single Nvidia A100 40G GPU.\\nInference details. We synthesize 100 novel parameters by\\nfeeding random noise into the latent diffusion model and\\nthe trained decoder. These synthesized parameters are thenconcatenated with the aforementioned fixed parameters to\\nform our generated models. From these generated models,\\nwe select the one with the best performance on the training\\nset. Subsequently, we evaluate its accuracy on the validation\\nset and report the results. That is a consideration of mak-\\ning fair comparisons with the models trained using SGD\\noptimization. We empirically find the performance on the\\ntraining set is good for selecting models for testing.\\nBaselines. 1) The best validation accuracy among the orig-\\ninal models is denoted as ‘original’. 2) Average weight\\nensemble (Krogh & Vedelsby, 1994; Wortsman et al., 2022)\\nof original models is denoted as ‘ensemble’.\\n3.2. Results\\nTab. 1 shows the result comparisons with two baselines\\nacross 8 datasets and 6 architectures. Based on the re-\\nsults, we have several observations as follows: i) In most\\ncases, our method achieves similar or better results than\\ntwo baselines. This demonstrates that our method can ef-\\nficiently learn the distribution of high-performing parame-\\nters and generate superior models from random noise. ii)\\nOur method consistently performs well on various datasets,\\nwhich indicates the good generality of our method.\\n3.3. Ablation studies and analysis\\nExtensive ablation studies are conducted in this section\\nto illustrate the characteristics of our method. We default\\nto training ResNet-18 on CIFAR-100 and report the best,\\naverage, and medium accuracy (if not otherwise stated).\\nThe number of training models. Tab. 2(a) varies the size\\nof training data, i.e.the number of original models. We find\\nthe performance gap of best results among different numbers\\nof the original models is minor. To comprehensively explore\\nthe influences of different numbers of training data on the\\nperformance stability, we also report the average (avg.) and\\nmedian (med.) accuracy as metrics of stability of our gener-\\nated models. Notably, the stability of models generated with\\na small number of training instances is much worse than\\nthat observed in larger settings. This can be explained by\\nthe learning principle of the diffusion model: the diffusion\\nprocess may be hard to model the target distribution well if\\n4', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 3}),\n",
       " Document(page_content='Neural Network Diffusion\\nTable 2. p-diff main ablation experiments. We ablate the number of original models K, the location of applying our approach, and the\\neffect of noise augmentation. The default settings are K= 200 , applying p-diff on the deep BN parameters (between layer16 to 18), and\\nusing noise augmentation in the input parameters and latent representations. Defaults are marked in gray .Bold entries are best results.\\n(a) Large Kcan improve the\\nperformance stability of our\\nmethod.\\nK best avg. med.\\n1 76.6 70.7 73.2\\n10 76.5 71.2 73.8\\n50 76.7 71.3 74.3\\n200 76.9 72.4 75.6\\n500 76.8 72.3 75.4(b) P-diff works well on deep layers. The\\nindex of layer is aligned with the standard\\nResNet-18.\\nparameters best avg. med.\\noriginal models 76.7 76.6 76.6\\nBN-layer10 to 14 76.8 71.9 75.3\\nBN-layer14 to 16 76.9 72.2 75.5\\nBN-layer16 to 18 76.9 72.4 75.6(c) Noise augmentation makes p-diff stronger.\\nAdding noise on latent representations is more im-\\nportant than on parameters.\\nnoise augmentation best avg. med.\\noriginal models 76.7 - -\\nno noise 76.7 65.8 65.0\\n+ para. noise 76.7 66.7 67.3\\n+ latent noise 76.7 72.1 75.3\\n+ para. and latent noise 76.9 72.4 75.6\\nTable 3. We present result comparisons of original, ensemble, and p-diff under synthesizing entire model parameters setting. Our method\\ndemonstrates good generalization on ConvNet-3 and MLP-3. Bold entries are best results.\\n(a) Result comparisons on ConvNet-3 (includes three convolu-\\ntional layers and one linear layer.\\nDataset \\\\Network ConvNet-3\\noriginal ensemble p-diff parameter number\\nCIFAR-10 77.2 77.3 77.5 24714\\nCIFAR-100 57.2 57.2 57.3 70884(b) Result comparisons on MLP-3 (includes three linear layers\\nand ReLU activation function).\\nDataset \\\\Network MLP-3\\noriginal ensemble p-diff parameter number\\nMNIST 85.3 85.2 85.4 39760\\nCIFAR-10 48.1 48.1 48.2 155135\\nonly a few input samples are used for training.\\nWhere to apply p-diff. We default to synthesizing the pa-\\nrameters of the last two normalization layers. To investigate\\nthe effectiveness of p-diff on other depths of normalization\\nlayers, we also explore the performance of synthesizing the\\nother shallow-layer parameters. To keep an equal number\\nof BN parameters, we implement our approach to three\\nsets of BN layers, which are between layers with different\\ndepths. As shown in Tab. 2(b), we empirically find that our\\napproach achieves better performances (best accuracy) than\\nthe original models on all depths of BN layers settings. An-\\nother finding is that synthesizing the deep layers can achieve\\nbetter accuracy than generating the shallow ones. This is\\nbecause generating shallow-layer parameters is more likely\\nto accumulate errors during the forward propagation than\\ngenerating deep-layer parameters.\\nNoise augmentation. Noise augmentation is designed to\\nenhance the robustness and generalization of training the au-\\ntoencoder. We ablate the effectiveness of applying this aug-\\nmentation in the input parameters and latent representations,\\nrespectively. The ablation results are presented in Tab. 2(c).\\nSeveral observations can be summarized as follows: i) Noise\\naugmentation plays a crucial role in generating stable and\\nhigh-performing models. ii) The performance gains of ap-\\nplying noise augmentation in the latent representations are\\nlarger than in the input parameters. iii) Our default set-\\nting, jointly using noise augmentation in parameters and\\nrepresentations obtains the best performances (includes best,average, and medium accuracy).\\nGeneralization on entire model parameters. Until now,\\nwe have evaluated the effectiveness of our approach in syn-\\nthesizing a subset of model parameters, i.e.,batch normal-\\nization parameters. What about synthesizing entire model\\nparameters? To evaluate this, we extend our approach to\\ntwo small architectures, namely MLP-3 (includes three lin-\\near layers and ReLU activation function) and ConvNet-3 (in-\\ncludes three convolutional layers and one linear layer). Dif-\\nferent from the aforementioned training data collection strat-\\negy, we individually train these architectures from scratch\\nwith 200 different random seeds. We take CIFAR-10 as\\nan example and show the details of these two architectures\\n(convolutional layer: kernel size ×kernel size, the number\\nof channels; linear layer: input dimension, output dimen-\\nsion) as follows:\\n•ConvNet-3: conv1. 3 ×3, 32, conv2. 3 ×3, 32, conv3. 3 ×3,\\n32, linear layer. 2048, 10.\\n•MLP-3: linear layer1. 3072, 50, linear layer2. 50, 25,\\nlinear layer3. 25, 10.\\nWe present result comparisons between our approach and\\ntwo baselines ( i.e.,original and ensemble) at Tab. 3. We\\nreport the comparisons and parameter numbers of ConvNet-\\n3 on CIFAR-10/100 and MLP-3 on CIFAR-10 and MNIST\\ndatasets. These experiments demonstrate the effectiveness\\nand generalization of our approach in synthesizing entire\\nmodel parameters, i.e.,achieving similar or even improved\\n5', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 4}),\n",
       " Document(page_content='Neural Network Diffusion\\nperformances over baselines. These results suggest the\\npractical applicability of our method. However, we can\\nnot synthesize the entire parameters of large architectures,\\nsuch as ResNet, ViT, and ConvNeXt series. It is mainly\\nconstrained by the limitation of the GPU memory.\\nParameter patterns of original models. Experimental re-\\nsults and ablation studies demonstrate the effectiveness of\\nour method in generating neural network parameters. To\\nexplore the intrinsic reason behind this, we use 3 random\\nseeds to train ResNet-18 model from scratch and visualize\\nthe parameters in Fig. 3. We visualize the heat map of pa-\\nrameter distribution via min-max normalization in different\\nlayers individually. Based on the visualizations of the param-\\neters of convolutional (Conv.-layer2) and fully connected\\n(FC-layer18) layers, there indeed exist specific parameter\\npatterns among these layers. Based on the learning of these\\npatterns, our approach can generate high-performing neural\\nnetwork parameters.\\n4. Is P-diff Only Memorizing?\\nIn this section, we mainly investigate the difference between\\noriginal and generated models. We first propose a similarity\\nmetric. Then several comparisons and visualizations are\\nconducted to illustrate the characteristics of our approach.\\nQuestions and experiment designs. Here, we first ask\\nthe following questions: 1) Does p-diff just memorize the\\nsamples from the original models in the training set? 2) Is\\nthere any difference among adding noise or fine-tuning the\\noriginal models, and the models generated by our approach?\\nIn our paper, we hope that our p-diff can generate some new\\nparameters that perform differently than the original models.\\nTo verify this, we design experiments to study the differ-\\nences between original, noise-added, fine-tuned, and p-diff\\nmodels by comparing their predictions and visualizations.\\nSimilarity metric. We conduct experiments on CIFAR-\\n100 (Krizhevsky et al., 2009) with ResNet-18 (He et al.,\\n2016) under the default setting, i.e.only generating the\\nparameters of the last two batch normalization layers. We\\nmeasure the similarity between the two models by calcu-\\nlating the Intersection over Union (IoU) on their wrong\\npredictions. The IoU can be formulated as follows,\\nIoU = |Pwrong\\n1∩Pwrong\\n2|/|Pwrong\\n1∪Pwrong\\n2|,(7)\\nwhere Pwrong\\n· denotes the indexes of wrong predictions on\\nthe validation set, ∩and∪represent union and intersection\\noperations. A higher IoU indicates a greater similarity be-\\ntween the predictions of the two models. From now on, we\\nuse IoU as the similarity metric in our paper. To mitigate\\nthe influence of the performance contrasts in experiments,\\nwe select models that perform better than 76.5% by default.\\nConv.-layer2FC-layer18Seed 1Seed 2Seed 3\\nMinMaxFigure 3. Visualizing the parameter distributions of convolutional\\n(Conv.-layer2) and fully connected (FC-layer18) layers. Parame-\\nters from different layers show variant patterns while these param-\\neters from the same layer show similar patterns. The index of layer\\nis aligned with the standard ResNet-18.\\nSimilarity of predictions. We evaluate the similarity be-\\ntween the original and p-diff models. For each model, we\\nobtain its similarity by averaging the IoUs with other models.\\nWe introduce four comparisons: 1) similarity among origi-\\nnal models; 2) similarity among p-diff models; 3) similarity\\nbetween original and p-diff models; and 4) max similarity\\n(nearest neighbor) between original and p-diff models. We\\ncalculate the IoUs for all models in the above four compar-\\nisons and report their averaged values in Fig. 4(a).\\nOne can find that the differences among generated models\\nare much larger than the differences among the original\\nmodels. Another finding is that even the maximum similar-\\nity between the original and generated models is also lower\\nthan the similarity among the original models. It shows our\\np-diff can generate new parameters that perform differently\\nwith their training data ( i.e.original models).\\nWe also compare our approach with the fine-tuned and noise-\\nadded models. Specifically, we randomly choose one gen-\\nerated model, and search its nearest neighbor ( i.e. max\\nsimilarity) from the original models. Then, we fine-tune\\nand add random noise from the nearest neighbor to obtain\\ncorresponding models. After that, we calculate the similar-\\nity of the original with fine-tuned and noise-added models,\\nrespectively. Finally, we repeat this operation fifty times\\nand report their average IoUs for analysis. In this experi-\\nment, we also constraint the performances of all models,\\ni.e.,only good models are used here for reducing the bias of\\nvisualization. We empirically set the amplitude of random\\nnoise with the range from 0.01 to 0.1 to prevent substantial\\nperformance drops.\\nBased on the results in Fig. 4(b), we find that the perfor-\\nmances of fine-tuned and noise-added models are hard to\\n6', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 5}),\n",
       " Document(page_content='Neural Network Diffusion\\n/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001c/uni00000016/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000019/uni00000013/uni00000011/uni0000001c/uni0000001a/uni00000036/uni0000004c/uni00000050/uni0000004c/uni0000004f/uni00000044/uni00000055/uni0000004c/uni00000057/uni0000005c/uni00000024/uni00000050/uni00000052/uni00000051/uni0000004a/uni00000003/uni00000052/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f\\n/uni00000024/uni00000050/uni00000052/uni00000051/uni0000004a/uni00000003/uni00000053/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049\\n/uni00000025/uni00000048/uni00000057/uni0000005a/uni00000048/uni00000048/uni00000051/uni00000003/uni00000052/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003/uni00000053/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049\\n/uni00000025/uni00000048/uni00000057/uni0000005a/uni00000048/uni00000048/uni00000051/uni00000003/uni00000052/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003/uni00000053/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049/uni00000003/uni0000000b/uni00000050/uni00000044/uni0000005b/uni0000000c\\n(a) Similarity comparisons of original and p-\\ndiff models.\\n/uni00000013/uni00000011/uni0000001b/uni00000017 /uni00000013/uni00000011/uni0000001b/uni0000001c /uni00000013/uni00000011/uni0000001c/uni00000017 /uni00000013/uni00000011/uni0000001c/uni0000001c\\n/uni00000036/uni0000004c/uni00000050/uni0000004c/uni0000004f/uni00000044/uni00000055/uni0000004c/uni00000057/uni0000005c/uni0000001a/uni00000019/uni00000011/uni00000017/uni0000001a/uni00000019/uni00000011/uni00000018/uni0000001a/uni00000019/uni00000011/uni00000019/uni0000001a/uni00000019/uni00000011/uni0000001a/uni0000001a/uni00000019/uni00000011/uni0000001b/uni00000024/uni00000046/uni00000046/uni00000011\\n/uni00000032/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f/uni00000003/uni00000050/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000055/uni00000044/uni00000051/uni0000004a/uni00000048\\n/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni00000048/uni00000047\\n/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000010/uni00000044/uni00000047/uni00000047/uni00000048/uni00000047\\n/uni00000033/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049(b) Similarity comparisons of fine-tuned, noise-\\nadded, and p-diff models.\\n/uni00000032/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f\\n/uni00000024/uni00000047/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048\\n/uni00000033/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049(c) t-SNE of the latent representations\\nof original, p-diff, and adding noise.\\nFigure 4. The similarity represents the Intersection of Union (IoU) over wrong predictions between/among two models (a) shows the\\ncomparisons in four cases: similarity among original models and p-diff models, similarity between original and p-diff models, and the\\nmaximum similarity (nearest neighbor) between original and p-diff models. (b) displays the accuracy and max similarity of fine-tuned,\\nnoise-added, and p-diff models. All the maximum similarities are calculated with the original models. (c) presents the t-SNE (Van der\\nMaaten et al., 2008) of latent representations of the original models, p-diff models, and adding noise operation.\\noutperform the original models. Besides, the similarities\\nbetween fine-tuned or noise-added and original models are\\nvery high, which indicates these two operations can not\\nobtain novel but high-performing models. However, our\\ngenerated models achieve diverse similarities and superior\\nperformances compared to the original models.\\nComparison of latent representations. In addition to pre-\\ndictions, we assess the distributions of latent representations\\nfor the original and generated models using t-SNE (Van der\\nMaaten et al., 2008). To identify the differences between\\nour approach and the operation of adding noise to the la-\\ntent representations of original models, we also include the\\nadding noise operation as a comparison in Fig. 4(c). The\\nadded noise is random Gaussian noise with an amplitude\\nof 0.1. One can find that p-diff can generate novel latent\\nrepresentations while adding noise just makes interpolation\\naround the latent representations of original models.\\nThe trajectories of p-diff process. We plot the generated\\nparameters of different time steps in the inference stage to\\nform trajectories to explore its generation process. Five\\ntrajectories (initialized by 5 random noise) are shown in\\nFig. 5(a). We also plot the average parameters of the origi-\\nnal models and their standard deviation (std). As the time\\nstep increases, the generated parameters are overall close\\nto the original models. Although we keep a narrow per-\\nformance range constraint for visualization, there is still a\\ncertain distance between the end points (orange triangles)\\nof trajectories and average parameters (five-pointed star).\\nAnother finding is that the five trajectories are diverse.\\nFrom memorizing to generate new parameters. To in-vestigate the impact of the number of original models ( K)\\non the diversity of generated models, we visualize the max\\nsimilarities between original and generated models with\\ndifferent Kin Fig. 5(b). Specifically, we continually gener-\\nate parameters until 50 models perform better than 76.5%\\nin all cases. The generated models almost memorize the\\noriginal model when K= 1, as indicated by the narrow sim-\\nilarity range and high value. The similarity range of these\\ngenerated models becomes larger as Kincreases, demon-\\nstrating our approach can generate parameters that perform\\ndifferently from the original models.\\n5. Related Work\\nDiffusion models. Diffusion models have achieved re-\\nmarkable results in visual generation. These methods (Ho\\net al., 2020; Dhariwal & Nichol, 2021; Ho et al., 2022;\\nPeebles & Xie, 2022; Hertz et al., 2023; Li et al., 2023)\\nare based on non-equilibrium thermodynamics (Jarzynski,\\n1997; Sohl-Dickstein et al., 2015), and the its pathway is\\nsimilar to GAN (Zhu et al., 2017; Isola et al., 2017; Brock\\net al., 2018a), V AE (Kingma & Welling, 2013; Razavi et al.,\\n2019), and flow-based model (Dinh et al., 2014; Rezende\\n& Mohamed, 2015). Diffusion models can be categorized\\ninto three main branches. The first branch focuses on en-\\nhancing the synthesis quality of diffusion models, exem-\\nplified by models like DALL ·E 2 (Ramesh et al., 2022),\\nImagen (Saharia et al., 2022), and Stable Diffusion (Rom-\\nbach et al., 2022). The second branch aims to improve\\nthe sampling speed, including DDIM (Song et al., 2021),\\nAnalytic-DPM (Bao et al., 2022), and DPM-Solver (Lu\\net al., 2022). The final branch involves reevaluating diffu-\\n7', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 6}),\n",
       " Document(page_content='Neural Network Diffusion\\nStart point\\nEnd point\\nStd of original models\\nMean of original models\\n(a) Visualization of parameter trajectories of p-diff.\\n/uni00000014 /uni00000014/uni00000013 /uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013\\n/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000052/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f/uni00000003/uni00000050/uni00000052/uni00000047/uni00000048/uni0000004f/uni0000000f/uni00000003i.e.,/uni00000003/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044/uni00000003/uni00000052/uni00000049/uni00000003/uni00000053/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049/uni00000011/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000013/uni00000011/uni0000001c/uni00000019/uni00000013/uni00000011/uni0000001c/uni0000001b/uni00000036/uni0000004c/uni00000050/uni0000004c/uni0000004f/uni00000044/uni00000055/uni0000004c/uni00000057/uni0000005c\\n/uni0000002e/uni00000020/uni00000014/uni00000003/uni00000003/uni00000003/uni00000003/uni00000003/uni00000036/uni0000004c/uni00000050/uni00000011/uni00000003/uni0000001c/uni00000019/uni00000061/uni0000001c/uni0000001b/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000002e/uni00000020/uni00000014/uni00000013/uni00000003/uni00000003/uni00000003/uni00000036/uni0000004c/uni00000050/uni00000011/uni00000003/uni0000001c/uni00000014/uni00000061/uni0000001c/uni0000001a/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000002e/uni00000020/uni00000018/uni00000013/uni00000003/uni00000003/uni00000003/uni00000036/uni0000004c/uni00000050/uni00000011/uni00000003/uni0000001b/uni0000001a/uni00000061/uni0000001c/uni0000001a/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000002e/uni00000020/uni00000015/uni00000013/uni00000013/uni00000003/uni00000036/uni0000004c/uni00000050/uni00000011/uni00000003/uni0000001b/uni00000018/uni00000061/uni0000001c/uni0000001b/uni00000003/uni0000000b/uni00000008/uni0000000c\\n/uni0000002e/uni00000020/uni00000018/uni00000013/uni00000013/uni00000003/uni00000036/uni0000004c/uni00000050/uni00000011/uni00000003/uni0000001b/uni00000018/uni00000061/uni0000001c/uni0000001a/uni00000003/uni0000000b/uni00000008/uni0000000c (b) IoUs of high-performing (Acc. ≥76.5%) generated models.\\nFigure 5. (a) shows the parameter trajectories of our approach and original models distribution via t-SNE. (b) illustrates max IoUs between\\ngenerated and original models in different Ksettings. Sim. denotes similarity.\\nsion models from a continuous perspective, like score-based\\nmodels (Song & Ermon, 2019; Feng et al., 2023).\\nParameter generation. HyperNet (Ha et al., 2017) dy-\\nnamically generates the weights of a model with variable\\narchitecture. Smash (Brock et al., 2018b) introduces a flexi-\\nble scheme based on memory read-writes that can define a\\ndiverse range of architectures. (Peebles et al., 2023) collect\\n23 million checkpoints and train a conditional generator\\nvia a transformer-based diffusion model. MetaDiff (Zhang\\n& Yu, 2023) introduces a diffusion-based meta-learning\\nmethod for few-shot learning, where a layer is replaced by\\na diffusion U-Net (Ronneberger et al., 2015). HyperDiffu-\\nsion (Erko c ¸et al., 2023) directly utilizes a diffusion model\\non MLPs to generate new neural implicit fields. Different\\nfrom them, we analyze the intrinsic differences between\\nimages and parameters and design corresponding modules\\nto learn the distributions of the high-performing parameters.\\nStochastic and Bayesian neural networks. Our approach\\ncould be viewed as learning a prior over network parameters,\\nrepresented by the trained diffusion model. Learning param-\\neter priors for neural networks has been studied in classical\\nliterature. Stochastic neural networks (SNNs) (Sompolin-\\nsky et al., 1988; Bottou et al., 1991; Wong, 1991; Schmidt\\net al., 1992; Murata et al., 1994) also learn such priors\\nby introducing randomness to improve the robustness and\\ngeneralization of neural networks. The Bayesian neural net-\\nworks (Neal, 2012; Kingma & Welling, 2013; Rezende et al.,\\n2014; Kingma et al., 2015; Gal & Ghahramani, 2016) aims\\nto model a probability distribution over neural networks to\\nmitigate overfitting, learn from small datasets, and asses the\\nuncertainty of model predictions. (Graves, 2011) propose\\nan easily implementable stochastic variational method as\\na practical approximation to Bayesian inference for neuralnetworks. They introduce a heuristic pruner to reduce the\\nnumber of network weights, resulting in improved general-\\nization. (Welling & Teh, 2011) combine Langevin dynamics\\nwith SGD to incorporate a Gaussian prior into the gradient.\\nThis transforms SGD optimization into a sampling process.\\nBayes by Backprop (Blundell et al., 2015) learns a probabil-\\nity distribution prior over the weights of a neural network.\\nThese methods mostly operate in small-scale settings, while\\np-diff shows its effectiveness in real-world architectures.\\n6. Discussion and Conclusion\\nNeural networks have several popular learning paradigms,\\nsuch as supervised learning (Krizhevsky et al., 2012; Si-\\nmonyan & Zisserman, 2014; He et al., 2016; Dosovitskiy\\net al., 2020), self-supervised learning (Devlin et al., 2018;\\nBrown et al., 2020; He et al., 2020; 2022), and more. In this\\nstudy, we observe that diffusion models can be employed to\\ngenerate high-performing and novel neural network param-\\neters, demonstrating their superiority. Using diffusion steps\\nfor neural network parameter updates shows a potentially\\nnovel paradigm in deep learning.\\nHowever, we acknowledge that images/videos and parame-\\nters are signals of different natures, and this distinction must\\nbe handled with care. Additionally, even though diffusion\\nmodels have achieved considerable success in image/video\\ngeneration, their application to parameters remains relatively\\nunderexplored. These pose a series of challenges for neural\\nnetwork diffusion. We propose an initial approach to ad-\\ndress some of these challenges. Nevertheless, there are still\\nunresolved challenges, including memory constraints for\\ngenerating the entire parameters of large architectures, the\\nefficiency of structure designs, and performance stability.\\n8', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 7}),\n",
       " Document(page_content='Neural Network Diffusion\\nAcknowledgments. We thank Kaiming He, Dianbo Liu,\\nMingjia Shi, Zheng Zhu, Bo Zhao, Jiawei Liu, Yong Liu,\\nZiheng Qin, Zangwei Zheng, Yifan Zhang, Xiangyu Peng,\\nHongyan Chang, David Yin, Dave Zhenyu Chen, Ahmad\\nSajedi, and George Cazenavette for valuable discussions\\nand feedbacks.\\nReferences\\nBao, F., Li, C., Zhu, J., and Zhang, B. Analytic-DPM: an\\nanalytic estimate of the optimal reverse variance in diffu-\\nsion probabilistic models. In ICLR , 2022. URL https:\\n//openreview.net/forum?id=0xiJLKH-ufZ .\\nBlundell, C., Cornebise, J., Kavukcuoglu, K., and Wier-\\nstra, D. Weight uncertainty in neural network. In ICML .\\nPMLR, 2015.\\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101–\\nmining discriminative components with random forests.\\nInECCV . Springer, 2014.\\nBottou, L. et al. Stochastic gradient learning in neural\\nnetworks. Proceedings of Neuro-Nımes , 91(8), 1991.\\nBrock, A., Donahue, J., and Simonyan, K. Large scale gan\\ntraining for high fidelity natural image synthesis. arXiv\\npreprint arXiv:1809.11096 , 2018a.\\nBrock, A., Lim, T., Ritchie, J., and Weston, N. SMASH:\\nOne-shot model architecture search through hypernet-\\nworks. In ICLR , 2018b. URL https://openreview.\\nnet/forum?id=rydeCEhs- .\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nNeurIPS , 33, 2020.\\nCoates, A., Ng, A., and Lee, H. An analysis of single-layer\\nnetworks in unsupervised feature learning. In Proceed-\\nings of the fourteenth international conference on arti-\\nficial intelligence and statistics . JMLR Workshop and\\nConference Proceedings, 2011.\\nCristianini, N., Shawe-Taylor, J., et al. An introduction to\\nsupport vector machines and other kernel-based learning\\nmethods . Cambridge university press, 2000.\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\\nL. Imagenet: A large-scale hierarchical image database.\\nInCVPR . Ieee, 2009.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805 ,\\n2018.Dhariwal, P. and Nichol, A. Diffusion models beat gans on\\nimage synthesis. NeurIPS , 34, 2021.\\nDinh, L., Krueger, D., and Bengio, Y . Nice: Non-linear\\nindependent components estimation. arXiv preprint\\narXiv:1410.8516 , 2014.\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\\nHeigold, G., Gelly, S., et al. An image is worth 16x16\\nwords: Transformers for image recognition at scale. arXiv\\npreprint arXiv:2010.11929 , 2020.\\nErko c ¸, Z., Ma, F., Shan, Q., Nießner, M., and Dai, A. Hyper-\\ndiffusion: Generating implicit neural fields with weight-\\nspace diffusion. arXiv preprint arXiv:2303.17015 , 2023.\\nFeng, B. T., Smith, J., Rubinstein, M., Chang, H., Bouman,\\nK. L., and Freeman, W. T. Score-based diffusion models\\nas principled priors for inverse imaging. arXiv preprint\\narXiv:2304.11751 , 2023.\\nGal, Y . and Ghahramani, Z. Dropout as a bayesian approxi-\\nmation: Representing model uncertainty in deep learning.\\nInICML . PMLR, 2016.\\nGraves, A. Practical variational inference for neural net-\\nworks. NeurIPS , 24, 2011.\\nHa, D., Dai, A. M., and Le, Q. V . Hypernetworks. In ICLR ,\\n2017. URL https://openreview.net/forum?\\nid=rkpACe1lx .\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\\nlearning for image recognition. In CVPR , 2016.\\nHe, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo-\\nmentum contrast for unsupervised visual representation\\nlearning. In CVPR , 2020.\\nHe, K., Chen, X., Xie, S., Li, Y ., Doll ´ar, P., and Girshick,\\nR. Masked autoencoders are scalable vision learners. In\\nCVPR , 2022.\\nHertz, A., Mokady, R., Tenenbaum, J., Aberman, K.,\\nPritch, Y ., and Cohen-or, D. Prompt-to-prompt im-\\nage editing with cross-attention control. In ICLR ,\\n2023. URL https://openreview.net/forum?\\nid=_CDixzkzeyb .\\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\\nbilistic models. NeurIPS , 33, 2020.\\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,\\nA., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\\net al. Imagen video: High definition video generation\\nwith diffusion models. arXiv preprint arXiv:2210.02303 ,\\n2022.\\n9', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 8}),\n",
       " Document(page_content='Neural Network Diffusion\\nIsola, P., Zhu, J.-Y ., Zhou, T., and Efros, A. A. Image-to-\\nimage translation with conditional adversarial networks.\\nInCVPR , 2017.\\nJarzynski, C. Equilibrium free-energy differences from\\nnonequilibrium measurements: A master-equation ap-\\nproach. Physical Review E , 56(5), 1997.\\nKingma, D. P. and Welling, M. Auto-encoding variational\\nbayes. arXiv preprint arXiv:1312.6114 , 2013.\\nKingma, D. P., Salimans, T., and Welling, M. Variational\\ndropout and the local reparameterization trick. NeurIPS ,\\n28, 2015.\\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\\nof features from tiny images. 2009.\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\\nclassification with deep convolutional neural networks.\\nNeurIPS , 25, 2012.\\nKrogh, A. and Vedelsby, J. Neural network ensembles, cross\\nvalidation, and active learning. NeurIPS , 7, 1994.\\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-\\nbased learning applied to document recognition. Proceed-\\nings of the IEEE , 86(11), 1998.\\nLi, A. C., Prabhudesai, M., Duggal, S., Brown, E., and\\nPathak, D. Your diffusion model is secretly a zero-shot\\nclassifier. arXiv preprint arXiv:2303.16203 , 2023.\\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\\nmanan, D., Doll ´ar, P., and Zitnick, C. L. Microsoft coco:\\nCommon objects in context. In Computer Vision–ECCV\\n2014: 13th European Conference, Zurich, Switzerland,\\nSeptember 6-12, 2014, Proceedings, Part V 13 , pp. 740–\\n755. Springer, 2014.\\nLiu, Z., Mao, H., Wu, C.-Y ., Feichtenhofer, C., Darrell, T.,\\nand Xie, S. A convnet for the 2020s. In CVPR , 2022.\\nLong, J., Shelhamer, E., and Darrell, T. Fully convolutional\\nnetworks for semantic segmentation. In Proceedings\\nof the IEEE conference on computer vision and pattern\\nrecognition , pp. 3431–3440, 2015.\\nLu, C., Zhou, Y ., Bao, F., Chen, J., Li, C., and Zhu, J.\\nDPM-solver: A fast ODE solver for diffusion probabilis-\\ntic model sampling in around 10 steps. In Oh, A. H.,\\nAgarwal, A., Belgrave, D., and Cho, K. (eds.), NeurIPS ,\\n2022. URL https://openreview.net/forum?\\nid=2uAaGwlP_V .\\nMurata, N., Yoshizawa, S., and Amari, S.-i. Network infor-\\nmation criterion-determining the number of hidden units\\nfor an artificial neural network model. IEEE transactions\\non neural networks , 5(6), 1994.Neal, R. M. Bayesian learning for neural networks , volume\\n118. Springer Science & Business Media, 2012.\\nNichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,\\nP., McGrew, B., Sutskever, I., and Chen, M. Glide:\\nTowards photorealistic image generation and editing\\nwith text-guided diffusion models. arXiv preprint\\narXiv:2112.10741 , 2021.\\nNilsback, M.-E. and Zisserman, A. Automated flower clas-\\nsification over a large number of classes. In 2008 Sixth\\nIndian conference on computer vision, graphics & image\\nprocessing . IEEE, 2008.\\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C.\\nCats and dogs. In CVPR . IEEE, 2012.\\nPeebles, W. and Xie, S. Scalable diffusion models with\\ntransformers. arXiv preprint arXiv:2212.09748 , 2022.\\nPeebles, W., Radosavovic, I., Brooks, T., Efros, A. A., and\\nMalik, J. Learning to learn with generative models of\\nneural network checkpoints, 2023. URL https://\\nopenreview.net/forum?id=JXkz3zm8gJ .\\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.\\nHierarchical text-conditional image generation with clip\\nlatents. arXiv preprint arXiv:2204.06125 , 1(2), 2022.\\nRazavi, A., Van den Oord, A., and Vinyals, O. Generating\\ndiverse high-fidelity images with vq-vae-2. NeurIPS , 32,\\n2019.\\nRen, S., He, K., Girshick, R., and Sun, J. Faster r-cnn:\\nTowards real-time object detection with region proposal\\nnetworks. Advances in neural information processing\\nsystems , 28, 2015.\\nRezende, D. and Mohamed, S. Variational inference with\\nnormalizing flows. In ICML . PMLR, 2015.\\nRezende, D. J., Mohamed, S., and Wierstra, D. Stochas-\\ntic backpropagation and approximate inference in deep\\ngenerative models. In ICML . PMLR, 2014.\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\\nOmmer, B. High-resolution image synthesis with latent\\ndiffusion models. In CVPR , 2022.\\nRonneberger, O., Fischer, P., and Brox, T. U-net: Con-\\nvolutional networks for biomedical image segmentation.\\nInMedical Image Computing and Computer-Assisted\\nIntervention–MICCAI 2015: 18th International Confer-\\nence, Munich, Germany, October 5-9, 2015, Proceedings,\\nPart III 18 , pp. 234–241. Springer, 2015.\\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\\n10', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 9}),\n",
       " Document(page_content='Neural Network Diffusion\\nB., Salimans, T., et al. Photorealistic text-to-image diffu-\\nsion models with deep language understanding. NeurIPS ,\\n35, 2022.\\nSchmidt, W. F., Kraaijveld, M. A., Duin, R. P., et al. Feed\\nforward neural networks with random weights. In ICPR .\\nIEEE Computer Society Press, 1992.\\nSimonyan, K. and Zisserman, A. Very deep convolu-\\ntional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556 , 2014.\\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\\nGanguli, S. Deep unsupervised learning using nonequi-\\nlibrium thermodynamics. In ICML . PMLR, 2015.\\nSompolinsky, H., Crisanti, A., and Sommers, H.-J. Chaos\\nin random neural networks. Physical review letters , 61\\n(3), 1988.\\nSong, J., Meng, C., and Ermon, S. Denoising diffu-\\nsion implicit models. In ICLR , 2021. URL https:\\n//openreview.net/forum?id=St1giarCHLP .\\nSong, Y . and Ermon, S. Generative modeling by estimating\\ngradients of the data distribution. NeurIPS , 32, 2019.\\nTian, Z., Shen, C., Chen, H., and He, T. Fcos: A simple and\\nstrong anchor-free object detector. IEEE T-PAMI , 44(4):\\n1922–1933, 2020.\\nVan der Maaten, L., Hinton, G., and Van der Maaten, L.\\nVisualizing data using t-sne. JMLR , 9(11), 2008.\\nWelling, M. and Teh, Y . W. Bayesian learning via stochastic\\ngradient langevin dynamics. In ICML , 2011.\\nWightman, R. Pytorch image models. https://github.\\ncom/rwightman/pytorch-image-models ,\\n2019.\\nWong, E. Stochastic neural networks. Algorithmica , 6(1-6),\\n1991.\\nWortsman, M., Ilharco, G., Gadre, S. Y ., Roelofs, R.,\\nGontijo-Lopes, R., Morcos, A. S., Namkoong, H.,\\nFarhadi, A., Carmon, Y ., Kornblith, S., et al. Model\\nsoups: averaging weights of multiple fine-tuned models\\nimproves accuracy without increasing inference time. In\\nICML , pp. 23965–23998. PMLR, 2022.\\nZhang, B. and Yu, D. Metadiff: Meta-learning with con-\\nditional diffusion for few-shot learning. arXiv preprint\\narXiv:2307.16424 , 2023.\\nZhu, J.-Y ., Park, T., Isola, P., and Efros, A. A. Unpaired\\nimage-to-image translation using cycle-consistent adver-\\nsarial networks. In ICCV , 2017.\\n11', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 10}),\n",
       " Document(page_content='Neural Network Diffusion\\nA. Experimental Settings\\nIn this section, we introduce detailed experiment settings, datasets, and instructions of code for reproducing.\\nA.1. Training recipe\\nWe provide our basic training recipe with specific details in Tab. 4. This recipe is based on the setting of ResNet-18 with\\nCIFAR-100 dataset. We introduce these details of general training hyperparameters, autoencoder, and latent diffusion model,\\nrespectively. It may be necessary to make adjustments to the learning rate and the training iterations for other datasets.\\nTraining Setting Configuration\\nK,i.e., the number of original models 200\\nbatch size 200\\nAutoencoder\\noptimizer AdamW\\nlearning rate 1e-3\\ntraining iterations 30, 000\\noptimizer momentum betas=(0.9, 0.999)\\nweight decay 2e-6\\nξV,i.e., noise added on the input parameters 0.001\\nξZ,i.e., noise added on the latent representations 0.1\\nDiffusion\\noptimizer AdamW\\nlearning rate 1e-3\\ntraining iterations 30, 000\\noptimizer momentum betas=(0.9, 0.999)\\nweight decay 2e-6\\nemaβ 0.9999\\nbetas start 1e-4\\nbetas end 2e-2\\nbetas schedule linear\\nT,i.e., maximum time steps in the training stage 1000\\nTable 4. Our basic training recipe based on CIFAR100 dataset and ResNet-18 backbone.\\nA.2. Datasets\\nWe evaluate the effectiveness of p-diff on 8 datasets. To be specific, CIFAR-10/100 (Krizhevsky et al., 2009). The CIFAR\\ndatasets comprise colored natural images of dimensions 32×32, categorized into 10 and 100 classes, respectively. Each\\ndataset consists of 50,000 images for training and 10,000 images for testing. ImageNet-1K (Deng et al., 2009) derived from\\nthe larger ImageNet-21K dataset, ImageNet-1K is a curated subset featuring 1,000 categories. It encompasses 1,281,167\\ntraining images and 50,000 validation images. STL-10 (Coates et al., 2011) comprises 96×96color images, spanning 10\\ndifferent object categories. It serves as a versatile resource for various computer vision tasks, including image classification\\nand object recognition. Flowers (Nilsback & Zisserman, 2008) is a dataset comprising 102 distinct flower categories,\\nwith each category representing a commonly occurring flower species found in the United Kingdom. Pets (Parkhi et al.,\\n2012) includes around 7000 images with 37 categories. The images have large variations in scale, pose, and lighting.\\nF-101 (Bossard et al., 2014) consists of 365K images that are crawled from Google, Bing, Yelp, and TripAdvisor using the\\nFood-101 taxonomy.\\nIn the appendix, we extend our p-diff in object detection, semantic segmentation, and image generation tasks. Therefore, we\\nalso introduce the extra-used datasets in the following. COCO (Lin et al., 2014) consists of over 200,000 images featuring\\ncomplex scenes with 80 object categories. It is widely used for object detection and segmentation tasks. We implement\\nimage generation task on CIFAR-10.\\n12', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 11}),\n",
       " Document(page_content='Neural Network Diffusion\\nTable 5. Comparison of using 1D CNNs and fully connected (FC) layers. 1D CNNs perform better than FC layers, especially in memory\\nand time.\\nArch. Method Dataset Time (s) ↓Best↑Average ↑Median ↑Worst↑Memory (MB) ↓\\nConvNet-3 FC MNIST 17 98.0 90.1 93.6 70.2 1375\\nConvNet-3 1D CNNs MNIST 16 99.2 92.1 94.2 73.6 1244\\nA.3. Instructions for code\\nWe have submitted the source code as the supplementary materials in a zipped file named as ‘p-diff.zip’ for reproduction. A\\nREADME is also included for the instructions for running the code.\\nB. Explorations of Designs and Strategies\\nIn this section, we introduce the reasons for the designs and strategies of our approach.\\nB.1. Why 1D CNNs?\\nConsidering the great differences between visual data and neural network parameters, we default to using 1D CNNs in\\nparameter autoencoder and generation. The detailed designs of 1D CNNs can be found in the following. Each layer in 1D\\nCNNs includes two 1D convolutional layers with a normalization layer and an activation layer. More details of the 1D\\nCNNs can be found at core/module/modules in our code zip file.\\nHere naturally raises a question: are there alternatives to 1D CNNs? We can use pure fully connected (FC) layers as\\nan alternative. To answer this question, we compare the performance of FC layers and 1D CNNs. The experiments are\\nconducted on MNIST with ConvNet-3 as the backbone. Based on our experimental results in Tab. 5, 1D CNNs consistently\\noutperform FC in all architectures. Meanwhile, the memory occupancy of 1D CNNs is smaller than FC.\\nTable 6. Comparison of using batch normalization, group normalization, and instance normalization in our approach. We also report the\\nresults without normalization. ‘norm.’ denotes normalization. Default settings are marked in gray . Bold entries are best results.\\n(a) Results on CIFAR-10.\\nnorm. best avg. med.\\noriginal 94.3 - -\\nno norm. 94.0 82.8 80.1\\nBN 88.7 84.3 88.2\\nGN 94.3 89.8 93.9\\nIN 94.4 88.5 94.2(b) Results on MNIST.\\nnorm. best avg. med.\\noriginal 99.6 - -\\nno norm. 99.5 84.1 98.4\\nBN 99.3 86.7 99.1\\nGN 99.6 93.2 99.3\\nIN 99.6 92.7 99.4(c) Results on CIFAR-100.\\nnorm. best avg. med.\\noriginal 76.7 - -\\nno norm. 76.1 67.4 69.9\\nBN 75.9 70.7 73.3\\nGN 76.8 72.1 75.8\\nIN 76.9 72.4 75.6\\nB.2. Is variational autoencoder an alternative to our approach?\\nVariational autoencoder (V AE) (Cristianini et al., 2000) can be regarded as a probabilistic generative model and achieve\\nmany remarkable results in the generation area. We also implement V AE to generate neural network parameters. We first\\nintroduce the details of V AE in our experiment. We implement vanilla V AE using the same backbone of the autoencoder in\\np-diff for a fair comparison. We evaluate the V AE generator in the case of different Kand compare its best, average, and\\nmedium performances with p-diff generated models. Based on the results in Tab. 7, our approach outperforms V AE by a\\nlarge margin in all cases. Another interesting finding is that the average performance of V AE generated models goes down\\nas the number of original models increases.\\nB.3. Which normalization strategy is suitable?\\nConsidering the intrinsic difference between images and neural network parameters, we explore the influence of different\\nnormalization strategies. We ablate batch normalization (BN), group normalization (GN), and instance normalization (IN)\\non CIFAR-10, MNIST, and CIFAR-100, respectively. We also implement our method without normalization for additional\\n13', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 12}),\n",
       " Document(page_content='Neural Network Diffusion\\nTable 7. Comparisons between V AE and our proposed p-diff. V AE performs worse than our approach, especially on the metric of average\\nand medium accuracy.\\n(a) Result of V AE\\nnum. of original models best avg. med.\\n1 75.6 61.2 70.4\\n10 76.5 65.8 71.5\\n50 76.5 63.0 71.8\\n200 76.7 62.7 70.8\\n500 76.7 62.6 71.9(b) P-diff vs V AE, improvements are reported in ().\\nnum. of original models best avg. med.\\n1 76.6 (+1.0) 70.7 (+9.5) 73.2 (+2.8)\\n10 76.5 (+0.0) 71.2 (+5.4) 73.8 (+2.3)\\n50 76.7 (+0.2) 71.3 (+8.3) 74.3 (+2.5)\\n200 76.9 (+0.2) 72.4 (+9.7) 75.6 (+4.8)\\n500 76.8 (+0.1) 72.3 (+9.7) 75.4 (+3.5)\\ncomparison. Their best, average, and medium performances of 100 generated models are reported in Tab. 6. Based on the\\nresults, we have the following observations: 1) BN obtains the worst overall performance on all three metrics. Since BN\\noperates in the batch dimension and introduces undesired correlations among model parameters 2) GN and IN perform\\nbetter than without normalization, i.e.‘no norm.’ in the Tab. 6. That could be explained by some outlier parameters affecting\\nthe performance a lot. 3) From the metrics, we find our method has good generalization among channel-wise normalization\\noperations, such as GN and IN.\\nTable 8. We design ablations about the intensity of input noise ξVand latent noise ξZ, generating variant types of parameters. ‘para.’\\ndenotes parameter. Default settings are marked in gray . Bold entries are best results.\\n(a) Ablation of input noise ξV.\\npara. noise best avg. med.\\n1e-4 76.7 72.1 75.6\\n1e-3 76.9 72.4 75.6\\n1e-2 76.3 70.4 74.4\\n1e-1 76.8 71.4 75.1(b) Ablation of latent noise ξZ.\\nlatent noise best avg. med.\\n1e-3 76.7 67.3 73.2\\n1e-2 76.6 70.1 74.7\\n1e-1 76.9 72.6 75.6\\n1e-0 76.7 74.0 75.0(c) Ablation of types of parameters.\\npara. type original best avg. med.\\nlinear 76.6 76.6 47.3 71.1\\nconv 76.2 76.2 71.3 76.1\\nshortcut 75.9 76.0 73.6 75.7\\nbn 76.7 76.9 72.4 75.6\\nC. More Ablations\\nIn this section, we introduce more ablation studies of our method. Same as the main paper, if not otherwise stated, we\\ndefault to training ResNet-18 on CIFAR-100 and report the best, average, and medium accuracy.\\nC.1. The intensity of noise added into input parameters\\nIn the main paper, we ablate the effectiveness of the added noise into input parameters. Here, we study the impact of the\\nintensity of this noise. Specifically, we explore four levels of noise intensity and report their best, average, and medium\\nresults in Tab. 8(a). One can find that, our default intensity achieves the best overall performance. Both too-large and\\ntoo-small noise intensities fail to obtain good results. That can be explained by that the too-large noise may destroy the\\noriginal distribution of parameters while too-small noise can not provide enough effectiveness of augmentation.\\nC.2. The intensity of noise added into latent representations\\nSimilar to Sec. C.1, we also ablate the noise intensity added into latent representations. As shown in Tab. 8(b), the\\nperformance stability of generated models becomes better as the noise intensity increases. However, too-large noise also\\nbreaks the distribution of the original latent representations.\\nC.3. The generalization on other types of parameters\\nIn the main paper, we investigate the effectiveness of our approach in generating normalization parameters. We also evaluate\\nour approach on other types of parameters, such as linear, convolutional, and shortcut layers. Here, we show the details\\n14', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 13}),\n",
       " Document(page_content='Neural Network Diffusion\\nTable 9. Exploring the influence of maximum time steps in the training stage. We conduct experiments on CIFAR-10, MNIST, and\\nCIFAR-100 datasets, respectively. Bold entries are best results.\\n(a) Results on CIFAR-10.\\nmaximum step best avg. med.\\n10 94.4 82.0 93.8\\n100 94.3 94.3 94.3\\n1000 94.4 88.5 94.2\\n2000 94.3 85.8 94.2(b) Results on MNIST.\\nmaximum step best avg. med.\\n10 99.6 89.9 98.9\\n100 99.6 99.6 99.6\\n1000 99.6 92.7 99.4\\n2000 99.6 94.1 99.5(c) Results on CIFAR-100.\\nmaximum step best avg. med.\\n10 76.6 70.6 74.9\\n100 76.8 75.9 76.5\\n1000 76.9 72.4 75.6\\n2000 76.8 73.1 75.1\\nof the above three type layers as follows: 1) linear layer: the last linear layer of ResNet-18. 2) convolutional layer: first\\nconvolutional layer of ResNet-18. 3) shortcut layer: the shortcut layer between 7th and 8th layer of ResNet-18. The training\\ndata preparation is the same as we mentioned in the main paper. As illustrated in Tab. 8, we find our approach consistently\\nachieves similar or improved performance compared to the original models.\\nD. Open Explorations\\nD.1. Do we need to train 1000-step diffusion model?\\nWe default to training the latent diffusion model via random sampling from 1000 time steps. Can we reduce the number\\nof time steps in the training stage? To study the impact of the time steps, we conduct an ablation and report the results in\\nTab. 9. Several findings can be summarized as follows: 1) Too small time steps might not be strong enough to generate\\nhigh-performing models with good stability. 2) The best stability performances are obtained by setting the maximum time\\nsteps as 100. 3). Increasing the maximum time steps from 1000 to 2000 can not improve the performance. We will further\\nupgrade our design based on this exploration.\\nD.2. Potential applications\\nNeural network diffusion can be utilized or help the following potential research areas. 1) Parameters initialization : our\\napproach can generate high-performing initialized parameters. Therefore, that would speed up the optimization and reduce\\nthe overall cost of training. 2) Domain adaptation : our approach may have three benefits in the domain adaptation area.\\nFirst, we can directly use the diffusion process to learn the well-performed models trained by different domain data. Second,\\nsome hard adaptations can be achieved by our approach. Third, the adaptation efficiency might be improved largely.\\nE. Other Finding and Comparison Results\\nE.1. How to select generated parameters?\\nP-diff can rapidly generate numerous high-performance models. How do we evaluate these models? There are two primary\\nstrategies. The first one is to directly test them on the validation set and select the best-performing model. The second one is\\nto compute the loss of model outputs compared to the ground truth on the training set to choose a model. We generated\\nhundred model parameters with performance distributions in different intervals and displayed their accuracy curves on both\\nthe training and validation sets in Fig. 6(a). The experimental results indicate that p-diff exhibits a high level of consistency\\nbetween the training and validation sets. To provide a fair comparison with baseline methods, we default to choose the\\nmodel that performs the best results on the training set and compare it with the baseline.\\nE.2. Parameter visualization\\nTo provide a more intuitive understanding, we compare the parameters generated by our approach, SGD optimization\\n(original), and randomly initialized. Taking ResNet-18 as an example, we report the mean, std, accuracy (acc.), and IoU of\\nthe normalization layer parameters of training on CIFAR-100 in Fig. 6(b). There is a significant difference between the\\nparameters generated by our approach and the randomly initialized parameters, mean: 0.37 vs 0.36, std: 0.22 vs 0.21 The\\nIoU between ours and SGD is 0.87. This visualization and results confirm that the diffusion process can learn the patterns of\\n15', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 14}),\n",
       " Document(page_content='Neural Network Diffusion\\n0 20 40 60 80 100\\nindex of generated model9092949698100accuracy on CIFAR-10validation set\\ntraining set\\n(a) Accuracy distribution in p-diff models.\\nVisualization of R-18 BN parameters\\nmin.max\\nmean: 0.37 std: 0.22 acc: 76.6%mean: 0.36 std: 0.21 acc: 76.8%\\nSimilarity: 0.87\\nmean: 0.50 std: 0.50 acc: 1.3%initial.originalp-diff (b) Visualization of initial, SGD-trained, p-diff generated model.\\nFigure 6. P-diff can generate models with great consistency on both training and validation sets contrast compared to the original model.\\n(a) shows the accuracy distribution of training and validation sets in hundred p-diff models. (b) displays a heat map of initial, SGD-trained,\\np-diff generated parameters of the normalization layer in ResNet-18.\\n15x faster19s300s\\n(a) Acc. of R-18.\\n44x faster45s2000s (b) Acc. of ViT-Base.\\nFigure 7. We compare the accuracy curves of our method and SGD under three cases. (a): ResNet-18 on CIFAR-100. (b): ViT-Base on\\nImageNet-1K. Our approach speeds up at least 15 ×than standard SGD process.\\nhigh-performance parameters and generate new good models from random noise. More importantly, our generated model\\nhas a great behavior contrast compared to the original model, which is reflected in the low IoU value.\\nE.3. Efficiency of parameter generation\\nTo evaluate the generation efficiency of our method, we compare the validation accuracy curves of our method and SGD\\ntraining among the following cases: 1) parameter diffusion with ResNet-18 on CIFAR-100; 2) parameter diffusion with\\nViT-Base on ImageNet-1K. We utilize the random initialized parameters for our method and SGD to make a fair comparison.\\nAs illustrated in Fig. 7, our method can speed up at least 15 ×compared to the SGD without performance drops. On\\nImageNet-1K, we can speed up by 44 ×when compared to the vanilla SGD optimization, which illustrates the more\\nsignificant potential when applying our approach to large training datasets.\\nF. Generalization on Other Tasks\\nWe implement our method for other visual tasks, i.e.,object detection, semantic segmentation, image generation. Experi-\\nmental results illustrate the ability of our method to generalize to various tasks.\\nF.1. Object detection\\nFaster R-CNN (Ren et al., 2015) utilizes a region proposal network (RPN) which shares full-image convolutional features\\nwith the detection network to improve Fast R-CNN on object detection task. The FCOS (Fully Convolutional One-\\n16', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 15}),\n",
       " Document(page_content='Neural Network Diffusion\\nStage) (Tian et al., 2020) model is a single-stage object detection model that simplifies the detection process by eliminating\\nthe need for anchor boxes. In the object detection task, We implement Faster R-CNN (Ren et al., 2015) and FCOS (Tian\\net al., 2020) with ResNet-50 backbone on the COCO (Lin et al., 2014) dataset based on torch/torchvision. Considering the\\ntime cost of data for p-diff, we directly use the pre-trained parameters as our first training data, then fine-tune it to obtain\\nother training data. The parameters of the boxing predictor layer are generated by p-diff. We report the results in Tab. 10.\\nOur method can get models with similar or even better performance than the original model in seconds.\\nmodel/performance best original mAP best p-diff mAP\\nFaster R-CNN 36.9 37.0\\nFCOS 39.1 39.1\\nTable 10. P-diff in object detection task. We report the mAP of best original model and best p-diff generated model.\\nF.2. Semantic segmentation\\nFully Convolutional Network (FCN) (Long et al., 2015) was designed to efficiently process and analyze images at the pixel\\nlevel, allowing for the semantic segmentation of objects within an image. Following the approach in object detection, we\\nimplement semantic segmentation task using FCN (Long et al., 2015) with ResNet-50 backbone to evaluate a subset of\\nCOCO val2017, on the 20 categories that are present in the Pascal VOC dataset. We generate a subset of the parameters\\nof backbone and report the results in Tab. 11. Our approach can generate high-performing neural network parameters in\\nsemantic segmentation task.\\nmodel/performance original p-diff\\nmean IoU pixelwise acc. mean IoU pixelwise acc.\\nFCN 60.5 91.4 60.7 91.5\\nTable 11. P-diff in semantic segmentation task. We report mean IoU and pixelwise accuracy of best original model and best p-diff model.\\nF.3. Image generation\\nmodel/performance original FID p-diff FID\\nDDPM UNet 3.17 3.19\\nTable 12. P-diff in image generation task. We report\\nthe FID score on the CIFAR-10 dataset.DDPM (Ho et al., 2020) is a diffusion-based method in image generation,\\nwhere UNet (Ronneberger et al., 2015) is used to model the noise. In\\nthe image generation task, we use p-diff to generate a subset of model\\nparameters of UNet. For comparison, we evaluate the p-diff model’s FID\\nscore on the CIFAR-10 dataset and report the results in Tab. 12. The best\\np-diff generated UNet get similar performance to the original model.\\n17', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 16}),\n",
       " Document(page_content='Intent-based Prompt Calibration: Enhancing prompt\\noptimization with synthetic boundary cases\\nElad Levi Eli Brosh Matan Friedmann\\nAbstract\\nPrompt engineering is a challenging and important task due to the high sensitivity\\nof Large Language Models (LLMs) to the given prompt and the inherent ambi-\\nguity of a textual task instruction. Automatic prompt engineering is essential to\\nachieve optimized performance from LLMs. Recent studies have demonstrated the\\ncapabilities of LLMs to automatically conduct prompt engineering by employing\\na meta-prompt that incorporates the outcomes of the last trials and proposes an\\nimproved prompt. However, this requires a high-quality benchmark to compare\\ndifferent prompts, which is difficult and expensive to acquire in many real-world\\nuse cases. In this work, we introduce a new method for automatic prompt engi-\\nneering, using a calibration process that iteratively refines the prompt to the user\\nintent. During the optimization process, the system jointly generates synthetic\\ndata of boundary use cases and optimizes the prompt according to the generated\\ndataset. We demonstrate the effectiveness of our method with respect to strong\\nproprietary models on real-world tasks such as moderation and generation. Our\\nmethod outperforms state-of-the-art methods with a limited number of annotated\\nsamples. Furthermore, we validate the advantages of each one of the system’s key\\ncomponents. Our system is built in a modular way, facilitating easy adaptation to\\nother tasks. The code is available at https://github.com/Eladlev/AutoPrompt.\\n1 Introduction\\nIn recent years, there has been significant enhancements in the capabilities of Large Language\\nModels (LLMs), demonstrating impressive generative performance across a variety of tasks [ 11,4].\\nNevertheless, despite these advancements, the quality of the models’ outputs is highly sensitive to the\\nconditioned prompt [ 18,40]. Even a slight modification in the prompt format can significantly impact\\nthe model’s performance [ 27]. This issue is even more evident in popular proprietary models, where\\na change in model version results in drastic changes in model behaviour on a wide range of tasks [ 6].\\nIn order to tackle the prompt sensitivity issue, several methods [ 15,16] proposed to use soft prompts\\nwhich require access to the LLM itself in order to perform the optimization. Recently, [ 35,37,24]\\ndemonstrated the effectiveness of using LLMs themselves to optimize the prompt. To this end, each\\nprompt is assigned a score based on a given benchmark and an appropriate metric. The optimization\\nprocess is performed iteratively by providing a meta-prompt that incorporates the history of the last\\nfew prompt scores and guiding the model to suggest a better prompt with a higher score. However,\\nthe high-quality, large benchmarks required by this approach to evaluate the performance of the\\ndifferent prompts often do not exist in many real-world use cases. Moreover, iterating on such large\\ndatasets can be costly.\\nLLMs have proven to be highly effective in generating high-quality and rich datasets that boost model\\nperformance on a diverse set of tasks [ 25,36,19,33]. Recent works demonstrate the capabilities\\nof LLMs to refine the prompt provided by the user, resolving the initial prompt ambiguity [ 10].arXiv:2402.03099v1  [cs.CL]  5 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 0}),\n",
       " Document(page_content='Figure 1: System diagram. (1) An initial prompt is provided by the user (2) Synthetic challenging\\ncases are generated (3) A user or an LLM annotates the examples (4) After evaluating the prompt\\nperformances, an LLM suggest a new prompt given the last prompt’s results. (5) This process is\\nrepeated iteratively until a certain stop criterion (6) The system outputs a calibrated prompt.\\nHowever, without additional information, the model has to guess the true intention of the user, which\\nin many cases can lead to inaccurate results.\\nIn this work, we introduce Intent-based Prompt Calibration (IPC), a system which aims to calibrate\\nthe prompt according to the intention of the user, by using synthetic examples. The calibration\\nprocess is performed by iteratively building a dataset of challenging boundary cases and optimising\\nthe prompt according to the generated benchmark. This novel aspect of our method, producing a\\nsmall benchmark tailored to the boundary cases of the user’s task as part of the optimization process,\\nis highly valuable for explainability, LLM distillation, and other use cases. In contrast to previous\\nworks, the system is optimized for real-world use cases such as moderation which usually suffers\\nfrom imbalanced data distribution. We also extend the prompt optimization to a new family of\\ngenerative tasks, by first fitting a ranking prompt and then performing the prompt optimization with\\nthe learned ranker. Learning a prompt ranker allows us to optimize generative tasks with minimal\\nannotation effort. As demonstrated in our experimentation section, using such an approach without\\nsynthetic data of boundary cases, e.g., as done in previous methods, would not be efficient due to the\\nnatural imbalance of the ranking distribution.\\nLastly, our system is built in a modular way, such that each part of the components can be used\\non its own in other tasks like synthetic data generation or prompt distillation between two LLMs.\\nWe describe the system components in detail and demonstrate the effectiveness of our proposed\\nmethod with respect to strong proprietary models like GPT-3.5/4-Turbo. We show that our method\\noutperforms previous methods using a very small amount of data and iteration steps . This significantly\\nreduces the total optimization efforts and costs, and makes the system applicable to various production\\nuse cases.\\n2 Method\\nOur system is illustrated in Figure 1. We start with the initial prompt suggestion and a task description.\\nThe user can also provide a few examples in a few-shot setting. Then, during the calibration\\noptimization process, the system iteratively: 1. Suggests a few samples of challenging and diverse\\nboundary cases for the task and the current prompt. 2. Evaluates the current prompt on the generated\\ndataset, and provides an analysis. 3. Given the history of the last few prompts, suggests a new prompt\\nwith a higher score. The optimization process is terminated when either there is no improvement in\\nthe last few steps, or when the maximum number of iterations has been reached.\\nThe base configuration of our system is optimized for classification tasks, with accuracy set as the\\nscore function, and the error analysis determined by a confusion matrix and the prompt misclassifica-\\ntions. An example of the system flow can be seen in Figure 2. In each iteration, new challenging\\nsamples are generated (according to the current prompt), and the misclassifications are used to refine\\nthe prompt until it is calibrated to the user intent.\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 2: Example of a real system flow. The user provides only the task description and initial\\nprompt. The model iteratively generates challenging samples and refines the prompt according to the\\ngenerated benchmark.\\n2.1 Generative tasks\\nTo extend the prompt calibration process from classification tasks to generative tasks, we split the\\noptimization process into two parts. In the first part, we use an LLM to rephrase the initial prompt\\nand task description in order to define a ranking task based on the modified initial prompt and task\\ndescription. We then calibrate a prompt for the ranking task, treating it as a classification task using\\nthe classification pipeline. Naturally, the ranker distribution tends to be a normal distribution with\\nits mean at the mean score. This distribution is imbalanced, especially in the interesting range of\\nthe top scores. Therefore, in the ranking case, the sample generator meta-prompt is instructed to\\ngenerate challenging boundary samples from the top two scores. In the second part, we leverage the\\nsame underlying process to optimize the original generative prompt. This step is done by iteratively\\napplying steps 2 and 3, described in the system overview, using the calibrated ranking prompt as\\nthe score function. It’s important to note that human annotations are required only in the ranking\\ncalibration process. Furthermore, by treating the intent as a classification task, the prompt can be\\ncalibrated using a small amount of annotation effort.\\n2.2 Meta-Prompts\\nThe meta-prompts consist of three separate prompts, as can be seen in Appendix A.\\nSample generator. The sample generation meta-prompt is determined according to the system state:\\nIn the first iteration, if the user doesn’t provide any samples (zero-shot setting), the meta-prompt\\ninstructs the model to generate diverse adversarial samples with even class distribution. In the next\\n3', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 2}),\n",
       " Document(page_content='iterations, the prompt is extended with the following additional context: (1) A history with prompts\\nand good adversarial samples that confused the prompts; and (2) A set of realistic samples from the\\ndataset, where the model is instructed to preserve the dataset style. The context-realistic samples are\\nchosen to be semantically close according to a given sentence embedding.\\nAnalyzer. The analyzer meta-prompt receives the prompt score, a confusion matrix in the classifica-\\ntion case, and a set of errors in all the classes. It is then instructed to produce an analysis summary of\\nthe prompt performances and the major failure cases.\\nPrompt generator. The input for the prompt generator meta-prompt is (1) A list of the last suggested\\nprompts and their scores (2) The performance analysis of the last prompt that is produced by the\\nAnalyzer prompt. The model is instructed to produce a prompt with a higher score according to the\\nhistory and the analysis.\\n2.3 System pipeline\\nAn overview of the system architecture can be seen in Figure 8. The system consists of four primary\\ncomponents.\\nDataset. This component manages the dataset and performs operations such as insertion, modification,\\ndeletion, and applying functions, on the dataset rows. The component also handles data cleaning by\\nremoving semantic duplications and performing semantic sampling. Since the system is optimized\\nfor small datasets, the current implementation is based on a local database using pandas.\\nEstimator. The estimator is responsible for estimating a batch of samples. We implement this\\ncomponent twice, once for the predictions and once for the annotations. This generic implementation\\nfor both types of use cases, allows us to modify the system simply for diverse use cases such as\\nprompt calibration, prompt distillation and prompt squashing. The currently supported types of\\nestimators are: (1) Human annotation, using Argilla UI [ 32]. The system is connected to the Argilla\\nserver and is waiting until the annotation task is completed; (2) LLM estimator, which uses an\\nLLM to estimate the sample given a prompt. We support various types of LLMs, using Langchain\\nintegration [ 5]. For efficiency, the system supports parallelism using both workers and async calls.\\nThe system also supports sending a few samples in one prompt (prompt batching), which can reduce\\nthe cost significantly; and (3) Batch estimator, the batch estimator runs multiple LLM estimators\\nand integrates their outputs through an aggregation layer. It is mainly used for prompt-squashing,\\nenabling users to optimize a single prompt that will perform as well as running few prompts multiple\\ntimes. For example, when a user wants to apply several moderation rules simultaneously.\\nEvaluator. The evaluator is responsible for evaluating the records after the prediction and annotation\\nstage. The evaluator accepts a function and applies it to each row. It’s important to note that the\\nfunction is generic. For example, in the generation pipeline, the function is performed by invoking an\\nLLM. The evaluator is also responsible for defining the errors and handling the error analysis using\\nthe Analyzer described in the meta-prompts section.\\nOptimizer (Optimization Pipeline). The optimizer manager handles the whole optimization process\\nflow, it performs the iteration steps described in the previous section and is responsible for stopping\\nand returning the final calibrated prompt. The currently supported criteria are either convergence\\n(determined by a patience hyper-parameter), or usage limit (determined by maximal cost if relevant,\\nor by the number of generated tokens).\\n3 Experiments\\nWe test our system on scenarios that reflect real-world moderation and generation use cases on strong\\nproprietary models (GPT-3.5/4-Turbo). We used the IMDB review dataset [ 20] as the base data for all\\nour experiments. We compare our proposed IPC method to two SOTA prompt optimization methods\\nthat are based on meta-prompts: 1. OPRO [35] and 2. The meta prompt provided by PE [37].\\n3.1 Classification\\nWe evaluate the prompt calibration process on three binary classification tasks: (1) Spoiler detection,\\n(2) Sentiment analysis, and (3) Parental Guidance (PG) detection. In each experiment, we start with\\n4', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 3: Accuracy on the spoiler and the PG classification tasks, with respect to the number of\\ntraining steps. As shown, IPC outperforms other tested methods and results in lower variance.\\nsome initial samples and a prompt, in addition to a short task description. To generate the ground\\ntruth (GT) we composed a highly detailed prompt with specific preferences. For the GT generation,\\nwe used a strong model (GPT-4 Turbo), as this process of generating the GT simulates a user’s\\nparticular preference for the given task. The baseline methods were trained on samples from the\\nIMDB dataset [ 20], whereas our proposed method was trained on the adversarial synthetic data which\\nwas provided with 10 initial samples from the original IMDB training data. All methods were trained\\nfor 50 iterations. The test dataset was taken from the IMDB reviews test split, with the generated\\nannotations provided by the GT prompt. We then collected 250 samples for each class in each one of\\nthe tested scenarios, such that the final dataset has equal class distribution. It’s important to note that\\nthe IMDB dataset includes only highly polarizing reviews (no reviews with ratings in the range of\\n4-7). To evaluate the method’s performance on more challenging cases, we also generate a synthetic\\ntest dataset with 300 samples (using the initial prompt) for the sentiment classification task.\\nWe present our results in Figures 3,7. As seen in the figures, IPC outperforms all other tested methods.\\nIn particular, it’s important to note the high variance of the other methods, especially in the case of a\\nsmall number of training samples. The gap in performance between the methods becomes even more\\nevident in the synthetic data case, where there are more boundary cases, as can be seen in Figure 6.\\nA qualitative comparison between the methods can be seen in Table 1. While OPRO [ 35] results\\nin mainly rephrasing the initial prompt, and the PE [ 37] prompt only partly fits the GT prompt, the\\nIPC prompt successfully captures the subtle details and nuances of the GT prompt. The significant\\ndifferences in data distributions between the original data and the synthetic generated data can be\\nseen in Figure 5. Both the spoiler and the PG classification tasks exhibit significant bias towards the\\n’No’ labels, where the Synthetic data is almost balanced.\\n3.2 Generation\\nThe generation setting is composed of two parts: generating the ranker prompt and then using the\\nranker to optimize the generation task prompt. We tested our generation pipeline on challenging\\nambiguous tasks: (1) Generate a movie review that is enthusiastic, reliable and adheres to a given\\nmovie description. (2) Generate a movie review that is sarcastic but has a positive sentiment. As in\\nthe classification case, we chose highly detailed prompts to generate the ranker GT with a scale of 1\\nto 5, which simulates the human preferences for the given task. For each tested method we fit a ranker\\nusing 50 labeled samples, and then optimized the generative task prompt according to the learned\\nranking model. The reported evaluation score is calculated by running the learned generative prompt\\non a test set of size 50 and evaluating the result using the target ranking prompt GT. For the baseline\\n5', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 4}),\n",
       " Document(page_content='Figure 4: Histogram of the ranking scores of the\\nIMDB review dataset vs the generated synthetic\\ndataset on the Authentic and enthusiastic genera-\\ntion task. The real data distribution contains very\\nfew ranked 5 reviews, whereas the synthetic data\\ncontains a more balanced dataset with respect to\\nthe top scores.\\nFigure 5: Histogram of the ’Yes’ labels density\\non the parental guidance (PG) and spoiler classi-\\nfication tasks, with respect to the IMDB review\\ndataset and the generated synthetic dataset. The\\nreal data exhibits a heavy imbalance in favor\\nof the ’No’ label, while the synthetic data ap-\\nproaches an even distribution.\\nFigure 6: Accuracy of of sentiment classification task with respect to the synthetic dataset for different\\nnumber of training steps. IPC outperforms other tested methods and results in lower variance.\\nmethods, we took samples from the IMDB review dataset [ 20] and generated a movie description for\\neach review. We then fed this data to the ranker optimization process. We ran both the ranker training\\nand the generator training for 30 iteration steps.\\nResults for GPT-4 Turbo LLM, including both ranker training and generation prompt training, are\\npresented in Table 3. A qualitative comparison is provided in Table 4. We see that using IPC improves\\nthe average ranking score of the generated reviews compared to the other tested methods in all tested\\nscenarios. It’s important to note that all the tested methods, except for IPC, performed worse than the\\ninitial prompt in some experiments. This can be explained by the distribution of the ranking scores in\\nthe real data, which is shown in Figure 4, where there are almost no samples with the top score. In\\ncontrast, the distribution of the generated synthetic samples is biased towards the top two scores.\\n3.3 Ablation study\\nWe examine the impact of each key component of the system on the spoiler classification task.\\nSpecifically, we look at the 50 training samples case. The effect of each one of the components can\\nbe seen in Table 2. Using synthetic data boosts model performance. It is also important to note that\\nthe analyzer component substantially improves the model’s performance. This stands in contradiction\\nto [35]’s findings that adding errors to the meta-prompts doesn’t improve the model performance,\\nand can also emphasise the gap between the standard general benchmarks and use cases such as\\nmoderation.\\n4 Related Work\\nPrompt Optimization. Several methods have been suggested to address the challenge of automating\\nthe prompt engineering process. A commonly used approach is to optimize a task-specific embedding,\\nin either a continuous [ 15,16,17] or discrete [ 34,29] manner. This approach requires access to the\\nLLM itself in order to perform the optimization. An alternative approach is to use reinforcement\\n6', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 5}),\n",
       " Document(page_content='GT prompt To improve the IMDB review spoiler classifier, label a review as ’Yes’ for containing\\na spoiler if it: 1. Even subtly hints at the outcome of subplots or the development of\\nthe story, such as mentioning a character’s transformation or key relationships without\\nexplicit details. 2. Makes observations on the narrative structure, like pacing or the\\nsignificance of certain scenes, which could indirectly indicate important plot points.\\n3. Alludes to twists, endings, or character fates with veiled language that doesn’t\\nexplicitly outline the events but provides enough context to infer them. Conversely,\\nlabel a review as ’No’ for no spoilers if it: 1. Focuses on broad discussions of character\\ntraits, emotional tone, or film structure without revealing any specific plot points. 2.\\nOffers critiques or praise of film elements like pacing, genre, actor performances, and\\nfilmmaking techniques that are independent of plot developments. 3. Explores the\\nfilm’s themes or the emotional journey it offers in a way that avoids any direct or\\nindirect plot or character spoilers.\\nInitial prompt Does this movie review contain a spoiler? answer Yes or No\\nOPRO [35] Examine the movie synopsis with vigilance for spoiler content and succinctly affirm\\ntheir presence with \"Yes\" or negate with \"No\".\\nPE [37] Analyze the IMDB review for elements that provide significant insights into the plot or\\nreveal crucial turning points, even if mentioned subtly or indirectly. Look for elements\\nthat give away the resolution of central conflicts, character arcs, or the outcome of\\npivotal events. Additionally, distinguish between powerful scene descriptions and\\nactual plot revelations; the former should not be classified as spoilers unless they\\ndisclose essential plot information. Based on your analysis, classify the review as ’Yes’\\nfor containing spoilers that impact the viewing experience, or ’No’ for free of such\\nspoilers.\\nIPC IMDB Review Spoiler Identification Protocol: For the task of classifying IMDB\\nreviews for the presence of spoilers, the classifier must label reviews with a heightened\\nsensitivity to nuanced language and indirect spoiler cues. The classification labels are\\n’Yes’ for spoilers and ’No’ for non-spoilers. Apply the following criteria rigorously:\\nLabel ’Yes’ if a review: - Contains subtle references or nuanced language that hints\\nat plot developments or character arcs, without explicit detail. - Includes emotional\\nresponses or descriptive language that indirectly reveals plot outcomes or twists. -\\nEmploys suggestive language that points to future events or endings, even if it does\\nnot reveal specific information. Label ’No’ if a review: - Discusses technical aspects,\\nacting, direction, or personal viewer impressions in a manner that does not hint at or\\nreveal any plot details. - Comments on thematic elements, genre characteristics, or\\nstorytelling techniques without disclosing or implying crucial plot twists. - Expresses\\nopinions on the movie’s quality, performances, or execution that are free from any\\nstoryline implications or potential spoiler content. In both ’Yes’ and ’No’ classifications,\\nspecial attention must be given to the implicit meaning behind words and the context in\\nwhich they are used to prevent misclassification of subtle spoiler content and to ensure\\na genuine viewing experience for others.\\nTable 1: A comparison of the prompts with the best training scores from each method on the Spoiler\\ndetection task, using 50 training samples. IPC succeeded in capturing the nuances of the GT prompt.\\nlearning [ 9,39,1]. This approach either requires access to the generated tokens’ probability or\\nrequires a large dataset for training a model. Recent works used the LLMs themselves for prompt\\noptimization [ 41,24,35,37]. These methods can also be applied to proprietary LLMs, , where\\naccess is limited to the final generated sentences. However, these methods still require a good valid\\nbenchmark in order to evaluate and compare the different generated prompts, which is not always\\navailable in real-world cases.\\n7', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 6}),\n",
       " Document(page_content='Enthusiastic and reliable Sarcastic and positive\\nInitial 4.40±0.05 4.28±0.02\\nOPRO [35] 4.31±0.02 4.22±0.03\\nPE [37] 4.09±0.09 4.76±0.09\\nIPC 4.80±0.1 4.92±0.07\\nTable 2: Average ranking score (std) of the top 5 prompts on the two generation tasks.\\nSpoiler classification GPT-4 Turbo Spoiler classification GPT-3.5\\nIPC (default) 88.4±1.7 72.6±3.8\\n- Iterative data generation 87.3±1.8 67.0±4.1\\n- Synthetic data 83.3±2.4 62.6±4.1\\n- Analyzer 77.8±2.2 64.3±3.8\\nTable 3: Investigation of each component’s effect on the model accuracy. In each row, we removed\\nthe investigated component from the system and trained the system without it.\\nSynthetic data. The utilization of synthetic data produced by LLMs has demonstrated remarkable\\neffectiveness across a wide range of tasks, including code generation [ 25,36], mathematical rea-\\nsoning [ 38,19], text embedding [ 33] and text2image [ 3]. The advantage of using synthetic data is\\nnot only in cost savings; it can also be beneficial for low-resource tasks or imbalanced data distribu-\\ntions [ 21]. Following these works, our system generates high-quality evenly distributed synthetic\\nboundary samples, that result in a more efficient optimization process and higher-quality results.\\nSynthetic data was also proven to be an effective method to distil knowledge from black-box LLMs,\\nby training on synthetic data that was generated by those models [ 31,22,12]. However, in these\\nworks the generated data was used to fully train the student model. In contrast, our work demonstrates\\nthe effectiveness of synthetic data to distil knowledge between two black-box models via automatic\\nprompt engineering.\\nCurriculum Learning. Arranging the data samples for training machine learning models in a\\nmeaningful way, starting from easier samples and progressing to more challenging ones, can yield\\nperformance enhancements compared to the conventional method of training based on random data\\nshuffling. This approach is known as curriculum learning [ 30,2]. Curriculum Learning has been\\nproven to be effective in various fields such as object localization [ 13,28], object detection [ 7,26]\\nand NLP [ 14,23]. Inspired by these ideas, in [ 8] they propose to fine-tune LLMs by iteratively\\ngenerating synthetic data and refining the policy to distinguish between the synthetic data and the\\nhuman-annotated data. In our work, we use a similar approach, where the system iteratively generates\\nmore challenging cases that resolve the previous prompt ambiguity in order to more efficiently tune\\nto the user intent.\\n5 Conclusions\\nIn this work, we introduced IPC, a system for automatic prompt engineering. The system combines\\na synthetic data generation module that generates challenging and diverse samples, and a prompt\\noptimization module that suggests new prompts. Both of them are implemented by prompting LLMs,\\nand they iteratively refine each other until the prompt converges. We further propose a new method to\\nextend the meta-prompt based prompt optimization process to generative tasks. We demonstrate the\\neffectiveness of our system on real-world use cases such as moderation and generation with respect\\nto strong proprietary models (GPT-3.5/4-Turbo).Our method significantly enhances the resulting\\nperformance of prompts in all tested scenarios.\\nOur system is built in a modular and flexible way that allows for easy modification and addition of new\\ncomponents. In future work, we intend to extend our system to new use cases such as multi-modality\\nand in-context learning. We also intend to explore further possibilities to optimize the meta-prompts\\nthemselves.\\n8', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 7}),\n",
       " Document(page_content='References\\n[1]A. F. Akyürek, E. Akyürek, A. Kalyan, P. Clark, D. T. Wijaya, and N. Tandon. RL4F: generating\\nnatural language feedback with reinforcement learning for repairing model outputs. In A. Rogers,\\nJ. L. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,\\nCanada, July 9-14, 2023 , pages 7716–7733. Association for Computational Linguistics, 2023.\\n[2]Y . Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In Proceedings of\\nthe 26th Annual International Conference on Machine Learning , ICML ’09, page 41–48, New\\nYork, NY , USA, 2009. Association for Computing Machinery.\\n[3]J. Betker, G. Goh, L. Jing, TimBrooks, J. Wang, L. Li, LongOuyang, JuntangZhuang, JoyceLee,\\nYufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu, YunxinJiao, and A. Ramesh. Improv-\\ning image generation with better captions. 2023.\\n[4]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child,\\nA. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan-\\nguage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan,\\nand H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages\\n1877–1901. Curran Associates, Inc., 2020.\\n[5]H. Chase. Langchain, 2022. cff-version: 1.2.0, message: \"If you use this software, please cite it\\nas below.\", date-released: 2022-10-17.\\n[6]L. Chen, M. Zaharia, and J. Zou. How is chatgpt’s behavior changing over time? CoRR ,\\nabs/2307.09009, 2023.\\n[7]X. Chen and A. Gupta. Webly supervised learning of convolutional networks. In 2015 IEEE\\nInternational Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13,\\n2015 , pages 1431–1439. IEEE Computer Society, 2015.\\n[8]Z. Chen, Y . Deng, H. Yuan, K. Ji, and Q. Gu. Self-play fine-tuning converts weak language\\nmodels to strong language models. CoRR , abs/2401.01335, 2024.\\n[9]M. Deng, J. Wang, C. Hsieh, Y . Wang, H. Guo, T. Shu, M. Song, E. P. Xing, and Z. Hu.\\nRlprompt: Optimizing discrete text prompts with reinforcement learning. In Y . Goldberg,\\nZ. Kozareva, and Y . Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods\\nin Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December\\n7-11, 2022 , pages 3369–3391. Association for Computational Linguistics, 2022.\\n[10] Y . Deng, W. Zhang, Z. Chen, and Q. Gu. Rephrase and respond: Let large language models ask\\nbetter questions for themselves. CoRR , abs/2311.04205, 2023.\\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional\\ntransformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors,\\nProceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-\\npers) , pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational\\nLinguistics.\\n[12] S. Gunasekar, Y . Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaheripi,\\nP. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah, H. S. Behl, X. Wang, S. Bubeck,\\nR. Eldan, A. T. Kalai, Y . T. Lee, and Y . Li. Textbooks are all you need. CoRR , abs/2306.11644,\\n2023.\\n[13] R. T. Ionescu, B. Alexe, M. Leordeanu, M. Popescu, D. P. Papadopoulos, and V . Ferrari. How\\nhard can it be? estimating the difficulty of visual search in an image. In 2016 IEEE Conference\\non Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30,\\n2016 , pages 2157–2166. IEEE Computer Society, 2016.\\n[14] T. Kocmi and O. Bojar. Curriculum learning and minibatch bucketing in neural machine\\ntranslation. In R. Mitkov and G. Angelova, editors, Proceedings of the International Conference\\nRecent Advances in Natural Language Processing, RANLP 2017, Varna, Bulgaria, September 2\\n- 8, 2017 , pages 379–386. INCOMA Ltd., 2017.\\n9', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 8}),\n",
       " Document(page_content='[15] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt\\ntuning. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, Proceedings of the 2021\\nConference on Empirical Methods in Natural Language Processing , pages 3045–3059, Online\\nand Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics.\\n[16] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In C. Zong,\\nF. Xia, W. Li, and R. Navigli, editors, Proceedings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language\\nProcessing (Volume 1: Long Papers) , pages 4582–4597, Online, Aug. 2021. Association for\\nComputational Linguistics.\\n[17] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. GPT understands, too. CoRR ,\\nabs/2103.10385, 2021.\\n[18] Y . Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and\\nwhere to find them: Overcoming few-shot prompt order sensitivity. ArXiv , abs/2104.08786,\\n2021.\\n[19] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang.\\nWizardmath: Empowering mathematical reasoning for large language models via reinforced\\nevol-instruct. CoRR , abs/2308.09583, 2023.\\n[20] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts. Learning word vectors\\nfor sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for\\nComputational Linguistics: Human Language Technologies , pages 142–150, Portland, Oregon,\\nUSA, June 2011. Association for Computational Linguistics.\\n[21] A. G. Møller, J. A. Dalsgaard, A. Pera, and L. M. Aiello. Is a prompt and a few samples all\\nyou need? using GPT-4 for data augmentation in low-resource classification tasks. CoRR ,\\nabs/2304.13861, 2023.\\n[22] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah. Orca: Progres-\\nsive learning from complex explanation traces of GPT-4. CoRR , abs/2306.02707, 2023.\\n[23] E. A. Platanios, O. Stretcu, G. Neubig, B. Póczos, and T. M. Mitchell. Competence-based\\ncurriculum learning for neural machine translation. In J. Burstein, C. Doran, and T. Solorio, edi-\\ntors, Proceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis,\\nMN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pages 1162–1172. Association\\nfor Computational Linguistics, 2019.\\n[24] R. Pryzant, D. Iter, J. Li, Y . Lee, C. Zhu, and M. Zeng. Automatic prompt optimization with\\n“gradient descent” and beam search. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of\\nthe 2023 Conference on Empirical Methods in Natural Language Processing , pages 7957–7968,\\nSingapore, Dec. 2023. Association for Computational Linguistics.\\n[25] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi, J. Liu, T. Remez,\\nJ. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. Canton-Ferrer, A. Grattafiori,\\nW. Xiong, A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and\\nG. Synnaeve. Code llama: Open foundation models for code. CoRR , abs/2308.12950, 2023.\\n[26] E. Sangineto, M. Nabi, D. Culibrk, and N. Sebe. Self paced deep learning for weakly supervised\\nobject detection. IEEE Trans. Pattern Anal. Mach. Intell. , 41(3):712–725, 2019.\\n[27] M. Sclar, Y . Choi, Y . Tsvetkov, and A. Suhr. Quantifying language models’ sensitivity to\\nspurious features in prompt design or: How I learned to start worrying about prompt formatting.\\nCoRR , abs/2310.11324, 2023.\\n[28] M. Shi and V . Ferrari. Weakly supervised object localization using size estimates. In B. Leibe,\\nJ. Matas, N. Sebe, and M. Welling, editors, Computer Vision - ECCV 2016 - 14th European\\nConference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V , volume\\n9909 of Lecture Notes in Computer Science , pages 105–121. Springer, 2016.\\n[29] T. Shin, Y . Razeghi, R. L. L. IV , E. Wallace, and S. Singh. Autoprompt: Eliciting knowledge\\nfrom language models with automatically generated prompts. In B. Webber, T. Cohn, Y . He, and\\nY . Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language\\nProcessing, EMNLP 2020, Online, November 16-20, 2020 , pages 4222–4235. Association for\\nComputational Linguistics, 2020.\\n10', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 9}),\n",
       " Document(page_content='[30] P. Soviany, R. T. Ionescu, P. Rota, and N. Sebe. Curriculum learning: A survey. Int. J. Comput.\\nVis., 130(6):1526–1565, 2022.\\n[31] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.\\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/\\nstanford_alpaca , 2023.\\n[32] D. Vila-Suero and F. Aranda. Argilla - open-source framework for data-centric nlp, 2023.\\ncff-version: 1.2.0, message: \"If you use this software, please cite it as below.\", date-released:\\n2023-01-12.\\n[33] L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings\\nwith large language models. CoRR , abs/2401.00368, 2024.\\n[34] Y . Wen, N. Jain, J. Kirchenbauer, M. Goldblum, J. Geiping, and T. Goldstein. Hard prompts\\nmade easy: Gradient-based discrete optimization for prompt tuning and discovery. CoRR ,\\nabs/2302.03668, 2023.\\n[35] C. Yang, X. Wang, Y . Lu, H. Liu, Q. V . Le, D. Zhou, and X. Chen. Large language models as\\noptimizers. CoRR , abs/2309.03409, 2023.\\n[36] Y . Yang, A. K. Singh, M. Elhoushi, A. Mahmoud, K. Tirumala, F. Gloeckle, B. Rozière, C. Wu,\\nA. S. Morcos, and N. Ardalani. Decoding data quality via synthetic corruptions: Embedding-\\nguided pruning of code data. CoRR , abs/2312.02418, 2023.\\n[37] Q. Ye, M. Axmed, R. Pryzant, and F. Khani. Prompt engineering a prompt engineer, 2023.\\n[38] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning\\nmathematical reasoning with large language models. CoRR , abs/2308.01825, 2023.\\n[39] T. Zhang, X. Wang, D. Zhou, D. Schuurmans, and J. E. Gonzalez. TEMPERA: test-time\\nprompting via reinforcement learning. CoRR , abs/2211.11890, 2022.\\n[40] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot\\nperformance of language models. In M. Meila and T. Zhang, editors, Proceedings of the 38th\\nInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event ,\\nvolume 139 of Proceedings of Machine Learning Research , pages 12697–12706. PMLR, 2021.\\n[41] Y . Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. Large language models\\nare human-level prompt engineers. In The Eleventh International Conference on Learning\\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\\n11', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 10}),\n",
       " Document(page_content='Appendix\\nA Implementation details\\nIn this section, we provide a additional information on the implementation details. An architecture\\noverview of our system is provided in Figure 8. We also provide a list of the meta-prompts utilized\\nwithin the pipeline of our system.\\nGenerating initial samples (iteration 0)\\n1Assistant is a large language model designed to generate challenging samples for\\nevery task.\\n2Generate a list of {num_samples} challenging samples for the following task.\\n3### Task description:\\n4{task_description}\\n5### Task Instruction:\\n6{instruction}\\n7###\\n8The generated samples should be challenging and diverse such that using the task\\ninstruction as a prompt will result in the wrong result.\\n9The number of generated samples from each class should be balanced (i.e. the same\\nnumber of samples for each class)\\nGenerating samples (iteration >0)\\n1Assistant is a large language model designed to generate challenging samples for\\nevery task.\\n2Below a few prompts that were build to answer the given task description and their\\nfailure case.\\n3Task description:\\n4{task_description}\\n5\\n6## Examples of common failure, each sample is followed by the the model prediction\\nand the GT (ground truth)\\n7{history}\\n8######\\n9Here are few unique samples derived from realistic scenarios for the task outlined\\nabove.\\n10## Realistic Samples\\n11{extra_samples}\\n12#####\\n13This was the new proposed prompt:\\n14## Prompt\\n15{prompt}\\n16\\n17Your task is to generate {num_samples} by following this guidelines:\\n181. The generated samples should be diverse\\n192. They should preserve the style and the length of the given examples\\n203. The samples must be challenging and hard to classify by the model. This can be\\nachieved by:\\n21 1. targeting the same weakness that the model failed on in the given examples\\n22 2. targeting weakness that are different from the existing examples in the\\nfailure cases\\n234. The number of generated samples from each class should be balanced (i.e. the same\\nnumber of samples for each class)\\nAnalyzer prompt\\n1Assistant is a large language model designed to provide a high quality analysis for\\nevery task.\\n12', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 11}),\n",
       " Document(page_content='2You are given the following task description\\n3{task_description}\\n4\\n5Here is the prompt instructions that was given to the model:\\n6{prompt}\\n7\\n8An expert ranker evaluated the model’s performance on the given task description.\\n9and rank according to the following scale: {labels}\\n10\\n11The mean score for this prompt is: {accuracy}\\n12##\\n13Here is a list of challenging cases for the given prompt and their rank:\\n14##Challenging Cases:\\n15{failure_cases}\\n16\\n17###\\n18Note that the ranker labels are __absolutely correct__, but the prompts (task\\ndescriptions) may be incorrect and need modification.\\n19Your task is to provide a brief analysis of the given prompt performance.\\n20Guidelines:\\n211. The analysis should contain only the following information:\\n22 - A summary of the common mistakes of the prompt and the ways he can be improve\\nhis generation, try to cluster the failure cases into groups and describe each\\ngroup.\\n232. The total length of your analysis should be less than 200 token!\\n24###\\n25Analysis:\\nGenerating new proposed prompt\\n1Assistant is a large language model designed to provide the best prompt for every\\ntask.\\n2Below are a few suggested prompts for the task and their score, for the following\\ntask:\\n3{task_description}\\n4\\n5## Examples\\n6{history}\\n7######\\n8This is the error analysis for the last prompt:\\n9{error_analysis}\\n10######\\n11Your task is to generate:\\n121. A new prompt that is\\n13 -Different from all the prompts above\\n14 -Follows exactly the error analysis modification suggestions, and fix the prompt\\nto prevent the failure cases.\\n15 -Has a higher score than all the prompts above.\\n162. The predicted score of this prompt\\n17\\n18You must adhere the error analysis instructions! even in case it seems there is a\\ncontradiction between these instructions, and the task. The error analysis is\\ntested on a ground truth, thus represent the exact intent of the task.\\n19The generated prompt should be phrased as a clear classification instruction! it\\nshould not include any instructions and descriptions on the modification that\\nshould be done to the prompt.\\n20Note that the previous prompt contains an implicit assumptions on the intent of the\\ntask that might be incorrect. You should replace this assumption with more\\naccurate assumptions using the score of the previous prompts and the error\\nanalysis.\\n21The result prompt should indicate that the task is a classification class with the\\nfollowing labels {labels}!\\nModifying the task description for training the ranker\\n13', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 12}),\n",
       " Document(page_content='1Assistant is a large language model designed to generate a task description.\\n2You are given a task description phrased as text generation task given some user\\ninput. Your task is to rephrase it as a task that suppose to evaluate the\\nquality of the given generative task and how well it adhere to the user input.\\n3#####\\n4Input task description: {task_description}\\n5#####\\n6Rephrased task description:\\nModifying the initial prompt for training the ranker\\n1Assistant is a large language model designed to generate instructions for every task\\n.\\n2You are given a instructions phrased as text generation task.\\n3Your task is to write an instruction for a classification ranking task that suppose\\nto evaluate the quality of a generated sample given a user prompt for this\\ngenerative instruction.\\n4Guidelines:\\n51. The classifier labels are {label_schema}. The result instructions should indicate\\nexplicitly that the task is a classification class with the following labels {\\nlabel_schema}!\\n62. The generated instruction must also evaluate how well the generated sample adhere\\nthe user prompt\\n7#####\\n8Input generative instruction: {prompt}\\n9#####\\n10Rephrased classification quality evaluation instruction:\\nB Experiments: Additional details\\nIn this section, we provide additional material on the experiments provided in the paper.\\nFigure 7: Accuracy on the sentiment classification task, with respect to different numbers of training\\nsteps. IPC outperforms other tested methods, and results in lower variance.\\n14', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 13}),\n",
       " Document(page_content='Figure 8: Architecture overview.\\n15', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 14}),\n",
       " Document(page_content='GT ranker Establish a revised five-point assessment scale for the movie review generator\\nthat emphasizes precise differentiation between all levels of reflective accuracy\\nand expressed enthusiasm. Assign the classification labels \"1\", \"2\", \"3\", \"4\", and\\n\"5\", with each level distinctly representing the depth and sincerity of reflection\\non the movie description as well as the intensity and authenticity of enthusiasm:\\n1. Deficient Reflection and Lacking Enthusiasm (\"1\"): Reviews that funda-\\nmentally misrepresent or ignore the movie description and display a lack of\\ngenuine enthusiasm, signaling a complete disconnect and an ineffective critique.\\n2. Basic Reflection and Low Enthusiasm (\"2\"): Reviews that merely mention\\naspects of the movie description without depth and show only a low level of\\nenthusiasm, resulting in a critique that lacks persuasive power and engagement.\\n3. Adequate Reflection and Moderate Enthusiasm (\"3\"): Reviews that reflect\\nthe movie description accurately and convey moderate enthusiasm, offering a\\nfair and constructive critique that is neither overly zealous nor dispassionate.\\n4. Detailed Reflection and High Enthusiasm (\"4\"): Reviews that capture the\\nnuances of the movie description in detail and exhibit high enthusiasm, present-\\ning a critique that is both engaging and substantively insightful. 5. Exceptional\\nReflection and Intense Enthusiasm (\"5\"): Reviews that deeply engage with\\nthe movie description, demonstrating both an exceptional level of reflection\\nand intense enthusiasm, with critiques that significantly enhance the analysis\\nand captivate the audience. This adjusted scale is designed to correct the prior\\nmisclassifications by ensuring clear and accurate recognition of each level of\\nperformance, thereby avoiding the underestimation of high-quality reviews and\\nproviding a fair representation for lower-quality outputs.\\nInitial Prompt\\n(Mean rank: 4.4)Generate an authentic enthusiastic movie review for the following movie\\nOPRO [35]\\n(Mean rank: 4.3)Compose a compelling and positive movie review for a recent release, capturing\\nthe film’s essence while highlighting unique elements that make it stand out.\\nPE [37]\\n(Mean rank: 4.1)Create a movie review generator that articulates sophisticated genre-specific\\nlanguage while ensuring that advanced terminology is immediately followed\\nby comprehensible explanations. This approach guarantees that the review\\nremains accessible to a broad readership, including both seasoned cinephiles\\nand those new to film criticism. The generator must offer an in-depth narrative\\nanalysis, employing narrative theory to enrich the commentary on the film’s plot,\\ncharacter arcs, and thematic substance, all while maintaining an educational yet\\nnon-academic tone. It should craft reviews that exude excitement and provide\\nan even-handed critique, spotlighting the film’s artistic accomplishments as\\nwell as constructively addressing areas for improvement. The result should be\\nan authoritative and engaging review that captures the essence of the movie\\nwith enthusiasm and critical precision, appealing to a wide spectrum of movie\\nenthusiasts.\\nIPC\\n(Mean rank: 4.8)Devise a movie review generator that excels in providing a balanced and nu-\\nanced critique, ensuring that each review exudes genuine enthusiasm for the\\nmovie’s triumphs while offering an insightful, yet subdued, evaluation of its less\\nsuccessful elements. The generator must consistently utilize precise and vibrant\\nexamples to underpin its praises, such as remarkable performances, poignant\\nplot developments, and groundbreaking directorial choices. It is vital to include\\na section in the review that thoughtfully places the film within its genre and\\nhistorical cinematic context, highlighting its unique contributions and potential\\ninfluence. Reviews should read as an authoritative, persuasive endorsement\\nof the film’s merits, inviting readers to not only enjoy but also understand its\\nsignificance in the film world, all while keeping the tone wholly positive and\\nengaging.\\nTable 4: A comparison between the prompt with the best training score in each method on the\\nauthentic and enthusiastic generative task. The GT ranker is provided in the first row. While OPRO\\nand PE result in a prompt that performs worse than the initial simple prompt. Using IPC results in a\\nsignificantly better prompt.16', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 15}),\n",
       " Document(page_content='Sentiment GT Does this movie review have a positive sentiment? Answer Yes\\nor No. Answer No only in case there is an explicit expression of\\ndissatisfaction otherwise, answer yes.\\nSentiment Initial Prompt Does this movie review have a positive sentiment? answer Yes or\\nNo\\nPG GT Augment the IMDB review classifier to better recognize and\\nclassify reviews containing adult themes. The classifier should\\nlabel a review as ’Yes’ when it detects not only explicit descrip-\\ntions of sex, violence, and strong language but also subtle adult\\nthemes, such as suggestive dialogue, nuanced portrayals of rela-\\ntionships, and indirect references to sex or desire. Additionally,\\nthe classifier should account for adult humour and jokes that are\\npresented in a lighthearted manner, ensuring they are not missed.\\nIt should also accurately identify implicit adult content conveyed\\nthrough cinematic techniques or complex narratives, as well as\\nnon-graphic mature content like restrained portrayals of infidelity\\nor taboo relationships. Furthermore, personal remarks on the\\nattractiveness of actors or suggestive comments that reviewers\\nmake must be included in the ’Yes’ category. Conversely, reviews\\nthat lack explicit adult content, significant implications of mature\\nthemes, and any aforementioned subtleties should be classified\\nas ’No’.\\nPG Initial Prompt Does this movie contain ah adult content? answer Yes or No\\nTable 5: Prompts for the PG and sentiment classification tasks.\\n17', metadata={'source': 'https://arxiv.org/pdf/2402.03099v1.pdf', 'page': 16}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D\\nObject with Gaussian Splatting\\nCHEN YANG∗,Shanghai Jiao Tong University, China\\nSIKUANG LI∗,Shanghai Jiao Tong University, China\\nJIEMIN FANG†,Huawei, China\\nRUOFAN LIANG, University of Toronto, Canada\\nLINGXI XIE, Huawei, China\\nXIAOPENG ZHANG, Huawei, China\\nWEI SHEN‡,Shanghai Jiao Tong University, China\\nQI TIAN, Huawei, China\\n(a) 4 Input Views\\n (d) FreeNeRF   \\n (e) 3DGS  \\n (c) DietNeRF \\n (f) Ours   \\n (b) Ground Truth   \\n17.7 dB / 0.15 22.6 dB / 0.08 18.2 dB / 0.16 25.6 dB / 0.05 PSNR   / LPIPS\\n8.17dB / 0.28 11.9 dB / 0.32 15.7dB / 0.17 25.5 dB  / 0.05 PSNR   / LPIPS\\nFig. 1. We introduce GaussianObject, a framework capable of reconstructing high-quality 3D objects from only 4 images with Gaussian splatting. Gaus-\\nsianObject demonstrates superior performance over previous SOTA methods on challenging objects.\\nReconstructing and rendering 3D objects from highly sparse views is of\\ncritical importance for promoting applications of 3D vision techniques and\\nimproving user experience. However, images from sparse views only con-\\ntain very limited 3D information, leading to two significant challenges: 1)\\nDifficulty in building multi-view consistency as images for matching are\\ntoo few; 2) Partially omitted or highly compressed object information as\\nview coverage is insufficient. To tackle these challenges, we propose Gaus-\\nsianObject, a framework to represent and render the 3D object with Gaussian\\nsplatting, that achieves high rendering quality with only 4 input images.\\nWe first introduce techniques of visual hull and floater elimination which\\nexplicitly inject structure priors into the initial optimization process for\\n∗Equal contribution.\\n†Project lead.\\n‡Corresponding author.\\nAuthors’ addresses: Chen Yang, ycyangchen@sjtu.edu.cn, Shanghai Jiao Tong Uni-\\nversity, Shanghai, China; Sikuang Li, whlisikuang@outlook.com, Shanghai Jiao Tong\\nUniversity, Shanghai, China; Jiemin Fang, jaminfong@gmail.com, Huawei, Shang-\\nhai, China; Ruofan Liang, ruofan@cs.toronto.edu, University of Toronto, Toronto,\\nCanada; Lingxi Xie, 198808xc@gmail.com, Huawei, Beijing, China; Xiaopeng Zhang,\\nzxphistory@gmail.com, Huawei, Shanghai, China; Wei Shen, wei.shen@sjtu.edu.cn,\\nShanghai Jiao Tong University, Shanghai, China; Qi Tian, tian.qi1@huawei.com,\\nHuawei, Shenzhen, China.helping build multi-view consistency, yielding a coarse 3D Gaussian repre-\\nsentation. Then we construct a Gaussian repair model based on diffusion\\nmodels to supplement the omitted object information, where Gaussians are\\nfurther refined. We design a self-generating strategy to obtain image pairs\\nfor training the repair model. Our GaussianObject is evaluated on several\\nchallenging datasets, including MipNeRF360, OmniObject3D, and OpenIl-\\nlumination, achieving strong reconstruction results from only 4 views and\\nsignificantly outperforming previous state-of-the-art methods. Please visit\\nour project page at https://gaussianobject.github.io/.\\n1 INTRODUCTION\\nReconstructing and rendering 3D objects from 2D images has been\\na long-standing and important topic, which plays critical roles in\\na vast range of real-life applications. One key factor that impedes\\nusers, especially ones without expert knowledge, from widely us-\\ning these techniques is that usually dozens of multi-view images\\nneed to be captured, which is cumbersome and sometimes impracti-\\ncal. Efficiently reconstructing high-quality 3D objects from highly\\nsparse captured images is of great value for expediting downstream\\napplications such as 3D asset creation for game/movie production\\nand AR/VR products.arXiv:2402.10259v2  [cs.CV]  20 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 0}),\n",
       " Document(page_content='2•Yang and Li, et al.\\nIn recent years, a series of methods [ 18,36,49,55,61,73,80,82]\\nhave been proposed to reduce reliance on dense captures. However,\\nit is still challenging to produce high-quality 3D objects when the\\nviews become extremely sparse ,e.g. only 4 images in a 360◦range,\\nas shown in Fig. 1. We delve into the task of sparse-view recon-\\nstruction and discover two main challenges behind it. The first\\none lies in the difficulty of building multi-view consistency from\\nhighly sparse input. The 3D representation is easy to overfit the\\ninput images and degrades into fragmented pixel patches of training\\nviews without reasonable structures. The other challenge is that\\nwith sparse captures in a 360◦range, some content of the object\\ncan be inevitably omitted or severely compressed when observed\\nfrom extreme views1. The omitted or compressed information is\\nimpossible or hard to be reconstructed in 3D only from the input\\nimages.\\nTo tackle the aforementioned challenges, we introduce Gaus-\\nsianObject, a novel framework designed to reconstruct high-quality\\n3D objects from as few as 4 input images. We choose 3D Gaussian\\nsplitting (3DGS) [ 22] as the basic representation as it is fast and,\\nmore importantly, explicit enough. Benefiting from its point-like\\nstructure, we design several techniques for introducing object struc-\\nture priors, e.g. the basic/rough geometry of the object, to help build\\nmulti-view consistency, including visual hull [ 26] to locate Gaus-\\nsians within the object outline and floater elimination to remove\\noutliers. To erase artifacts caused by omitted or highly compressed\\nobject information, we propose a Gaussian repair model driven by\\n2D large diffusion models [ 44] which translates corrupted rendered\\nimages into high-fidelity ones. As normal diffusion models lack\\nthe ability to repair corrupted images, we design self-generating\\nstrategies to construct image pairs to tune the diffusion models,\\nincluding rendering images from leave-one-out training models and\\nadding 3D noises to Gaussian attributes. Images generated from the\\nrepair model can be used for refining the 3D Gaussians optimized\\nwith structure priors, where the rendering quality can be further\\nimproved.\\nOur contributions are summarized as follows:\\n•We propose to optimize 3D Gaussians from highly sparse views\\nwith explicit structure priors, where several techniques are de-\\nsigned, including the visual hull for initialization and floater\\nelimination for training.\\n•A Gaussian repair model based on diffusion models is proposed\\nto remove artifacts caused by omitted or highly compressed\\nobject information, where the rendering quality can be further\\nimproved.\\n•The overall framework GaussianObject shows strong perfor-\\nmance on several challenging real-world datasets, consistently\\noutperforming previous state-of-the-art methods for both quali-\\ntative and quantitative evaluation.\\n1When the view is orthogonal to the surface of the object, the observed information\\nattached to the surface can be largely reserved; On the contrary, the information will\\nbe severely compressed.2 RELATED WORKS\\n2.1 Differentiable Point-based Rendering\\nRecent studies, such as DSS [ 75] and SynSin [ 65], have introduced\\ndifferentiable point-based rendering methods to effectively utilize\\ninformation from point clouds. These methods enhance point clouds\\nwith additional features and employ neural networks to convert\\nthese features into images. Point-NeRF [ 71] accelerates the neural\\nradiance field (NeRF) [ 35] by rendering discrete neural points, yet it\\nstill requires a few seconds to render an image. Point-Radiance [ 78]\\nemploys spherical harmonics to represent the color of points and\\nutilizes a splatting strategy for real-time rendering. Built upon these\\nadvancements, 3DGS [ 22] integrates 3D Gaussians, achieving high-\\nquality reconstruction of real-world scenes at real-time speeds. How-\\never, point-based methods heavily depend on precise and dense\\ninitial point configurations, a significant hurdle in sparse 360◦re-\\nconstruction. To address this challenge, our approach integrates\\nobject structure priors, providing a robust solution for sparse se-\\ntups.\\n2.2 Neural Rendering for Sparse View Reconstruction\\nVanilla NeRF struggles in sparse settings. Techniques like Deng et al .\\n[9], Roessle et al . [43] , Somraj et al . [52] , Somraj and Soundarara-\\njan[53] use SfM-derived [ 46] visibility or depth mainly focus on\\nclosely aligned views. [ 70] uses ground truth depth maps, which\\nare costly to obtain in real-world images. Some methods [ 55,61]\\nestimate depths with monocular depth estimation models[ 40,42] or\\nsensors, but these are often too coarse. [ 18] uses a vision-language\\nmodel[ 39] for unseen view rendering, but the semantic consistency\\nis too high-level to guide low-level reconstruction. [ 49] combines a\\ndeep image prior with factorized NeRF, effectively capturing overall\\nappearance but missing fine details in input views. Priors based\\non information theory[ 23], continuity[ 36], symmetry[ 47], and fre-\\nquency regularization[ 56,73] are effective for specific scenarios,\\nlimiting their further applications. Besides, there are some methods\\n[16,19,20,63,72,83] that employ Visual Transformer (ViT)[ 11] to\\nreduce the requirements for constructing NeRFs.\\nThe recent progress in text-to-image diffusion models like the La-\\ntent Diffusion Model (LDM) [ 44] has spurred notable advancements\\nin 3D applications. Dreamfusion [ 38] proposes Score Distillation\\nSampling (SDS) for distilling NeRFs with 2D priors from a pre-\\ntrained diffusion model for 3D object generation from text prompts.\\nIt has been further refined for text-to-3D [ 6,27,34,50,60,62,64,74]\\nand 3D/4D editing [ 14,48] by various studies, demonstrating the\\nversatility of 2D diffusion models in 3D contexts. Chan et al . [4], Liu\\net al. [29] , Zhu and Zhuang [81] have adapted these methods for 3D\\ngeneration and view synthesis from single images, although these of-\\nten have strict input requirements and can produce overly saturated\\nimages. In sparse reconstruction, approaches like DiffusioNeRF [ 68],\\nSparseFusion [ 80], Deceptive-NeRF [ 30] and ReconFusion [ 66] in-\\ntegrate diffusion models with NeRFs, which require pertaining on\\nsubstantial 3D data.\\nWhile 3D Gaussian splatting (3DGS) shows strong power in novel\\nview synthesis, it struggles with sparse 360◦views similar to NeRF.\\nInspired by few-shot NeRFs, methods [ 7,69,82] have been developed\\nfor sparse 360◦reconstruction. However, these methods still severely', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 1}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting •3\\nReference Images\\nRendered ImagesVisual Hull\\n3D GaussiansCameras\\nNoisy Gaussians\\ng\\nÒA photo of [V]ÓÒA photo of [V]Ó\\nReference Images\\n Rendered ImagesDegraded Images Reference Images\\nNovel Renderings\\n Repaired Images\\n(a) Optimization with Structure Priors (b) Gaussian Repair Model Setup (c) Repair with Distance-Aware SamplingAdding \\n3D Noise\\nRenderingRenderingLeave-One-Out\\nInitializingDistance-Aware\\nSamplingGaussian Repair \\n  Model        \\n3D Gaussians\\n\\ud83d㷒\\ng\\nGaussian Repair \\n  Model        \\n3D Gaussians\\nRendering\\nUpdating\\nFig. 2. (a) We initialize 3D Gaussians by constructing a visual hull with camera parameters and masked images, optimizing them with the Lgsand refining\\nthrough floater elimination. (b) We use a novel ‘leave-one-out’ strategy and add 3D noise to Gaussians to generate corrupted Gaussian renderings. These\\nrenderings, paired with their corresponding reference images, facilitate the training of the Gaussian repair model employing Ltune. (c) Once trained, the\\nGaussian repair model is frozen and used to correct views that need to be rectified. These views are identified through distance-aware sampling. The repaired\\nimages and reference images are used to further optimize 3D Gaussians with LrepandLgs.\\nrely on the SfM points. Our GaussianObject proposes structure-\\nprior-aided Gaussian initialization to tackle this issue, drastically\\nreducing the required input views to only 4, a significant improve-\\nment compared with over 20 views required by FSGS [82].\\n3 METHOD\\nThe subsequent sections detail the methodology: Sec. 3.1 reviews\\nfoundational techniques; Sec. 3.2 introduces our overall framework;\\nSec. 3.3 describes how we take full advantage of the structure priors\\nfor initial optimization; Sec. 3.4 details the setup of our Gaussian\\nrepair model, and Sec. 3.5 illustrates the repair of 3D Gaussians using\\nthis model.\\n3.1 Preliminary\\n3D Gaussian Splatting. 3D Gaussian Splatting [ 22] represents a\\n3D scene with 3D Gaussians. Each 3D Gaussian is composed of the\\ncenter location 𝜇, rotation quaternion 𝑞, scaling vector 𝑠, opacity𝜎,\\nand spherical harmonic (SH) coefficients 𝑠ℎ. Thus, a scene is param-\\neterized as a set of Gaussians G={𝐺𝑖:𝜇𝑖,𝑞𝑖,𝑠𝑖,𝜎𝑖,𝑠ℎ𝑖}𝑃\\n𝑖=1. 3DGS\\nemploys the neural point-based 𝛼-blending technique to render the\\ncolor𝐶(𝑢)of a pixel𝑢. Considering the view direction 𝑣𝑖, the pixel\\ncolor is computed as:\\n𝐶(𝑢)=∑︁\\n𝑖∈𝑁SH(𝑠ℎ𝑖,𝑣𝑖)𝛼𝑖𝑖−1Ö\\n𝑗=1(1−𝛼𝑗), 𝛼𝑖=𝜎𝑖𝐺𝑖(𝑝). (1)\\nDiffusion Models and ControlNet. Diffusion models [ 10,51,57] are\\na category of generative models capable of sampling realistic images\\nfrom data distribution 𝑞(𝑋0), starting with Gaussian noise 𝜖. This\\nis achieved using various sampling schedulers [ 12,15,21,32,54,\\n58]. The main process involves reversing a discrete-time stochastic\\nprocess{𝑋𝑡}𝑇\\n𝑡=0, represented as:\\n𝑋𝑡=√¯𝛼𝑡𝑋0+√1−¯𝛼𝑡𝜖,where𝜖∼N( 0,𝐼), (2)where𝑡∈[0,𝑇]is the noise level and ¯𝛼1:𝑇∈(0,1]𝑇is a decreasing\\nsequence. In the reversed process, a diffusion model 𝑝𝜃(𝑋𝑡−1|𝑋𝑡)is\\ntrained to approximate 𝑞(𝑋𝑡−1|𝑋𝑡). Substituting 𝑋0with its latent\\ncode𝑍0from a Variational Autoencoder (VAE) [ 24] leads to the de-\\nvelopment of Latent Diffusion Models (LDM) [ 44]. ControlNet [ 76]\\nfurther enhances the generative process with additional image con-\\nditioning by integrating a network structure similar to the diffusion\\nmodel, which slightly changes the loss function to:\\nL𝐶𝑜𝑛𝑑 =E𝑍0,𝑡,𝜖[∥𝜖𝜃(√¯𝛼𝑡𝑍0+√1−¯𝛼𝑡𝜖,𝑡,𝑐tex,𝑐img)−𝜖∥2\\n2],(3)\\nwhere𝑐texand𝑐imgdenote the text and image conditioning respec-\\ntively.\\n3.2 Overall Framework\\nGiven a sparse collection of 𝑁reference images 𝑋ref={𝑥𝑖}𝑁\\n𝑖=1,\\ncaptured within a 360◦range and encompassing one object, along\\nwith the corresponding camera parameters Πref={𝜋𝑖}𝑁\\n𝑖=1and\\nmasks𝑀ref={𝑚𝑖}𝑁\\n𝑖=1of the object, our target is to obtain a 3D\\nrepresentationG, which can achieve photo-realistic rendering from\\nany viewpoint 𝑥=G(𝜋|{𝑥𝑖,𝜋𝑖,𝑚𝑖}𝑁\\n𝑖=1). To achieve this, we employ\\nthe 3DGS model for its simplicity for structure priors embedding\\nand fast rendering capabilities. The process begins with initializing\\n3D Gaussians using a visual hull [ 26], followed by optimization with\\nfloater elimination, enhancing the structure of Gaussians. Then we\\ndesign self-generating strategies to supply sufficient image pairs\\nfor constructing a Gaussian repair model, which is used to rectify\\nincomplete object information. Our overall framework is shown in\\nFig. 2.\\n3.3 Initial Optimization with Structure Priors\\nSparse views, especially for only 4 images, provide very limited\\n3D information for reconstruction. In this case, SfM points, which\\nare the key for 3DGS initialization, are often absent. Besides, in-\\nsufficient multi-view consistency leads to ambiguity among shape', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 2}),\n",
       " Document(page_content='4•Yang and Li, et al.\\nand appearance, resulting in many floaters during reconstruction.\\nWe propose two techniques to initially optimize the 3D Gaussian\\nrepresentation, which take full advantage of structure priors from\\nthe limited views and result in a satisfactory outline of the object.\\nInitialization with Visual Hull To better leverage object struc-\\nture information from limited reference images, we utilize the view\\nfrustums and object masks to create a visual hull that serves as a geo-\\nmetric scaffold for initializing our 3D Gaussians. Compared with few\\nSfM points that contain little valid information, the visual hull pro-\\nvides more structure priors that help build multi-view consistency\\nby excluding unreasonable Gaussian distributions. The cost of the\\nvisual hull is just several masks of sparse 360◦images, which can be\\neasily acquired via segmentation model [ 25]. Specifically, points are\\nrandomly initialized within the visual hull using rejection sampling:\\nwe project uniformly sampled random 3D points onto image planes\\nand retain those within the intersection of all image-space masks.\\nPoint colors are averaged from bilinearly interpolated pixel colors\\nacross reference image projections. Then we transform these 3D\\npoints into 3D Gaussians. For each point, we assign its position as 𝜇\\nand convert its color into 𝑠ℎ. The mean distance between adjacent\\npoints is used to form the scale 𝑠, while the rotation 𝑞is set to a\\nunit quaternion as default. The opacity 𝜎is initialized to a constant\\nvalue. This initialization strategy relies on the initial masks. Despite\\npotential inaccuracies in these masks or unrepresented concavities\\nby the visual hull, we have observed that subsequent optimization\\nprocesses reliably yield high-quality reconstructions.\\nFloater Elimination While the visual hull builds a coarse estima-\\ntion of the object geometry, it often contains regions that do not\\nbelong to the object due to the inadequate coverage of reference\\nimages. These regions usually appear to be floaters which damage\\nthe quality of novel view synthesis. These floaters are problematic as\\nthe optimization process struggles to adjust them due to insufficient\\nobservational data regarding their position and appearance.\\nTo mitigate this issue, we propose to utilize the statistical distri-\\nbution of distances among the 3D Gaussians to distinguish the pri-\\nmary object and the floaters. This is implemented by the K-Nearest\\nNeighbors (KNN) algorithm, which calculates the average distance\\nto the nearest√\\n𝑃Gaussians for each element in G𝑐. We then es-\\ntablish a normative range by computing the mean and standard\\ndeviation of these distances. Based on this statistical analysis, we\\nexclude any Gaussian whose mean neighbor distance exceeds the\\nadaptive threshold 𝜏. This thresholding process is repeated periodi-\\ncally throughout optimization, where 𝜏is linearly decreased to 0 to\\nprogressively refine the scene representation.\\nInitial Optimization The optimization of G𝑐incorporates color,\\nmask, and monocular depth losses. The color loss combines L1 and\\nD-SSIM losses:\\nL1=∥𝑥−𝑥ref∥1,LD-SSIM =1−SSIM(𝑥,𝑥ref), (4)\\nwhere𝑥is the rendering and 𝑥refis the corresponding reference\\nimage. A binary cross entropy (BCE) loss is applied as mask loss:\\nLm=−(𝑚reflog𝑚+(1−𝑚ref)log(1−𝑚)), (5)\\nwhere𝑚denotes the object mask. A shift and scale invariant depth\\nloss is utilized to guide geometry:\\nLd=∥𝐷∗−𝐷∗\\npred∥1, (6)\\nÒA photo of [V]Ó\\nLoRA\\nDegraded Image \\ud835㗉!\\nControlNet\\nStable\\nDiffusion\\nPredicted Noise \\ud835㗵\\ud835㗳+\\nGaussian Noise \\ud835㗵\\n Reference Image \\ud835㗉#$%\\ud835㗉\\ud835㗉\\n\\ud83d㷒Fig. 3. Illustration of the optimization of our Gaussian repair model.\\nwhere𝐷∗and𝐷∗\\npredare per-frame rendered depths and monocularly\\nestimated depths [ 2] respectively. The depth values are computed\\nfollowing a normalization strategy [41]:\\n𝐷∗=𝐷−median(𝐷)\\n1\\n𝑀Í𝑀\\n𝑖=1|𝐷−median(𝐷)|, (7)\\nwhere𝑀denotes the number of valid pixels. The overall loss com-\\nbines these components as\\nL𝑔𝑠=(1−𝜆SSIM)L1+𝜆SSIMLD−SSIM+𝜆mLm+𝜆dLd,(8)\\nwhere𝜆SSIM,𝜆m, and𝜆dcontrol the magnitude of each term. Thanks\\nto the efficient initialization, our training speed is remarkably fast.\\nIt only takes 1 minute to train a coarse Gaussian representation G𝑐\\nat a resolution of 779 ×520.\\n3.4 Gaussian Repair Model Setup\\nThe integration of visual hull initialization with a floater elimina-\\ntion strategy remarkably improves the performance of 3DGS for\\nnovel view synthesis in sparse 360◦contexts. While the fidelity of\\nour reconstruction is generally passable, G𝑐still suffers in regions\\nthat are poorly observed, regions with occlusion, or even unob-\\nserved regions. These challenges loom over the completeness of the\\nreconstruction like the sword of Damocles.\\nTo mitigate these issues, we introduce a Gaussian repair model\\nRdesigned to correct the aberrant distribution of G𝑐. OurRis\\npowered by 2D diffusion models, which takes corrupted rendered\\nimages𝑥′(G𝑐,𝜋nov)as input and outputs photo-realistic and high-\\nfidelity images ˆ𝑥. This image repair capability can be used to refine\\nthe 3D Gaussians, leading to learn better structure and appearance\\ndetails.\\nSufficient data pairs are essential for training Rbut are rare in\\nexisting datasets. To this end, we adopt two main strategies for\\ngenerating adequate image pairs, i.e.,leave-one-out training and\\nadding 3D noises . For leave-one-out training, we build 𝑁subsets\\nfrom the𝑁input images, each containing 𝑁−1reference images\\nand 1left-out image 𝑥out. Then we train 𝑁3DGS models with\\nreference images of these subsets, termed as {G𝑖𝑐}𝑁−1\\n𝑖=0. After specific\\niterations, we use the left-out image 𝑥outto continue training each\\nGaussian model{G𝑖𝑐}𝑁−1\\n𝑖=0into{ˆG𝑖𝑐}𝑁−1\\n𝑖=0. Throughout this process,\\nthe rendered images from the left-out view at different iterations\\nare stored to form the image pairs along with left-out image 𝑥outfor\\ntraining the repair model. Note that training these left-out models', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 3}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting •5\\ncost little, with less than 𝑁minutes in total. The other strategy is to\\nadd 3D noises 𝜖𝑠onto the attributes of Gaussians. The 𝜖𝑠are derived\\nfrom the mean 𝜇Δand variance 𝜎Δof attribute differences between\\n{G𝑖𝑐}𝑁−1\\n𝑖=0and{ˆG𝑖𝑐}𝑁−1\\n𝑖=0. This allows us to render more degraded\\nimages𝑥′(G𝑐(𝜖𝑠),𝜋ref)at all reference views from the created noisy\\nGaussians, resulting in extensive image pairs (𝑋′,𝑋ref).\\nWe inject LoRA weights and fine-tune a pre-trained Control-\\nNet [ 77] using the generated image pairs to serve as our Gaussian\\nrepair model. The training procedure is shown in Fig. 3. The loss\\nfunction, based on Eq. 3, is defined as:\\nLtune=E𝑥ref,𝑡,𝜖,𝑥′h\\n∥(𝜖𝜃(𝑥ref\\n𝑡,𝑡,𝑥′,𝑐tex)−𝜖)∥2\\n2i\\n, (9)\\nwhere𝑐texdenotes an object-specific language prompt, defined as “a\\nphoto of [V],” as per Dreambooth [ 45]. Specifically, we inject LoRA\\nlayers into the text encoder, image condition branch and U-Net for\\nfine-tuning. Please refer to the Appendix for details.\\n3.5 Gaussian Repair with Distance-Aware Sampling\\nAfter trainingR, we distill its target object priors into G𝑐to refine\\nits rendering quality. The object information near the reference\\nviews is abundant. This observation motivates designing distance\\nas a criterion in identifying views that need to be rectified, leading\\nto distance-aware sampling.\\nSpecifically, we establish an elliptical path aligned with the train-\\ning views and focus on a central point. Arcs near Π𝑟𝑒𝑓, where we\\nassumeG𝑐renders high-quality images, form the reference path.\\nThe other arcs, yielding renderings need to be rectified, define the\\nrepair path, as depicted in Fig. 4. In each iteration, novel viewpoints,\\n𝜋𝑗∈Πnov, are randomly sampled among the repair path. For each\\n𝜋𝑗, we render the corresponding image 𝑥𝑗=𝑥𝑗(G𝑐,𝜋𝑗), encode it\\nto beE(𝑥𝑗)by the latent diffusion encoder Eand passE(𝑥𝑗)to the\\nimage conditioning branch of R. Simultaneously, a cloned E(𝑥𝑗)is\\ndisturbed into a noisy latent 𝑧𝑡:\\n𝑧𝑡=√¯𝛼𝑡E(𝑥𝑗)+√1−¯𝛼𝑡𝜖,\\nwhere𝜖∼N( 0,𝐼),𝑡∈[0,𝑇], (10)\\nwhich is similar to SDEdit [ 33]. We then generate a sample ˆ𝑥𝑗from\\nRby running DDIM sampling [ 54] over𝑘=⌊50·𝑡\\n𝑇⌋steps and\\nforwarding the diffusion decoder D:\\nˆ𝑥𝑗=D(DDIM(𝑧𝑡,E(𝑥𝑗))). (11)\\nThe distance from 𝜋𝑗to𝜋𝑟𝑒𝑓is used to weight the reliability of ˆ𝑥𝑗,\\nguiding the optimization with a loss function:\\nLrep=E𝜋𝑗,𝑡\\x02\\n𝑤(𝑡)𝜆(𝜋𝑗)\\x00∥𝑥𝑗−ˆ𝑥𝑗∥1+∥𝑥𝑗−ˆ𝑥𝑗∥2+𝐿𝑝(𝑥𝑗,ˆ𝑥𝑗)\\x01\\x03\\n,\\nwhere𝜆(𝜋𝑗)=2·min𝑁\\n𝑖=1(∥𝜋𝑗−𝜋𝑖∥2)\\n𝑑max. (12)\\nHere,𝐿𝑝denotes the perceptual similarity metric LPIPS [ 79],𝑤(𝑡)\\nis a noise-level modulated weighting function, 𝜆(𝜋𝑗)denotes a\\ndistance-based weighting function, and 𝑑maxis the maximal distance\\namong neighboring reference viewpoints. To ensure coherence be-\\ntween 3D Gaussians and reference images, we continue training G𝑐\\nwithLgsduring the whole Gaussian repair procedure.\\nRepair Path\\n Reference PathFig. 4. Illustration of our distance-aware sampling. Blue and red indicate\\nthe reference and repair path, respectively.\\n4 EXPERIMENTS\\n4.1 Implementation Details\\nOur framework, illustrated in Fig. 2, is based on 3DGS [ 22] and\\nthreestudio [ 13]. The 3DGS model is trained for 10k iterations in\\nthe initial optimization, with periodic floater elimination every 500\\niterations. We inherent densification and reset opacity from 3DGS,\\nwith every 100 and 1000 iterations. The monocular depth for Ldis\\npredicted by ZoeDepth [2].\\nIn the Gaussian repair model setup, the backbone is a ControlNet-\\nTile [ 77] model based on stable diffusion v1.5 [ 44], optimized using\\nan AdamW optimizer [ 31] with𝛽values of(0.9,0.999). LoRA [ 17]\\nweights, injected into the text-encoder and transformer blocks using\\nminLoRA [ 5], are trained for 1800 steps at a LoRA rank of 64 and a\\nlearning rate of 10−3.\\nG𝑐is trained for another 4k iterations during distance-aware\\nsampling. For the first 2800 iterations, optimization involves both a\\nreference image and a repaired novel view image, with the weight\\nofLrepprogressively decayed from 1.0to0.1. The final 1200-step\\ntraining only involves reference views. The whole process of Gaus-\\nsianObject takes about 30 minutes on a GeForce RTX 3090 GPU for\\n4 input images at a 779 ×520 resolution.\\n4.2 Datasets\\nWe evaluate our proposed method on three datasets suited for sparse-\\nview 360◦object reconstruction with varying input views, including\\nMip-NeRF360 [ 1], OmniObject3D [ 67], and OpenIllumination [ 28].\\nFor datasets that do not provide object masks, we employ SA3D [ 3]\\nto get the target object masks. More details about datasets can be\\nreferred to in the Appendix.\\n4.3 Evaluation\\nWe evaluate the performance of GaussianObject against several re-\\nconstruction baselines, including the vanilla 3DGS [ 22] with random\\ninitialization and DVGO [ 59], and various few-view reconstruction\\nmodels on the three datasets. Compared methods of RegNeRF [ 36],\\nDietNeRF [ 18], SparseNeRF [ 61], and ZeroRF [ 49] utilize a variety\\nof regularization techniques. Besides, FSGS [ 82] is also built upon\\nGaussian splatting with SfM-point initialization. Note that we sup-\\nply extra SfM points to FSGS so that it can work with the highly\\nsparse 360◦setting. All models are trained using the publicly re-\\nleased implementation codes, with further implementation details\\navailable in the Appendix.', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 4}),\n",
       " Document(page_content='6•Yang and Li, et al.\\nGT Ours FSGS ZeroRF SparseNeRF FreeNeRF RegNeRF DietNeRF 3DGS DVGOMipNeRF360\\n OmniObject3D\\nFig. 5. Qualitative examples on the MipNeRF360 and OmniObject3D dataset with 4 input views. Many methods fail to reach a coherent 3D representation,\\nresulting in floaters and disjoint pixel patches. Note that a pure white image indicates a total miss of the object by the corresponding method usually caused\\nby an overfitting to the input images.', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 5}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting •7\\nTable 1. Comparisons on MipNeRF360 and OmniObject3D datasets with varying input views. LPIPS∗=LPIPS×102throughout this paper.\\nMethod4-view 6-view 9-view\\nLPIPS∗↓PSNR↑SSIM↑LPIPS∗↓PSNR↑SSIM↑LPIPS∗↓PSNR↑SSIM↑MipNeRF360DVGO [59] 24.43 14.39 0.7912 26.67 14.30 0.7676 25.66 14.74 0.7842\\n3DGS [22] 10.80 20.31 0.8991 8.38 22.12 0.9134 6.42 24.29 0.9331\\nDietNeRF [18] 11.17 18.90 0.8971 6.96 22.03 0.9286 5.85 23.55 0.9424\\nRegNeRF [36] 20.44 13.59 0.8476 20.72 13.41 0.8418 19.70 13.68 0.8517\\nFreeNeRF [73] 16.83 13.71 0.8534 6.84 22.26 0.9332 5.51 27.66 0.9485\\nSparseNeRF [61] 17.76 12.83 0.8454 19.74 13.42 0.8316 21.56 14.36 0.8235\\nZeroRF [49] 19.88 14.17 0.8188 8.31 24.14 0.9211 5.34 27.78 0.9460\\nFSGS [82] 9.51 21.07 0.9097 7.69 22.68 0.9264 6.06 25.31 0.9397\\nGaussianObject (Ours) 4.98 24.81 0.9350 3.63 27.00 0.9512 2.75 28.62 0.9638OmniObject3DDVGO [59] 14.48 17.14 0.8952 12.89 18.32 0.9142 11.49 19.26 0.9302\\n3DGS [22] 8.60 17.29 0.9299 7.74 18.29 0.9378 6.50 20.26 0.9483\\nDietNeRF [18] 11.64 18.56 0.9205 10.39 19.07 0.9267 10.32 19.26 0.9258\\nRegNeRF [36] 16.75 15.20 0.9091 14.38 15.80 0.9207 10.17 17.93 0.9420\\nFreeNeRF [73] 8.28 17.78 0.9402 7.32 19.02 0.9464 7.25 20.35 0.9467\\nSparseNeRF [61] 17.47 15.22 0.8921 21.71 15.86 0.8935 23.76 17.16 0.8947\\nZeroRF [49] 4.44 27.78 0.9615 3.11 31.94 0.9731 3.10 32.93 0.9747\\nFSGS [82] 6.25 24.71 0.9545 6.05 26.36 0.9582 4.17 29.16 0.9695\\nGaussianObject (Ours) 2.07 30.89 0.9756 1.55 33.31 0.9821 1.20 35.49 0.9870\\nTable 2. Quantitative comparisons on the OpenIllumination dataset. Meth-\\nods with †means the metrics are from the ZeroRF paper [49].\\nMethod4-view 6-view\\nLPIPS∗↓PSNR↑SSIM↑LPIPS∗↓PSNR↑SSIM↑\\nDVGO 11.84 21.15 0.8973 8.83 23.79 0.9209\\n3DGS 30.08 11.50 0.8454 29.65 11.98 0.8277\\nDietNeRF†10.66 23.09 0.9361 9.51 24.20 0.9401\\nRegNeRF†47.31 11.61 0.6940 30.28 14.08 0.8586\\nFreeNeRF†35.81 12.21 0.7969 35.15 11.47 0.8128\\nSparseNeRF 22.28 13.60 0.8808 26.30 12.80 0.8403\\nZeroRF†9.74 24.54 0.9308 7.96 26.51 0.9415\\nOurs 6.71 24.64 0.9354 5.44 26.54 0.9443\\nTable 1 and Table 2 present the view-synthesis performance of\\nGaussianObject compared to existing methods on the MipNeRF360,\\nOmniObject3D, and OpenIllumination datasets. Experiments show\\nthat GaussianObject consistently achieves state-of-the-art results\\nacross all datasets, especially in the perceptual quality – LPIPS. Al-\\nthough GaussianObject is designed to address extremely sparse in-\\nput views, it still outperforms other methods with more input views,\\ne.g. 6 and 9, further proving the effectiveness. Notably, GaussianOb-\\nject excels with as few as 4 views and significantly improves LPIPS\\nover FSGS from 0.0951 to 0.0498 on MipNeRF360. This improvement\\nis critical, as LPIPS is a key indicator of perceptual quality [37].\\nFig. 5 and Fig. 6 illustrate rendering results of various methods\\nacross different datasets with only 4 input views. We observe that\\nGaussianObject achieves significantly better visual quality and fi-\\ndelity compared to the competing models. Notably, We find that\\nimplicit representation-based methods and random initialized 3DGS\\nfail in extremely sparse settings, typically reconstructing objects\\nOurs\\n ZeroRF\\n GTFig. 6. Qualitative results on the OpenIllumination dataset. Although Ze-\\nroRF shows competitive PSNR and SSIM, its renderings often appear blurred.\\nWhile GaussianObject outperforms in restoring fine details, thereby achiev-\\ning a significant advantage in perceptual quality.\\nas fragmented pixel patches. This confirms the effectiveness of in-\\ntegrating structure priors with explicit representations. Although\\nZeroRF exhibits competitive PSNR and SSIM on OpenIllumination,\\nwe observe that its renderings are blurred and lack details, as shown\\nin Fig. 6. In contrast, GaussianObject demonstrates fine-detailed\\nreconstruction. This superior perceptual quality highlights the effec-\\ntiveness of the Gaussian repair model. It is highly suggested to refer\\nto comprehensive video comparisons included in the supplementary.\\n4.4 Ablation Studies\\nKey Components We conduct a series of experiments to validate\\nthe effectiveness of each component. The following experiments\\nare performed on MipNeRF360 with 4 input views and averaged', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 6}),\n",
       " Document(page_content='8•Yang and Li, et al.\\nTable 3. Ablation study on key components.\\nMethod LPIPS∗↓PSNR↑SSIM↑\\nOurs w/o Visual Hull 12.72 15.95 0.8719\\nOurs w/o Floater Elimination 4.99 24.73 0.9346\\nOurs w/o Gaussian Repair 5.55 24.37 0.9297\\nOurs w/ SDS [38] 6.07 22.42 0.9188\\nGaussianObject (Ours) 4.98 24.81 0.9350\\nOurs\\n w/ SDS\\n w/o GR\\n w/o FE\\n w/o VH\\n GT\\nFig. 7. Ablation study on different components. “VH” denotes for visual\\nhull, “FE” is floater elimination and “GR” means Gaussian repair.\\nmetric values are reported. We disable visual hull initialization,\\nfloater elimination and Gaussian repair once at a time to verify their\\neffectiveness. The Gaussian repair loss is further compared with the\\nScore Distillation Sampling (SDS) loss [ 38]. The results, presented\\nin Table 3 and Fig. 7, indicate that each component significantly\\ncontributes to performance, with their absence leading to a decline\\nin results. Particularly, omitting visual hull initialization results in a\\nmarked decrease in performance, while Gaussian repair markedly\\nenhances perceptual quality. Contrary to its effectiveness in text-\\nto-3D or single image-to-3D, SDS in our context results in unstable\\noptimization and diminished performance.\\nStructure of Repair Model Our repair model is designed to gen-\\nerate photorealistic and 3D-consistent views of the target object.\\nThis is achieved by leave-one-out training and perturbing the at-\\ntributes of 3D Gaussians to create image pairs for fine-tuning a\\npretrained image-conditioned ControlNet. Similarities can be found\\nin Dreambooth [ 45], which aims to generate specific subject im-\\nages from limited inputs. To validate the efficacy of our design,\\nwe evaluate the samples generated by our Gaussian repair model\\nand other alternative structures. The first one is implemented with\\nDreambooth [ 45] for embedding target object priors with seman-\\ntic modifications. The second one uses SDEdit [ 33] to guide the\\nimage generation. Inspired by Song et al . [55] , the third one intro-\\nduces a monocular depth conditioning ControlNet [ 76] which is\\nfinetuned using data pair generation as in Sec. 3.4. We also assess\\nthe performance using masked depth conditioning. Furthermore, we\\nconsider Zero123-XL [ 8,29], a well-known single-image reconstruc-\\ntion model, requiring object-centered input images with precise\\ncamera rotations and positions. Here, we manually align the coordi-\\nnate system and select the closest image to the novel viewpoint as\\nits reference.\\nThe results, as shown in Table 4 and Fig. 8, reveal that semantic\\nmodifications alone fail in 3D-coherent synthesis. Monocular depth\\nconditioning, despite some improvements, still struggles with depthTable 4. Ablation study about alternatives of the Gaussian repair model.\\nMethod LPIPS∗↓PSNR↑SSIM↑\\nDreambooth [45] 6.58 21.85 0.9093\\nDepth Condition 7.00 21.87 0.9112\\nDepth Condition w/ Mask 6.87 21.92 0.9117\\nGaussianObject (Ours) 5.79 23.55 0.9220\\nOurs\\n MDepth\\n SDEdit\\n Zero123-XL\\n Input\\nFig. 8. Qualitative comparisons by ablating different Gaussian repair model\\nalternatives. “MDepth” denotes the repair model with masked monocular\\ndepth estimation as the condition.\\nroughness and artifacts. Zero123-XL, while generating visually ac-\\nceptable images, the multi-view structure consistency is lacking. In\\ncontrast, our model excels in both 3D consistency and detail fidelity,\\noutperforming others qualitatively and quantitatively.\\n5 DISCUSSION AND CONCLUSION\\nLimitations &Future Work. GaussianObject demonstrates notable\\nperformance in sparse 360◦object reconstruction, yet several av-\\nenues for future research exist, including reducing reliance on pre-\\ncise camera poses, mitigating popping artifacts in extreme views,\\nand addressing color shifts in stable diffusion VAEs. The current\\ndependency of GaussianObject on precise poses for visual hull con-\\nstruction and optimization raises challenges among daily life. A po-\\ntential solution is to optimize camera parameters together with 3D\\nreconstruction under specific capturing rules. Additionally, 3DGS-\\nlike methods occasionally exhibit popping artifacts under extreme\\nviewpoints, affecting visual quality. Future work could explore anti-\\naliasing techniques or a more comprehensive repair path, such as a\\nhemisphere repair path, to address this issue. Moreover, we observe\\nconsistent color shifts on white backgrounds due to the VAE of\\nstable diffusion, which could potentially impact the Gaussian repair\\nprocess. Training the repair model from a more powerful diffusion\\nmodel may mitigate this issue.\\nIn summary, GaussianObject is a novel framework designed\\nfor high-quality 3D object reconstruction from extremely sparse\\n360◦views, based on 3DGS with real-time rendering capabilities. We\\ndesign two main methods to achieve this goal: structure-prior-aided\\noptimization for facilitating the multi-view consistency construction\\nand a Gaussian repair model to remove artifacts caused by omitted\\nor highly compressed object information. We hope that GaussianOb-\\nject can advance daily-life applications of reconstructing 3D objects,', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 7}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting •9\\nmarkedly reducing capture requirements and broadening applica-\\ntion prospects.\\nREFERENCES\\n[1] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter\\nHedman. 2021. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields.\\n2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\n(2021), 5460–5469. https://api.semanticscholar.org/CorpusID:244488448\\n[2] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller.\\n2023. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv\\npreprint arXiv:2302.12288 (2023).\\n[3]Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie,\\nDongsheng Jiang, Xiaopeng Zhang, and Qi Tian. 2023. Segment Anything in 3D\\nwith NeRFs. In NeurIPS .\\n[4] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W Bergman, Jeong Joon\\nPark, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wet-\\nzstein. 2023. GeNVS: Generative novel view synthesis with 3D-aware diffusion\\nmodels.\\n[5] Jonathan Chang. 2023. minLoRA. https://github.com/cccntu/minLoRA.\\n[6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentan-\\ngling Geometry and Appearance for High-quality Text-to-3D Content Creation.\\nInProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) .\\n22246–22256.\\n[7] Jaeyoung Chung, Jeongtaek Oh, and Kyoung Mu Lee. 2024. Depth-Regularized\\nOptimization for 3D Gaussian Splatting in Few-Shot Images. arXiv:2311.13398\\n(Jan. 2024). https://doi.org/10.48550/arXiv.2311.13398\\n[8] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya\\nKusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli\\nVanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani,\\nLudwig Schmidt, and Ali Farhadi. 2023. Objaverse-XL: A Universe of 10M+ 3D\\nObjects. arXiv preprint arXiv:2307.05663 (2023).\\n[9]Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. 2022. Depth-\\nsupervised NeRF: Fewer Views and Faster Training for Free. In 2022 IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR) . 12872–12881.\\nhttps://doi.org/10.1109/CVPR52688.2022.01254\\n[10] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion Models Beat GANs on\\nImage Synthesis. In Advances in Neural Information Processing Systems , M. Ranzato,\\nA. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34.\\nCurran Associates, Inc., 8780–8794. https://proceedings.neurips.cc/paper_files/\\npaper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf\\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\\nWorth 16x16 Words: Transformers for Image Recognition at Scale. In Interna-\\ntional Conference on Learning Representations . https://openreview.net/forum?id=\\nYicbFdNTTy\\n[12] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen,\\nLu Yuan, and Baining Guo. 2022. Vector Quantized Diffusion Model for Text-to-\\nImage Synthesis. In 2022 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) . 10686–10696. https://doi.org/10.1109/CVPR52688.2022.01043\\n[13] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti,\\nGuan Luo, Chia-Hao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-\\nHai Zhang. 2023. threestudio: A unified framework for 3D content generation.\\nhttps://github.com/threestudio-project/threestudio.\\n[14] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo\\nKanazawa. 2023. Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions. In\\nProceedings of the IEEE/CVF International Conference on Computer Vision .\\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\\nmodels. Advances in neural information processing systems 33 (2020), 6840–6851.\\n[16] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu,\\nKalyan Sunkavalli, Trung Bui, and Hao Tan. 2023. Lrm: Large reconstruction\\nmodel for single image to 3d. arXiv preprint arXiv:2311.04400 (2023).\\n[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\\nlanguage models. arXiv preprint arXiv:2106.09685 (2021).\\n[18] Ajay Jain, Matthew Tancik, and Pieter Abbeel. 2021. Putting NeRF on a Diet:\\nSemantically Consistent Few-Shot View Synthesis. In 2021 IEEE/CVF Interna-\\ntional Conference on Computer Vision (ICCV) . 5865–5874. https://doi.org/10.1109/\\nICCV48922.2021.00583\\n[19] Wonbong Jang and Lourdes Agapito. 2023. NViST: In the Wild New View Syn-\\nthesis from a Single Image with Transformers. (2023). arXiv:2312.08568 [cs.CV]\\n[20] Hanwen Jiang, Zhenyu Jiang, Yue Zhao, and Qixing Huang. 2023. LEAP: Liberate\\nSparse-view 3D Modeling from Camera Poses. arXiv preprint arXiv:2310.01410\\n(2023).[21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 2022. Elucidating the\\nDesign Space of Diffusion-Based Generative Models. In Proc. NeurIPS .\\n[22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis.\\n2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM\\nTransactions on Graphics 42, 4 (2023).\\n[23] Mijeong Kim, Seonguk Seo, and Bohyung Han. 2022. Infonerf: Ray entropy\\nminimization for few-shot neural volume rendering. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition . 12912–12921.\\n[24] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In\\n2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,\\nCanada, April 14-16, 2014, Conference Track Proceedings , Yoshua Bengio and Yann\\nLeCun (Eds.). http://arxiv.org/abs/1312.6114\\n[25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al .\\n2023. Segment anything. arXiv preprint arXiv:2304.02643 (2023).\\n[26] A. Laurentini. 1994. The visual hull concept for silhouette-based image under-\\nstanding. IEEE Transactions on Pattern Analysis and Machine Intelligence 16, 2\\n(1994), 150–162. https://doi.org/10.1109/34.273735\\n[27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun\\nHuang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. Magic3D:\\nHigh-Resolution Text-to-3D Content Creation. In IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR) .\\n[28] Isabella Liu, Linghao Chen, Ziyang Fu, Liwen Wu, Haian Jin, Zhong Li, Chin\\nMing Ryan Wong, Yi Xu, Ravi Ramamoorthi, Zexiang Xu, and Hao Su. 2023.\\nOpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation\\non Real Objects. NeuRIPS 2023 .\\n[29] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov,\\nand Carl Vondrick. 2023. Zero-1-to-3: Zero-shot One Image to 3D Object. In\\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) .\\n9298–9309.\\n[30] Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai, and Chi-Keung\\nTang. 2023. Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-\\nObservations from Diffusion Models. arXiv preprint arXiv:2305.15171 (2023).\\n[31] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.\\nInInternational Conference on Learning Representations . https://openreview.net/\\nforum?id=Bkg6RiCqY7\\n[32] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan LI, and Jun Zhu. 2022.\\nDPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in\\nAround 10 Steps. In Advances in Neural Information Processing Systems , S. Koyejo,\\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran\\nAssociates, Inc., 5775–5787. https://proceedings.neurips.cc/paper_files/paper/\\n2022/file/260a14acce2a89dad36adc8eefe7c59e-Paper-Conference.pdf\\n[33] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and\\nStefano Ermon. 2021. Sdedit: Guided image synthesis and editing with stochastic\\ndifferential equations. arXiv preprint arXiv:2108.01073 (2021).\\n[34] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or.\\n2023. Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures. In\\n2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) .\\n12663–12673. https://doi.org/10.1109/CVPR52729.2023.01218\\n[35] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi\\nRamamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance\\nFields for View Synthesis. In ECCV (Lecture Notes in Computer Science, Vol. 12346) .\\nSpringer, 405–421. https://doi.org/10.1007/978-3-030-58452-8_24\\n[36] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi,\\nAndreas Geiger, and Noha Radwan. 2022. RegNeRF: Regularizing Neural Radiance\\nFields for View Synthesis from Sparse Inputs. In 2022 IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) . 5470–5480. https://doi.org/10.\\n1109/CVPR52688.2022.00540\\n[37] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz,\\nDan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. 2021. Hypernerf:\\nA higher-dimensional representation for topologically varying neural radiance\\nfields. arXiv preprint arXiv:2106.13228 (2021).\\n[38] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2022. Dreamfusion:\\nText-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022).\\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\\nKrueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From\\nNatural Language Supervision. In Proceedings of the 38th International Conference\\non Machine Learning . PMLR, 8748–8763. https://proceedings.mlr.press/v139/\\nradford21a.html\\n[40] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. 2021. Vision Transformers\\nfor Dense Prediction. ICCV (2021).\\n[41] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun.\\n2020. Towards robust monocular depth estimation: Mixing datasets for zero-\\nshot cross-dataset transfer. IEEE transactions on pattern analysis and machine\\nintelligence 44, 3 (2020), 1623–1637.', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 8}),\n",
       " Document(page_content=\"10 •Yang and Li, et al.\\n[42] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun.\\n2022. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-\\nShot Cross-Dataset Transfer. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence 44, 3 (2022).\\n[43] Barbara Roessle, Jonathan T Barron, Ben Mildenhall, Pratul P Srinivasan, and\\nMatthias Nießner. 2022. Dense depth priors for neural radiance fields from sparse\\ninput views. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition . 12892–12901.\\n[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn\\nOmmer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models.\\nIn2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) .\\n10674–10685. https://doi.org/10.1109/CVPR52688.2022.01042\\n[45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and\\nKfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for\\nsubject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition . 22500–22510.\\n[46] Johannes Lutz Schönberger and Jan-Michael Frahm. 2016. Structure-from-Motion\\nRevisited. In Conference on Computer Vision and Pattern Recognition (CVPR) .\\n[47] Seunghyeon Seo, Yeonjin Chang, and Nojun Kwak. 2023. Flipnerf: Flipped re-\\nflection rays for few-shot novel view synthesis. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision . 22883–22893.\\n[48] Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen\\nZhang, and Yebin Liu. 2023. Control4D: Efficient 4D Portrait Editing with Text.\\n(2023).\\n[49] Ruoxi Shi, Xinyue Wei, Cheng Wang, and Hao Su. 2023. ZeroRF: Fast Sparse\\nView 360 °Reconstruction with Zero Pretraining. CoRR abs/2312.09249 (2023).\\nhttps://doi.org/10.48550/ARXIV.2312.09249 arXiv:2312.09249\\n[50] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. 2023.\\nMVDream: Multi-view Diffusion for 3D Generation. arXiv:2308.16512 (2023).\\n[51] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.\\n2015. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In\\nProceedings of the 32nd International Conference on Machine Learning (Proceedings\\nof Machine Learning Research, Vol. 37) , Francis Bach and David Blei (Eds.). PMLR,\\nLille, France, 2256–2265. https://proceedings.mlr.press/v37/sohl-dickstein15.html\\n[52] Nagabhushan Somraj, Adithyan Karanayil, and Rajiv Soundararajan. 2023. Sim-\\npleNeRF: Regularizing Sparse Input Neural Radiance Fields with Simpler Solutions.\\nInSIGGRAPH Asia 2023 Conference Papers (SA ’23) . Association for Computing\\nMachinery, New York, NY, USA, 1–11. https://doi.org/10.1145/3610548.3618188\\n[53] Nagabhushan Somraj and Rajiv Soundararajan. 2023. ViP-NeRF: Visibility Prior\\nfor Sparse Input Neural Radiance Fields. (August 2023). https://doi.org/10.1145/\\n3588432.3591539\\n[54] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion\\nimplicit models. arXiv preprint arXiv:2010.02502 (2020).\\n[55] Jiuhn Song, Seonghoon Park, Honggyu An, Seokju Cho, Min-Seop Kwak, Sungjin\\nCho, and Seungryong Kim. 2023. DaRF: Boosting Radiance Fields from Sparse\\nInputs with Monocular Depth Adaptation. arXiv:2305.19201 (Sept. 2023). https:\\n//doi.org/10.48550/arXiv.2305.19201 arXiv:2305.19201 [cs].\\n[56] Liangchen Song, Zhong Li, Xuan Gong, Lele Chen, Zhang Chen, Yi Xu, and\\nJunsong Yuan. 2023. Harnessing Low-Frequency Neural Fields for Few-Shot View\\nSynthesis. arXiv preprint arXiv:2303.08370 (2023).\\n[57] Yang Song and Stefano Ermon. 2019. Generative Modeling by Estimating Gra-\\ndients of the Data Distribution. In Advances in Neural Information Processing\\nSystems , H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and\\nR. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/\\npaper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf\\n[58] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano\\nErmon, and Ben Poole. 2021. Score-Based Generative Modeling through Stochastic\\nDifferential Equations. In International Conference on Learning Representations .\\nhttps://openreview.net/forum?id=PxTIG12RRHS\\n[59] Cheng Sun, Min Sun, and Hwann-Tzong Chen. 2022. Direct Voxel Grid Optimiza-\\ntion: Super-fast Convergence for Radiance Fields Reconstruction. In CVPR .\\n[60] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. 2023. Dream-\\nGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation. arXiv\\npreprint arXiv:2309.16653 (2023).\\n[61] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. 2023.\\nSparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthe-\\nsis. arXiv:2303.16196 (Aug. 2023). https://doi.org/10.48550/arXiv.2303.16196\\narXiv:2303.16196 [cs].\\n[62] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich.\\n2023. Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Gen-\\neration. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR) . 12619–12629. https://doi.org/10.1109/CVPR52729.2023.01214\\n[63] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping\\nWang, Zexiang Xu, and Kai Zhang. 2023. PF-LRM: Pose-Free Large Reconstruction\\nModel for Joint Pose and Shape Prediction. arXiv preprint arXiv:2311.12024 (2023).[64] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun\\nZhu. 2023. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation\\nwith Variational Score Distillation. In Advances in Neural Information Processing\\nSystems (NeurIPS) .\\n[65] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. 2020. SynSin:\\nEnd-to-end View Synthesis from a Single Image. In CVPR .\\n[66] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel\\nWatson, Pratul P Srinivasan, Dor Verbin, Jonathan T Barron, Ben Poole, et al .\\n2023. ReconFusion: 3D Reconstruction with Diffusion Priors. arXiv preprint\\narXiv:2312.02981 (2023).\\n[67] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu,\\nLei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, and Ziwei Liu. 2023. OmniObject3D:\\nLarge-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and\\nGeneration. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR) (2023), 803–814. https://api.semanticscholar.org/CorpusID:255998491\\n[68] Jamie Wynn and Daniyar Turmukhambetov. 2023. DiffusioNeRF: Regularizing\\nNeural Radiance Fields with Denoising Diffusion Models. In 2023 IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition (CVPR) . 4180–4189. https:\\n//doi.org/10.1109/CVPR52729.2023.00407\\n[69] Haolin Xiong, Sairisheek Muttukuru, Rishi Upadhyay, Pradyumna Chari, and\\nAchuta Kadambi. 2023. SparseGS: Real-Time 360◦Sparse View Synthesis using\\nGaussian Splatting. arXiv:2312.00206 (Nov. 2023). https://doi.org/10.48550/arXiv.\\n2312.00206\\n[70] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang\\nWang. 2022. SinNeRF: Training Neural Radiance Fields on Complex Scenes\\nfrom a Single Image. In Computer Vision – ECCV 2022 (Lecture Notes in Com-\\nputer Science) , Shai Avidan, Gabriel Brostow, Moustapha Cissé, Giovanni Maria\\nFarinella, and Tal Hassner (Eds.). Springer Nature Switzerland, Cham, 736–753.\\nhttps://doi.org/10.1007/978-3-031-20047-2_42\\n[71] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli,\\nand Ulrich Neumann. 2022. Point-nerf: Point-based neural radiance fields. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\\n5438–5448.\\n[72] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan\\nShi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang. 2023.\\nDMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model.\\narXiv:2311.09217 [cs.CV]\\n[73] Jiawei Yang, Marco Pavone, and Yue Wang. 2023. FreeNeRF: Improving Few-\\nShot Neural Rendering with Free Frequency Regularization. In 2023 IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR) . 8254–8263. https:\\n//doi.org/10.1109/CVPR52729.2023.00798\\n[74] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang,\\nWenyu Liu, Qi Tian, and Xinggang Wang. 2023. GaussianDreamer: Fast Genera-\\ntion from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models. arXiv\\npreprint arXiv:2310.08529 (2023).\\n[75] Wang Yifan, Felice Serena, Shihao Wu, Cengiz Öztireli, and Olga Sorkine-Hornung.\\n2019. Differentiable Surface Splatting for Point-based Geometry Processing. ACM\\nTransactions on Graphics (proceedings of ACM SIGGRAPH ASIA) 38, 6 (2019).\\n[76] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control\\nto text-to-image diffusion models. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision . 3836–3847.\\n[77] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. ControlNet-v1-1-nightly.\\nhttps://github.com/lllyasviel/ControlNet-v1-1-nightly.\\n[78] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. 2022.\\nDifferentiable Point-Based Radiance Fields for Efficient View Synthesis. In SIG-\\nGRAPH Asia 2022 Conference Papers (SA ’22) . Association for Computing Machin-\\nery, New York, NY, USA, Article 7, 12 pages.\\n[79] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.\\n2018. The unreasonable effectiveness of deep features as a perceptual metric.\\nInProceedings of the IEEE conference on computer vision and pattern recognition .\\n586–595.\\n[80] Zhizhuo Zhou and Shubham Tulsiani. 2023. SparseFusion: Distilling View-\\nConditioned Diffusion for 3D Reconstruction. In 2023 IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) . 12588–12597. https://doi.org/\\n10.1109/CVPR52729.2023.01211\\n[81] Junzhe Zhu and Peiye Zhuang. 2023. HiFA: High-fidelity Text-to-3D Generation\\nwith Advanced Diffusion Guidance. arXiv:2305.18766 [cs.CV]\\n[82] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. 2023. FSGS:\\nReal-Time Few-shot View Synthesis using Gaussian Splatting. arXiv preprint\\narXiv:2312.00451 (2023).\\n[83] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao,\\nand Song-Hai Zhang. 2023. Triplane Meets Gaussian Splatting: Fast and Generaliz-\\nable Single-View 3D Reconstruction with Transformers. arXiv:2312.09147 [cs.CV]\", metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 9}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting •11\\nA APPENDIX\\nA.1 Dataset Details\\nMipNeRF360. The sparse MipNeRF360 dataset, derived from the\\ndataset provided by [ 1], focuses on three scenes containing a pri-\\nmary object: bonsai, garden, and kitchen. We employ [ 3] to extract\\nthe plastic bonsai, the flower pot and the Lego bulldozer from the\\noriginal scenes. For performance evaluation, the scenes are tested\\nusing images downsampled by a factor of 4 ×, following the train-test\\nsplits from [66].\\nOmniObject3D. OmniObject3D includes 6k real 3D objects in\\n190 large-vocabulary categories. We selected 17 objects from Om-\\nniObject3D [ 67] and generated their masks using [ 3]. The items\\nchosen include: back-pack_016 ,box_043 ,broccoli_003 ,corn_007 ,di-\\nnosaur_006 ,flower_pot_007 ,gloves_009 ,guitar_002 ,hamburger_012 ,\\npicnic_basket_009 ,pineapple_013 ,sandwich_003 ,suitcase_006 ,timer_010 ,\\ntoy_plane_005 ,toy_truck_037 , and vase_012 for a fair evaluation. We\\nmanually choose camera views for training, and use every eighth\\nview left for testing. Most scenes are originally in 1080p resolution,\\nwhile gloves_009 andtimer_010 are in 720p resolution. To maintain\\nconsistency across all scenes, we upscaled the images from these\\ntwo scenes to 1080p resolution. All the images are downsampled to\\na factor of 2×.\\nOpenIllumination. OpenIllumination is a real-world dataset cap-\\ntured by the LightStage. We use the sparse OpenIllumination dataset\\nproposed in ZeroRF [ 49]. Note that ZeroRF re-scaled and re-centered\\nthe camera poses to align the target object with the world center.\\nWe test on the sparse OpenIllumination dataset [ 28] with provided\\nobject masks and train-test splits, downscaling the images by a\\nfactor of 4×, following the same experimental setting as in [49].\\nA.2 Implement Details\\nWe build our GaussianObject upon 3DGS [ 22] and Threestudio [ 13].\\nFor visual hull reconstruction, we adopt a coarse-to-fine approach:\\nstarting with rough spatial sampling to estimate the bounding box,\\nthen proceeding to detailed sampling within it. Since the sparse\\ninput views cannot provide sufficient multi-view consistency, we\\nreconstruct all objects with only two degrees of spherical harmonics\\n(SH). As for floater elimination, we start it from 500 iterations and\\nconduct it every 500 iterations until 6k iterations. The adaptive\\nthreshold𝜏is initially set to the mean plus the standard deviation of\\nthe average distance calculated via KNN, and is linearly decreased\\nto 0 in 6k iterations. The loss weight for Lgsis set to:𝜆SSIM=0.2,\\n𝜆m=0.001andLd=0.0005.\\nFor the Gaussian repair model setup, we use leave-one-out train-\\ning to generate image pairs and 3D Noise 𝜖𝑠. We use the visual\\nhull corresponding to 𝑁reference images as initialization of all\\n3DGS models during leave-one-out training. We add noises to all\\nthe attributes of 3D Gaussians except for SH coefficients. Every time\\nwe need a data pair from adding 3D noises, we use the mean 𝜇Δ\\nand variance 𝜎Δto generate new noisy Gaussians for rendering,\\nthus enabling sufficient data generation. All the images are constant\\npadded or center croped before fed into the Gaussian repair model to\\npreserve the real ratio of target object. At the first iteration of train-\\ning the Gaussian repair model, manual noise generated according toa distribution is used for training with a 100% probability. Each time\\ntraining is conducted with manual noise, this probability is reduced\\nby0.5%, to increasingly utilize cached images from leave-one-out\\ntraining. The [V] used for object-specific text prompt is “xxy5syt00”.\\nFor the Gaussian repair process, we optimize the coarse 3DGS\\nmodel for 4k steps, with first 2800 steps supervised by Lrepand\\nLgstogether. Based on the camera poses of 𝑁reference views, we\\nestimate an elliptic trajectory that encircles the object, and find\\n𝑁points on the trajectory that are respectively the closest to the\\nreference views. These 𝑁points divide the ellipse into 𝑁segments\\nof arc. On each segment of the arc, novel views are only sampled\\nfrom the middle 80% of the arc (repair path), the remaining arcs\\nconstitute reference paths. To expedite training, we avoid using the\\ntime-consuming DDIM process [ 54] for every novel view rendering.\\nInstead, we employ cached images for optimization. Specifically, at\\nthe beginning of every 200 iterations, we sample and repair two\\nnovel views from each repair path. Throughout these 200 iterations,\\nwe utilize these images to repair 3D Gaussians. We set the weights\\nforLrepas follows: 𝜆1=0.5,𝜆2=0.5and𝜆𝑝=2.0accross all\\nexperiments. We employ densification and opacity resetting to reg-\\nulate the number of Gaussians. The Gaussian densification process\\nis effective from 400 to 3600 steps. The Gaussian model undergoes\\ndensification and pruning at intervals of every 100 steps, and its\\nopacity is reset every 500 steps.\\nA.3 Experiment Details\\nFSGS [ 82] requires SfM points generated from the input images,\\nwhich are too sparse in our setting of 4 input views. Alternatively,\\nwe randomly pick 𝑁pickpoints from the SfM points with full images\\nas input:\\n𝑁pick=max\\x12\\n150,𝑘sparse\\n𝑘all𝑁all\\x13\\n, (13)\\nwhere𝑘sparse is the number of input images, 𝑘allis the toatl number\\nof images, and 𝑁allis the total number of points.\\nA.4 More Experimental Results\\nWe provide per-scene qualitative metrics of MipNeRF360 in Tab. 5,\\nTab. 6 and Tab. 7; of OmniObject3D in Tab. 8, Tab. 9, Tab. 10, Tab. 11,\\nTab. 12 and Tab. 13; of OpenIllumination in Tab. 14 and Tab. 15.\\nAdditional qualitative comparisons on OpenIllumination dataset are\\nshown in Fig. 10.\\nFurthermore, We provide extra validations on our Gaussian re-\\npair process. Figure 9 illustrates both the distribution of coarse 3D\\nGaussians and the refined distribution after the Gaussian repair.\\nThis comparison clearly demonstrates the significant enhancement\\nin geometry achieved by our repair process. We also provide quali-\\ntative samples of the Gaussian repair model in Fig. 11, showing that\\nthe Gaussian repair model can effectively generate high-quality and\\nconsistent images from multiple views.\\nA.5 Abaltion Study Details\\nWe employ ControlNet-Tile [ 76,77] as the Gaussian repair model of\\nGaussianObject. ControlNet-Tile, based on Stable Diffusion v1.5, is\\ndesigned to repair low-resolution or low-fidelity images by taking\\nthem as conditions. In our ablation study, we use other diffusion', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 10}),\n",
       " Document(page_content='12 •Yang and Li, et al.\\nCoarse Gaussian Model A昀琀er Gaussian Repair\\nFig. 9. Point clouds of GaussianObject before and after Gaussian repair. In the dinosaur scene, the Gaussian repair model significantly enhances the point\\ncloud density, resulting in a more distinct and clear representation of the dinosaur’s body. In the vase scene, noticeable artifacts around the handle and the\\nmouth are effectively eliminated by the Gaussian repair model, and the incomplete bottom of the vase is repaired. These key areas are highlighted with blue\\nboxes.\\nmodels serving as our Gaussian repair model in Sec.4.4. First, we use\\nStable Diffusion XL Image-to-Image [ 33], which cannot directly take\\nimages as input conditions. Therefore, we add 50%Gaussian noise\\nto the novel view images and use the model to generate high-fidelity\\nimages by denoising the noisy images. Additionally, we evaluate\\nControlNet-ZoeDepth, based on Stable Diffusion XL. ControlNet-\\nZoeDepth, which takes the ZoeDepth [2] as a condition, is used to\\ngenerate images aligned with the depth map. However, ZoeDepth\\nmay incorrectly assume that a single object with a white backgroundis on a white table, leading to erroneous depth map predictions. In\\nsuch cases, we test two settings: one using the entire predicted\\ndepth map and another using the object-masked depth map. Due to\\nthe ambiguity of monocular predicted depth, we add partial noise\\non the input image to preserve the image information, similar to\\nSDEdit. Specifically, we add 50%Gaussian noise to the novel view\\nimages and set the weight scale of the ControlNet model to 0.55.\\nIn this experimental setup, we obtained the results of the ablation\\nexperiments on the structure of the Gaussian repair model.', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 11}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting •13\\nGT Ours ZeroRF SparseNeRF FreeNeRF RegNeRF DietNeRF 3DGS DVGO\\nFig. 10. Qualitative examples on the OpenIllumination dataset with four input views.', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 12}),\n",
       " Document(page_content='14 •Yang and Li, et al.\\nInput Views Gaussian Repair Model Results\\nFig. 11. Qualitative samples of the Gaussian repaired models on several scenes from different views.', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 13}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting •15\\nTable 5. Comparisons of per-scene metrics of MipNeRF360 with 4 input views.\\nMethodbonsai garden kitchen\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.3324 10.45 0.6980 0.0559 21.75 0.9652 0.3447 10.97 0.7104\\n3DGS [22] 0.1408 16.42 0.8458 0.0417 26.05 0.9769 0.1414 18.47 0.8746\\nDietNeRF [18] 0.1333 16.30 0.8682 0.0231 28.63 0.9780 0.1787 11.77 0.8453\\nRegNeRF [36] 0.2736 9.99 0.7847 0.0300 20.89 0.9794 0.3094 9.89 0.7788\\nFreeNeRF [73] 0.2172 10.16 0.8052 0.0300 20.89 0.9760 0.2578 10.08 0.7789\\nSparseNeRF [61] 0.2148 10.08 0.8037 0.0618 18.36 0.9556 0.2562 10.04 0.7769\\nZeroRF [49] 0.2206 10.36 0.7810 0.0434 22.52 0.9596 0.3324 9.62 0.7157\\nFSGS [82] 0.1258 17.96 0.8707 0.0279 25.84 0.9766 0.1317 19.40 0.8818\\nGaussianObject 0.0690 21.51 0.9113 0.0121 30.56 0.9833 0.0687 22.36 0.9104\\nTable 6. Comparisons of per-scene metrics of MipNeRF360 with 6 input views.\\nMethodbonsai garden kitchen\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.3908 10.11 0.6531 0.0416 21.90 0.9728 0.3677 10.88 0.6769\\n3DGS [22] 0.1133 18.42 0.8736 0.0330 27.54 0.9809 0.1051 20.40 0.8858\\nDietNeRF [18] 0.0863 22.39 0.9085 0.0274 20.89 0.9760 0.0951 22.80 0.9012\\nRegNeRF [36] 0.2736 9.34 0.7736 0.0300 20.88 0.9794 0.3180 10.00 0.7726\\nFreeNeRF [73] 0.0904 22.09 0.9083 0.0300 20.89 0.9760 0.0847 23.78 0.9153\\nSparseNeRF [61] 0.2530 9.45 0.7695 0.0395 20.82 0.9738 0.2997 10.01 0.7516\\nZeroRF [49] 0.1038 20.86 0.8992 0.0190 31.77 0.9829 0.1264 19.80 0.8813\\nFSGS [82] 0.1100 19.69 0.8902 0.0274 25.48 0.9778 0.0932 22.86 0.9112\\nGaussianObject 0.0499 23.53 0.9313 0.0104 31.97 0.9864 0.0487 25.50 0.9358\\nTable 7. Comparisons of per-scene metrics of MipNeRF360 with 9 input views.\\nMethodbonsai garden kitchen\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.3658 11.17 0.6938 0.0367 22.09 0.9747 0.3672 10.95 0.6843\\n3DGS [22] 0.0932 20.68 0.9004 0.0222 29.46 0.9839 0.0771 22.74 0.9149\\nDietNeRF [18] 0.0706 24.45 0.9263 0.0274 20.89 0.9760 0.0774 25.30 0.9247\\nRegNeRF [36] 0.2875 9.45 0.7706 0.0300 20.89 0.9794 0.2735 10.70 0.8050\\nFreeNeRF [73] 0.0788 23.99 0.9263 0.0164 33.05 0.9859 0.0702 25.96 0.9335\\nSparseNeRF [61] 0.2857 9.38 0.7447 0.0298 23.04 0.9776 0.3313 10.67 0.7481\\nZeroRF [49] 0.0657 24.72 0.9312 0.0174 32.75 0.9841 0.0770 25.88 0.9227\\nFSGS [82] 0.0816 22.72 0.9164 0.0198 29.28 0.9815 0.0804 23.91 0.9213\\nGaussianObject 0.0382 25.65 0.9502 0.0089 33.09 0.9886 0.0353 27.11 0.9526', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 14}),\n",
       " Document(page_content='16 •Yang and Li, et al.\\nTable 8. Comparisons of per-scene metrics of OmniObject3D with 4 input views.\\nMethodbackpack_016 box_043 broccoli_003\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.2674 11.41 0.7794 0.0814 20.21 0.9455 0.0953 16.14 0.9229\\n3DGS [22] 0.1542 15.13 0.8505 0.0536 18.57 0.9665 0.0677 15.68 0.9458\\nDietNeRF [18] 0.2903 11.30 0.8183 0.1866 15.54 0.8867 0.1028 14.65 0.9172\\nRegNeRF [36] 0.2987 9.56 0.8022 0.1298 16.40 0.9455 0.1109 14.08 0.9239\\nFreeNeRF [73] 0.1358 11.57 0.8899 0.0775 17.59 0.9516 0.0570 16.36 0.9559\\nSparseNeRF [61] 0.2570 9.78 0.7985 0.0960 16.52 0.9437 0.1068 14.18 0.9208\\nZeroRF [49] 0.0528 25.13 0.9459 0.0211 33.94 0.9827 0.0228 30.32 0.9745\\nFSGS [82] 0.1257 19.48 0.9089 0.0835 23.70 0.9590 0.0399 23.25 0.9670\\nGaussianObject 0.0425 25.61 0.9511 0.0140 33.22 0.9833 0.0138 30.59 0.9781\\nMethodcorn_007 dinosaur_006 flower_pot_007\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.1135 21.81 0.9279 0.1924 16.08 0.8539 0.1281 15.04 0.8951\\n3DGS [22] 0.0826 18.91 0.9496 0.1124 18.31 0.8952 0.0741 14.79 0.9330\\nDietNeRF [18] 0.0244 33.80 0.9729 0.1168 16.07 0.9161 0.0482 16.22 0.9539\\nRegNeRF [36] 0.1869 16.65 0.9200 0.2371 14.60 0.8838 0.1499 13.35 0.9065\\nFreeNeRF [73] 0.0732 24.35 0.9517 0.1071 16.01 0.9197 0.0962 15.45 0.9259\\nSparseNeRF [61] 0.1678 16.77 0.9041 0.1781 15.22 0.8881 0.1379 14.20 0.9139\\nZeroRF [49] 0.0343 33.68 0.9694 0.0460 26.59 0.9545 0.0235 30.77 0.9759\\nFSGS [82] 0.0650 26.37 0.9577 0.0969 21.19 0.9266 0.0363 26.88 0.9659\\nGaussianObject 0.0166 34.44 0.9781 0.0349 27.68 0.9607 0.0156 31.22 0.9795\\nMethodgloves_009 guitar_002 hamburger_012\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.2285 12.45 0.8229 0.1178 15.46 0.9102 0.1510 19.00 0.9028\\n3DGS [22] 0.1303 13.04 0.8945 0.1130 12.74 0.8995 0.0954 16.24 0.9274\\nDietNeRF [18] 0.1222 14.08 0.9140 0.0917 24.20 0.9342 0.2062 15.46 0.8675\\nRegNeRF [36] 0.1590 12.90 0.8989 0.1874 12.52 0.8660 0.1654 15.60 0.9154\\nFreeNeRF [73] 0.1458 13.50 0.8951 0.1004 13.44 0.9206 0.0790 17.33 0.9442\\nSparseNeRF [61] 0.1440 13.03 0.8912 0.3535 10.53 0.7153 0.1716 15.68 0.8914\\nZeroRF [49] 0.1578 14.61 0.8793 0.0785 20.87 0.9373 0.0355 29.33 0.9634\\nFSGS [82] 0.1427 16.80 0.9140 0.0416 27.92 0.9659 0.0747 22.20 0.9474\\nGaussianObject 0.0297 26.94 0.9695 0.0141 32.37 0.9816 0.0224 31.46 0.9694', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 15}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting •17\\nTable 9. Comparisons of per-scene metrics of OmniObject3D with 4 input views. continued\\nMethodpicnic_basket_009 pineapple_013 sandwich_003 suitcase_006\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.1540 16.08 0.8887 0.1677 14.39 0.8747 0.0423 25.15 0.9787 0.1865 14.52 0.8697\\n3DGS [22] 0.0837 19.60 0.9167 0.0573 16.74 0.9484 0.0285 22.44 0.9850 0.1028 15.18 0.9331\\nDietNeRF [18] 0.1512 15.02 0.8940 0.0933 16.48 0.9336 0.0253 22.52 0.9851 0.0542 26.60 0.9639\\nRegNeRF [36] 0.1313 14.73 0.9303 0.1431 14.01 0.9005 0.0293 22.52 0.9875 0.2636 12.03 0.8587\\nFreeNeRF [73] 0.0772 15.82 0.9433 0.0638 16.74 0.9471 0.0293 22.52 0.9850 0.0755 25.66 0.9422\\nSparseNeRF [61] 0.3376 14.99 0.8717 0.1918 16.01 0.9113 0.0478 20.47 0.9750 0.2487 12.55 0.8384\\nZeroRF [49] 0.0391 28.33 0.9648 0.0569 22.81 0.9413 0.0171 30.44 0.9886 0.0520 23.83 0.9611\\nFSGS [82] 0.0417 25.60 0.9612 0.0608 20.87 0.9447 0.0110 33.34 0.9900 0.0682 23.70 0.9528\\nGaussianObject 0.0220 31.44 0.9733 0.0208 28.16 0.9676 0.0072 34.34 0.9919 0.0259 28.69 0.9758\\nMethodtimer_010 toy_plane_005 toy_truck_037 vase_012\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.1104 18.90 0.9345 0.0929 22.37 0.9423 0.1415 16.34 0.9075 0.1912 16.01 0.8612\\n3DGS [22] 0.0940 17.41 0.9407 0.0470 22.94 0.9672 0.0567 18.26 0.9583 0.1086 17.96 0.8968\\nDietNeRF [18] 0.1411 17.30 0.9143 0.0321 24.12 0.9752 0.1059 17.73 0.9382 0.1856 14.41 0.8637\\nRegNeRF [36] 0.1557 16.79 0.9310 0.0354 24.12 0.9810 0.1531 15.37 0.9240 0.3109 13.18 0.8799\\nFreeNeRF [73] 0.0616 18.80 0.9664 0.0354 24.12 0.9752 0.0612 18.28 0.9562 0.1309 14.74 0.9130\\nSparseNeRF [61] 0.1742 17.37 0.9317 0.0655 22.32 0.9603 0.1257 15.55 0.9161 0.1664 13.63 0.8939\\nZeroRF [49] 0.0258 32.97 0.9848 0.0228 30.52 0.9827 0.0251 30.68 0.9758 0.0440 27.51 0.9634\\nFSGS [82] 0.0362 29.15 0.9782 0.0256 29.93 0.9798 0.0517 26.15 0.9588 0.0607 23.53 0.9490\\nGaussianObject 0.0152 33.43 0.9867 0.0102 34.30 0.9884 0.0137 32.23 0.9820 0.0327 29.10 0.9676', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 16}),\n",
       " Document(page_content='18 •Yang and Li, et al.\\nTable 10. Comparisons of per-scene metrics of OmniObject3D with 6 input views.\\nMethodbackpack_016 box_043 broccoli_003\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.2363 12.56 0.8040 0.1028 19.74 0.9412 0.0853 18.16 0.9385\\n3DGS [22] 0.0901 21.26 0.9122 0.0582 18.16 0.9607 0.0483 16.77 0.9621\\nDietNeRF [18] 0.2117 11.25 0.8546 0.1111 17.25 0.9325 0.0994 15.03 0.9267\\nRegNeRF [36] 0.2818 10.08 0.8291 0.1251 15.85 0.9420 0.0483 16.84 0.9663\\nFreeNeRF [73] 0.2255 12.30 0.8181 0.0530 18.68 0.9656 0.0483 16.84 0.9625\\nSparseNeRF [61] 0.4828 11.35 0.7691 0.1030 18.16 0.9602 0.0483 16.84 0.9625\\nZeroRF [49] 0.0494 27.87 0.9529 0.0195 36.07 0.9843 0.0212 32.54 0.9768\\nFSGS [82] 0.0815 22.34 0.9308 0.0266 32.41 0.9799 0.1163 20.96 0.9473\\nGaussianObject 0.0318 27.60 0.9593 0.0121 35.15 0.9868 0.0108 32.78 0.9842\\nMethodcorn_007 dinosaur_006 flower_pot_007\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.1056 22.43 0.9540 0.1456 17.91 0.8879 0.1348 15.36 0.8890\\n3DGS [22] 0.0796 19.22 0.9551 0.1030 19.04 0.8968 0.0675 15.43 0.9392\\nDietNeRF [18] 0.0225 35.23 0.9775 0.1905 14.69 0.8592 0.0482 16.22 0.9539\\nRegNeRF [36] 0.1444 17.56 0.9413 0.2226 14.38 0.8905 0.1323 13.50 0.9122\\nFreeNeRF [73] 0.0794 19.44 0.9523 0.0970 16.26 0.9275 0.0525 16.22 0.9539\\nSparseNeRF [61] 0.0793 19.42 0.9523 0.3566 15.12 0.8636 0.1861 12.72 0.8600\\nZeroRF [49] 0.0309 34.98 0.9726 0.0409 29.03 0.9633 0.0217 32.83 0.9791\\nFSGS [82] 0.0511 29.35 0.9634 0.0503 26.38 0.9577 0.0386 27.17 0.9658\\nGaussianObject 0.0135 35.77 0.9821 0.0256 29.65 0.9708 0.0107 34.40 0.9866\\nMethodgloves_009 guitar_002 hamburger_012\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.1373 15.00 0.9011 0.0803 17.61 0.9368 0.1697 18.75 0.9068\\n3DGS [22] 0.1284 13.42 0.8967 0.1176 12.50 0.8944 0.0949 16.48 0.9278\\nDietNeRF [18] 0.1647 13.92 0.8797 0.0401 28.63 0.9650 0.1563 16.14 0.9029\\nRegNeRF [36] 0.1919 13.12 0.8849 0.1228 14.95 0.9105 0.2122 14.52 0.9015\\nFreeNeRF [73] 0.0905 14.39 0.9331 0.0272 33.19 0.9767 0.0790 17.33 0.9442\\nSparseNeRF [61] 0.1672 13.14 0.8744 0.3992 10.46 0.7581 0.5058 13.82 0.8368\\nZeroRF [49] 0.0352 30.52 0.9728 0.0669 23.41 0.9423 0.0292 33.60 0.9710\\nFSGS [82] 0.0930 20.34 0.9333 0.0509 26.17 0.9627 0.0512 27.72 0.9568\\nGaussianObject 0.0214 30.75 0.9788 0.0116 34.24 0.9861 0.0152 34.29 0.9801', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 17}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting •19\\nTable 11. Comparisons of per-scene metrics of OmniObject3D with 6 input views. continued\\nMethodpicnic_basket_009 pineapple_013 sandwich_003 suitcase_006\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.1371 17.15 0.9021 0.1448 17.23 0.8945 0.0793 25.23 0.9663 0.1489 15.70 0.9050\\n3DGS [22] 0.0490 26.20 0.9494 0.0682 16.16 0.9388 0.0374 21.32 0.9784 0.1026 15.19 0.9331\\nDietNeRF [18] 0.1264 15.24 0.9123 0.0564 16.74 0.9472 0.0253 22.52 0.9851 0.0373 29.00 0.9750\\nRegNeRF [36] 0.0772 15.82 0.9524 0.1467 14.15 0.8969 0.0293 22.52 0.9875 0.1351 15.80 0.9186\\nFreeNeRF [73] 0.0772 15.82 0.9433 0.0638 16.74 0.9471 0.0293 22.52 0.9850 0.0682 26.99 0.9469\\nSparseNeRF [61] 0.3520 15.91 0.9024 0.1997 15.63 0.9061 0.0591 20.90 0.9734 0.2311 15.50 0.8714\\nZeroRF [49] 0.0337 31.11 0.9698 0.0257 29.64 0.9671 0.0058 38.85 0.9958 0.0513 26.33 0.9676\\nFSGS [82] 0.0283 31.43 0.9742 0.0299 28.42 0.9697 0.0079 37.49 0.9938 0.1395 18.73 0.9197\\nGaussianObject 0.0185 33.30 0.9779 0.0146 31.03 0.9797 0.0041 38.29 0.9954 0.0192 30.54 0.9826\\nMethodtimer_010 toy_plane_005 toy_truck_037 vase_012\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.1335 19.63 0.9395 0.0919 21.98 0.9451 0.1244 18.42 0.9176 0.1343 18.49 0.9125\\n3DGS [22] 0.0751 18.15 0.9556 0.0559 22.24 0.9639 0.0583 18.23 0.9581 0.0822 21.16 0.9204\\nDietNeRF [18] 0.1221 16.94 0.9225 0.0321 24.12 0.9752 0.0988 17.53 0.9335 0.2242 13.73 0.8517\\nRegNeRF [36] 0.1444 16.39 0.9410 0.0354 24.12 0.9810 0.1553 15.88 0.9187 0.2404 13.07 0.8783\\nFreeNeRF [73] 0.0616 18.80 0.9664 0.0354 24.12 0.9752 0.0612 18.28 0.9562 0.0952 15.51 0.9354\\nSparseNeRF [61] 0.1266 17.09 0.9349 0.0569 23.93 0.9688 0.1290 15.97 0.9122 0.2075 13.66 0.8827\\nZeroRF [49] 0.0246 34.57 0.9860 0.0128 36.87 0.9896 0.0218 33.79 0.9801 0.0377 31.05 0.9716\\nFSGS [82] 0.0582 25.42 0.9655 0.0207 31.97 0.9838 0.1065 19.30 0.9370 0.0782 22.55 0.9478\\nGaussianObject 0.0139 34.73 0.9885 0.0075 36.59 0.9918 0.0080 35.40 0.9885 0.0253 31.69 0.9763', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 18}),\n",
       " Document(page_content='20 •Yang and Li, et al.\\nTable 12. Comparisons of per-scene metrics of OmniObject3D with 9 input views.\\nMethodbackpack_016 box_043 broccoli_003\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.2299 12.43 0.8179 0.0664 22.20 0.9677 0.0683 21.03 0.9618\\n3DGS [22] 0.0519 26.80 0.9494 0.0506 18.65 0.9669 0.0531 16.45 0.9552\\nDietNeRF [18] 0.1806 11.48 0.8684 0.0481 18.68 0.9656 0.0995 14.19 0.9211\\nRegNeRF [36] 0.2967 10.46 0.8276 0.1051 17.39 0.9490 0.0483 16.84 0.9663\\nFreeNeRF [73] 0.2422 12.39 0.8084 0.0530 18.68 0.9656 0.0483 16.84 0.9625\\nSparseNeRF [61] 0.4932 10.46 0.7481 0.0615 18.59 0.9639 0.2476 13.34 0.8900\\nZeroRF [49] 0.0459 30.15 0.9582 0.0184 37.80 0.9857 0.0216 33.42 0.9775\\nFSGS [82] 0.1250 14.84 0.8963 0.0226 33.72 0.9858 0.0169 33.03 0.9817\\nGaussianObject 0.0229 31.41 0.9737 0.0089 37.98 0.9910 0.0085 34.91 0.9882\\nMethodcorn_007 dinosaur_006 flower_pot_007\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.0921 22.05 0.9598 0.0985 19.56 0.9269 0.0987 19.15 0.9319\\n3DGS [22] 0.0801 19.11 0.9510 0.0620 23.57 0.9377 0.0622 15.65 0.9438\\nDietNeRF [18] 0.0184 36.58 0.9824 0.2514 14.57 0.8359 0.0482 16.22 0.9539\\nRegNeRF [36] 0.0870 20.45 0.9546 0.1914 14.51 0.8951 0.0525 16.22 0.9598\\nFreeNeRF [73] 0.0221 36.42 0.9779 0.0970 16.26 0.9275 0.0525 16.22 0.9539\\nSparseNeRF [61] 0.2863 18.94 0.9340 0.4706 13.86 0.8191 0.1816 13.70 0.8855\\nZeroRF [49] 0.0352 35.69 0.9715 0.0393 30.28 0.9679 0.0199 34.60 0.9816\\nFSGS [82] 0.0280 34.01 0.9792 0.0533 27.19 0.9593 0.0295 30.55 0.9750\\nGaussianObject 0.0089 38.33 0.9879 0.0201 31.61 0.9780 0.0087 36.13 0.9896\\nMethodgloves_009 guitar_002 hamburger_012\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.0879 17.36 0.9425 0.0481 21.24 0.9614 0.0896 23.11 0.9592\\n3DGS [22] 0.1239 13.78 0.9039 0.1060 12.90 0.9060 0.0937 16.56 0.9282\\nDietNeRF [18] 0.1353 13.77 0.8923 0.0318 32.15 0.9730 0.1504 16.01 0.9007\\nRegNeRF [36] 0.0905 14.39 0.9428 0.0928 16.97 0.9268 0.0790 17.33 0.9567\\nFreeNeRF [73] 0.1393 13.64 0.8944 0.0236 35.56 0.9818 0.0790 17.33 0.9442\\nSparseNeRF [61] 0.1584 13.29 0.8768 0.2553 19.14 0.8907 0.4415 15.47 0.8795\\nZeroRF [49] 0.0349 31.94 0.9753 0.0768 20.01 0.9370 0.0314 34.14 0.9713\\nFSGS [82] 0.0817 21.46 0.9422 0.0436 28.66 0.9693 0.0259 33.34 0.9781\\nGaussianObject 0.0180 32.89 0.9838 0.0100 35.55 0.9888 0.0103 37.09 0.9875', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 19}),\n",
       " Document(page_content='GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting •21\\nTable 13. Comparisons of per-scene metrics of OmniObject3D with 9 input views. continued\\nMethodpicnic_basket_009 pineapple_013 sandwich_003 suitcase_006\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.1501 16.50 0.9012 0.0927 18.63 0.9393 0.0509 25.44 0.9818 0.0965 17.72 0.9430\\n3DGS [22] 0.0232 32.98 0.9768 0.0573 16.73 0.9483 0.0316 21.91 0.9825 0.1026 15.19 0.9331\\nDietNeRF [18] 0.1416 13.74 0.8889 0.1247 14.14 0.8914 0.0253 22.52 0.9851 0.0325 30.99 0.9799\\nRegNeRF [36] 0.1746 13.91 0.8965 0.0638 16.74 0.9534 0.0293 22.52 0.9875 0.0317 32.63 0.9903\\nFreeNeRF [73] 0.0772 15.82 0.9433 0.0638 16.74 0.9471 0.0293 22.52 0.9850 0.0510 30.81 0.9699\\nSparseNeRF [61] 0.4269 14.14 0.8208 0.4313 14.55 0.8322 0.0448 22.50 0.9843 0.0438 31.75 0.9803\\nZeroRF [49] 0.0320 33.12 0.9735 0.0254 30.79 0.9692 0.0057 39.80 0.9961 0.0463 28.34 0.9722\\nFSGS [82] 0.0393 28.61 0.9712 0.0186 31.83 0.9807 0.0076 35.88 0.9945 0.0452 29.53 0.9746\\nGaussianObject 0.0139 35.27 0.9836 0.0109 33.00 0.9858 0.0029 41.15 0.9971 0.0143 32.99 0.9876\\nMethodtimer_010 toy_plane_005 toy_truck_037 vase_012\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.1811 17.27 0.9222 0.2574 16.89 0.8464 0.1297 17.93 0.9270 0.1154 18.89 0.9242\\n3DGS [22] 0.0450 28.44 0.9669 0.0499 22.88 0.9650 0.0557 18.27 0.9584 0.0566 24.65 0.9486\\nDietNeRF [18] 0.1010 17.56 0.9359 0.0321 24.12 0.9752 0.1074 17.25 0.9337 0.2255 13.52 0.8559\\nRegNeRF [36] 0.0616 18.80 0.9725 0.0354 24.12 0.9810 0.0612 18.28 0.9647 0.2283 13.26 0.8901\\nFreeNeRF [73] 0.0616 18.80 0.9664 0.0354 24.12 0.9752 0.0612 18.28 0.9562 0.0952 15.51 0.9354\\nSparseNeRF [61] 0.1180 17.47 0.9353 0.0354 24.10 0.9752 0.1425 15.93 0.9067 0.2013 14.46 0.8883\\nZeroRF [49] 0.0231 35.46 0.9877 0.0124 37.76 0.9902 0.0215 34.09 0.9805 0.0366 32.43 0.9742\\nFSGS [82] 0.0427 28.50 0.9804 0.0204 32.56 0.9829 0.0686 23.09 0.9565 0.0406 28.96 0.9732\\nGaussianObject 0.0106 36.87 0.9921 0.0058 38.48 0.9941 0.0072 36.72 0.9905 0.0219 32.94 0.9799\\nTable 14. Comparisons of per-scene metrics of OpenIllumination with 4 input views. Methods with †means the metrics are from the ZeroRF paper [49].\\nMethodstone pumpkin toy potato\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.0829 21.99 0.8957 0.0945 23.07 0.9370 0.0928 21.69 0.9121 0.1196 21.37 0.9097\\n3DGS [22] 0.2890 10.93 0.8107 0.2950 11.43 0.8692 0.2898 11.16 0.8213 0.3063 12.03 0.8639\\nDietNeRF†[18] 0.0850 24.05 0.9210 0.0600 26.54 0.9700 0.0790 24.98 0.9490 0.1030 23.00 0.9490\\nRegNeRF†[36] 0.4830 10.26 0.6020 0.4650 11.74 0.7490 0.4760 10.04 0.6370 0.5050 11.63 0.7190\\nFreeNeRF†[73] 0.2100 12.91 0.7790 0.3120 11.54 0.8270 0.3510 10.79 0.7860 0.4610 11.70 0.7960\\nSparseNeRF [61] 0.2600 12.99 0.8315 0.3029 11.44 0.7991 0.1181 13.67 0.9200 0.2821 13.26 0.8798\\nZeroRF†[49] 0.0720 25.07 0.9180 0.0750 26.07 0.9610 0.0890 23.72 0.9360 0.0960 26.27 0.9460\\nGaussianObject 0.0542 23.96 0.9204 0.0538 25.89 0.9579 0.0493 24.95 0.9453 0.0690 25.09 0.9424\\nMethodpine shroom cow cake\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.1177 19.04 0.8791 0.1754 18.13 0.8533 0.1854 18.14 0.8523 0.0790 25.74 0.9396\\n3DGS [22] 0.3038 10.07 0.8095 0.3594 11.03 0.8182 0.3157 11.81 0.8573 0.2475 13.54 0.9133\\nDietNeRF†[18] 0.0930 20.94 0.9240 0.1660 19.91 0.9110 0.2070 16.30 0.8940 0.0600 28.97 0.9710\\nRegNeRF†[36] 0.4860 9.37 0.5710 0.5510 10.66 0.6580 0.4600 11.99 0.7480 0.3590 17.21 0.8680\\nFreeNeRF†[73] 0.2200 10.17 0.7910 0.5540 11.46 0.7510 0.4580 11.18 0.7460 0.2990 17.95 0.8990\\nSparseNeRF [61] 0.1412 12.28 0.8981 0.2026 13.33 0.8963 0.2170 13.64 0.9003 0.2584 18.21 0.9214\\nZeroRF†[49] 0.1160 20.68 0.9030 0.1340 23.14 0.9120 0.1390 21.91 0.9050 0.0580 29.44 0.9650\\nGaussianObject 0.0761 21.68 0.9143 0.0872 24.16 0.9211 0.0970 22.81 0.9208 0.0499 28.58 0.9613', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 20}),\n",
       " Document(page_content='22 •Yang and Li, et al.\\nTable 15. Comparisons of per-scene metrics of OpenIllumination with 6 input views. Methods with †means the metrics are from the ZeroRF paper [49].\\nMethodstone pumpkin toy potato\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.0621 24.22 0.9132 0.0682 25.82 0.9527 0.0675 24.44 0.9313 0.0931 24.24 0.9279\\n3DGS [22] 0.3139 10.87 0.7702 0.2547 12.98 0.8823 0.2993 11.10 0.7943 0.3197 11.70 0.8462\\nDietNeRF†[18] 0.0850 24.87 0.9210 0.0730 24.80 0.9660 0.0860 25.37 0.9440 0.0870 25.63 0.9550\\nRegNeRF†[36] 0.2880 13.80 0.8480 0.3500 13.58 0.8480 0.2370 13.54 0.8840 0.3480 13.92 0.8540\\nFreeNeRF†[73] 0.2360 11.62 0.7910 0.2930 11.71 0.8640 0.3460 10.65 0.8140 0.3970 11.35 0.8320\\nSparseNeRF [61] 0.2452 12.91 0.8091 0.1468 14.81 0.9406 0.1181 13.67 0.9200 0.3519 12.36 0.8614\\nZeroRF†[49] 0.0630 26.30 0.9290 0.0640 27.87 0.9660 0.0620 27.28 0.9500 0.0840 27.26 0.9510\\nGaussianObject 0.0430 26.07 0.9301 0.0415 28.06 0.9648 0.0402 27.55 0.9535 0.0571 26.56 0.9495\\nMethodpine shroom cow cake\\nLPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑\\nDVGO [59] 0.0924 20.61 0.8976 0.1327 20.88 0.8891 0.1331 22.00 0.8997 0.0571 28.15 0.9562\\n3DGS [22] 0.2687 11.73 0.8112 0.3633 11.49 0.7671 0.3246 11.55 0.8329 0.2277 14.38 0.9174\\nDietNeRF†[18] 0.1190 18.16 0.9020 0.1190 23.71 0.9300 0.1330 21.50 0.9300 0.0590 29.58 0.9730\\nRegNeRF†[36] 0.3370 11.87 0.8070 0.3290 13.22 0.8630 0.4050 13.07 0.8070 0.1280 19.66 0.9580\\nFreeNeRF†[73] 0.3280 8.85 0.7530 0.5050 10.12 0.7640 0.4420 11.09 0.7840 0.2650 16.33 0.9000\\nSparseNeRF [61] 0.3807 6.61 0.5537 0.3551 11.92 0.8290 0.2227 13.64 0.8983 0.2840 16.46 0.9100\\nZeroRF†[49] 0.0880 22.26 0.9180 0.1060 26.34 0.9280 0.1180 23.74 0.9210 0.0520 31.00 0.9690\\nGaussianObject 0.0629 23.25 0.9264 0.0712 26.35 0.9307 0.0804 24.43 0.9325 0.0389 30.06 0.9673', metadata={'source': 'https://arxiv.org/pdf/2402.10259v2.pdf', 'page': 21}),\n",
       " Document(page_content='UFO\\n : A UI-Focused Agent for Windows OS Interaction\\nChaoyun Zhang∗, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang,\\nQingwei Lin, Saravan Rajmohan, Dongmei Zhang & Qi Zhang\\nMicrosoft\\nUFO-Agent@microsoft.com\\nUser Request UFO\\nPowerPoint\\n Photos\\n Word\\nExtract\\nObserve\\nSummarize\\nOutlook\\nSend\\nEdit\\nClick\\nFigure 1: An illustration of the high-level concept of the Windows agent UFO . It completes\\na user request by extracting information from a Word document, observing a figure in Photos ,\\nand summarizing content from a PowerPoint presentation on Windows OS. Then, UFO\\nutilizes this information to compose an email and send, all accomplished automatically .\\nAbstract\\nWe introduce UFO\\n , an innovative UI-Focused agent to fulfill user re-\\nquests tailored to applications on Windows OS, harnessing the capabilities\\nof GPT-Vision. UFO employs a dual-agent framework to meticulously\\nobserve and analyze the graphical user interface (GUI) and control informa-\\ntion of Windows applications. This enables the agent to seamlessly navigate\\nand operate within individual applications and across them to fulfill user\\nrequests, even when spanning multiple applications. The framework incor-\\nporates a control interaction module, facilitating action grounding without\\nhuman intervention and enabling fully automated execution. Consequently,\\nUFO transforms arduous and time-consuming processes into simple tasks\\nachievable solely through natural language commands. We conducted\\ntesting of UFO across 9 popular Windows applications, encompassing a\\nvariety of scenarios reflective of users’ daily usage. The results, derived\\nfrom both quantitative metrics and real-case studies, underscore the su-\\nperior effectiveness of UFO in fulfilling user requests. To the best of our\\nknowledge, UFO stands as the first UI agent specifically tailored for task\\ncompletion within the Windows OS environment. The open-source code\\nfor UFO is available on https://github.com/microsoft/UFO .\\n∗Corresponding author.\\n1arXiv:2402.07939v3  [cs.HC]  23 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nThe advent of Large Language Models (LLMs) Ouyang et al. (2022); Achiam et al. (2023);\\nTouvron et al. (2023) has showcased revolutionary potential in solving complex problems\\nakin to human reasoning, planning, and collaboration Hao et al. (2023); Ding et al. (2023);\\nTalebirad & Nadiri (2023). This development brings humanity closer to the realization of\\nArtificial General Intelligence (AGI) Liu et al. (2023), offering assistance in various tasks\\nin our daily lives and exhibiting a robust and comprehensive level of ability that was\\npreviously considered challenging Chen et al. (2023b); Shao et al. (2023); Jin et al. (2023).\\nThe advancement towards more sophisticated multi-modal visual LLMs, exemplified by\\nGPT-Vision Zhang et al. (2023); Yang et al. (2023b), introduces a visual dimension to LLMs,\\nexpanding their capabilities to encompass a myriad of visual tasks and broadening their\\nscope to address challenges that require visual capability Wu et al. (2023a); Zheng et al.\\n(2024); Wake et al. (2023).\\nThe applications of Visual Large Language Models (VLM) are continually emerging and\\nthriving. One notable application involves using VLMs to interact with the User Interface\\n(UI) or Graphical User Interface (GUI) of software applications, fulfilling user requests\\nexpressed in natural language and grounding them in physical devices. While GUIs are\\nprimarily designed for human users to see and interact with, the elements and controls\\nwithin the application’s UI provide a crucial bridge for VLMs to interact, especially by\\nperceiving their visual information Hong et al. (2023b) and grounding their actions in a\\nmanner similar to humans. This facilitates the evolution from Large Language Models\\n(LLMs) to Large Action Models (LAMs) Brohan et al. (2023), enabling their decisions to\\nmanifest in physical actions and have tangible real-world impacts.\\nIn this context, the Windows operating system (OS) stands out as representative platform\\nfor LAMs, due to its high market share in the daily use of computer systems Adekotujo et al.\\n(2020), the presence of versatile applications and GUIs built upon it Ramler et al. (2018), and\\nthe complexity of tasks that necessitate long-term planning and interaction across various\\napplications Stallings (2005). The prospect of having a general intelligent agent that can\\ncomprehend user requests in natural language, and autonomously interact with the UIs\\nof applications built on Windows is highly appealing. Despite the evident demand for\\ndeveloping VLM agents tailored for Windows OS to fulfill user requests, this direction\\nremains largely unexplored, as most existing agents predominantly focus on smartphones\\nYang et al. (2023a) or web applications Zheng et al. (2024). This gap in exploration presents\\na vast, untapped opportunity to develop a Windows-specific agent.\\nTo bridge this gap, we introduce UFO\\n , a specialized UI-Focused Agent designed for\\nseamless interaction with the Windows OS, leveraging the cutting-edge capabilities of the\\nVLM GPT-Vision Yang et al. (2023b). UFO adopts a dual-agent framework, with each agent\\nanalyzing screenshots and extracting information from the GUI to make informed decisions\\nin selecting applications. Subsequently, it navigates and executes actions on their controls,\\nemulating human users to fulfill requests in natural language. The system incorporates a\\ncontrol interaction component, which plays a crucial role in translating actions from GPT-\\nVision into grounded execution on applications. This feature ensures complete automation\\nwithout the need for human intervention, thereby establishing it as a comprehensive LAM\\nframework.\\nAcknowledging that user requests often span multiple applications in their daily routines,\\nUFO incorporates an application-switching mechanism, allowing it to transition to a dif-\\nferent application when needed. This expanded capability equips UFO to handle more\\nintricate tasks that are typically overlooked by other agents. Furthermore, UFO is highly\\nextensible, enabling users to design and customize actions and controls for specific tasks\\nand applications, enhancing its versatility. In summary, UFO streamlines various tasks for\\nusers engaged in daily computer activities, transforming lengthy and tedious processes\\ninto simple tasks achievable through only textual commands. This positions UFO as a\\nvaluable, user-friendly, and automated co-pilot for the Windows OS, effectively reducing\\nthe overall complexity of usage. We illustrate this concept at a high level in Figure 1, where\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 1}),\n",
       " Document(page_content='UFO composes and sends an email by integrating text extracted from a Word document,\\nobservations from an image, and summaries from a PowerPoint presentation.\\nIn order to assess its efficacy, we conducted versatile testing of our UFO framework,\\nsubjecting it to a battery of 50 tasks spanning 9 widely used Windows applications. These\\ntasks were carefully chosen to cover a diverse spectrum of scenarios reflective of users’ daily\\ncomputational needs. The evaluation involved both quantitative metrics and in-depth case\\nstudies, highlighting the robustness and adaptability of our design, especially in the context\\nof extended and intricate requests that traverse multiple applications. To the best of our\\nknowledge, UFO stands as the pioneering agent tailored for general applications within the\\nWindows OS environment.\\n2 Related Work\\nIn this section, we review research efforts relevant to UFO , with a focus on the domains of\\nLLM agents and LLM-based GUI intelligence.\\n2.1 LLM Agents\\nThe advent of LLM agents has significantly expanded the capabilities of LLMs Wang et al.\\n(2023); Xi et al. (2023); Talebirad & Nadiri (2023), empowering them to engage in planning,\\nobservation, memorization, and responsive actions. This augmentation enables LLMs\\nto undertake more intricate tasks by emulating human-like decision-making processes.\\nNotably, AutoGPT Significant Gravitas stands as a pioneering agent within this domain,\\nfacilitating interaction with users and decomposing LLMs’ actions into discrete components\\nsuch as thoughts, reasoning, and criticism, thereby effectively addressing user requests.\\nAdditionally, TaskWeaver Qiao et al. (2023) represents a noteworthy code-centric agent\\nframework designed to disassemble user requests and translate them into manageable\\nsubtasks executable via Python code. The LangChain Agent Chase (2022), serves as an\\nextension of the LangChain framework. This extension empowers the utilization of a LLM\\nto intelligently select a sequence of actions, which may involve the utilization of customized\\ntools. The integration of LLMs within such agents not only augments their decision-making\\ncapabilities but also signifies a significant stride towards the eventual realization of AGI.\\nFurthermore, the incorporation of multi-agent LLMs represents a more potent and scalable\\nframework. This architecture facilitates the allocation of tasks to individual agents based\\non their respective strengths, fostering collaboration or competition among agents to effec-\\ntively accomplish complex tasks. AutoGen Wu et al. (2023b) exemplifies this approach by\\ndesigning each agent to be highly customizable and conversable. This design philosophy\\nenables each agent to leverage its specific strengths, thereby contributing optimally to the\\noverall task accomplishment within the multi-agent system. MetaGPT Hong et al. (2023a)\\nconstitutes another notable multi-agent framework. Within this framework, distinct roles\\nare assigned to individual GPTs, collectively forming a collaborative software entity adept\\nat addressing complex tasks. Furthermore, another multi-agent framework named AutoA-\\ngents Chen et al. (2023a) generates and coordinates multiple specialized agents to form an\\nAI team tailored for complex tasks. These multi-agent frameworks signify a burgeoning\\nbranch in LLM agent development, offering heightened scalability for tackling intricate\\ntasks.\\n2.2 LLM-based GUI Intelligence\\nThe utilization of multimodal LLM systems Durante et al. (2024) for navigating and control-\\nling GUIs in applications has emerged as a prominent and burgeoning research area. Yan et\\nal., Yan et al. (2023) employed GPT-4V Yang et al. (2023b) to navigate mobile applications\\nby inputting screenshots of GUIs to the LLM, demonstrating state-of-the-art performance\\nacross various datasets and human-designed navigation tasks. Similarly, AppAgent Yang\\net al. (2023a) leverages GPT-4V as smartphone users, enabling them to take actions on\\nmobile applications based on smartphone snapshots, thereby autonomously fulfilling user\\nrequests on physical phones. MobileAgent Wang et al. (2024) integrates Optical Character\\n3', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 2}),\n",
       " Document(page_content='AppAgent\\nAppAgent\\nGlobal Plan\\nGlobal Plan\\nSelected Application 1\\nObservations\\nObservations\\nObservations\\nLocal Plan Actions+\\nLocal Plan Actions+Selected Application 2\\nActAgent\\nUser Request\\nUser Request\\n...\\n...\\nActAgentActAgent\\nRe-selection\\nRe-selection\\nMemory\\nMemory\\n...AppAgent\\nAppAgent\\nPhase 1 Phase 2\\n......Control \\nInteractionFigure 2: The overall architecture of the UFO.\\nRecognition (OCR) tools to augment the GPT-V employed within a comparable mobile\\nagent designed for task completion on mobile phones. This integration results in a notable\\nenhancement, enabling MobileAgent to achieve completion rates comparable to human\\nperformance. On the other hand, CogAgent Hong et al. (2023b) takes a distinct approach\\nby training a dedicated visual language model specialized in GUI understanding and\\nnavigation, providing a more tailored solution for various GUI intelligence tasks.\\nDistinguishing itself from existing frameworks, our proposed UFO stands out as a multi-\\nmodal LLM agent framework specifically tailored to fulfilling user requests and manipu-\\nlating application UI within the Windows OS. This framework transcends the constrains\\nposed by different applications, enabling the seamless completion of more intricate tasks\\nwithin the Windows environment.\\n3 The Design of UFO\\nWe present UFO\\n , a groundbreaking UI-Focused Multimodal Agent designed for the\\nWindows OS interaction. UFO possesses the capability to comprehend users’ requests\\nexpressed in natural language, breaking them down into a series of step-by-step sub-tasks.\\nIt then observe the UI screenshots of of applications and operates on their control elements\\nto fulfill the overall objective. This unique functionality enables UFO to seamlessly navigate\\nacross multiple applications, effectively completing complex tasks and transcending the\\nboundaries of different applications. In Section 3.1, we provide an overview of the design\\nofUFO , followed by detailed discussions on each core component in the subsequent\\nsubsections.\\n3.1 UFO in a Nutshell\\nFirst, we present the comprehensive architecture of UFO in Figure 2. UFO operates as a dual-\\nagent framework, encompassing (i)anApplication Selection Agent (AppAgent) tasked\\nwith choosing an application for fulfilling user requests. This agent may also switch to a\\ndifferent application when a request spans multiple applications, and the task is partially\\ncompleted in the preceding application. Additionally, (ii)anAction Selection Agent\\n(ActAgent) is incorporated, responsible for iteratively executing actions on the selected\\napplications until the task is successfully concluded within a specific application. Both\\nagents leverage the multi-modal capabilities of GPT-Vision to comprehend the application\\nUI and fulfill the user’s request. They utilize a Control Interaction module to ground their\\nactions, enabling tangible impacts on the system.\\n4', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 3}),\n",
       " Document(page_content=\"AppAgent\\nUser Request\\nUser Request\\nMemory\\nMemory\\nDesktop Screenshots\\nDesktop Screenshots\\nApplication Information\\n{Label: 1, Name: Outlook, Type: Dialog  }\\n{Label: 2, Name: Word,     Type: Dialog  }\\n...Application Information\\n{Label: 1, Name: Outlook, Type: Dialog  }\\n{Label: 2, Name: Word,     Type: Dialog  }\\n...\\nObservation\\nI observe that  the Word file of meeting \\nnote is open and visible, and ...Thought\\nTo answer the user's request, I need to \\nselect the Word file and read the \\nmeeting note ...\\nSelected Application\\n{Label: 2, Name: Word}Status\\nCONTINUE\\nGlobal Plan\\n(1) Select the Word application with \\nmeeting note open.\\n(2) After identifying the working items, \\nswitch to the image viewer to …\\n(3) ...Comment\\nI am now opening the Word to extract \\nworking items from the meeting note.Output\\n Examples\\n ExamplesInputFigure 3: An illustration of the AppAgent.\\nUpon receiving a user request, the AppAgent undertakes an analysis of the demand. It\\nendeavors to select a fitting application for fulfilling the request from the currently active\\napplications. UFO equips AppAgent with the full desktop screenshot and a list of available\\napplications for reference, facilitating AppAgent’s decision-making process. Subsequently,\\nAppAgent selects an appropriate application and formulates a comprehensive global plan\\nfor request completion. This plan is then passed on to the ActAgent.\\nOnce a suitable application is identified, it is brought into focus on the desktop. The\\nActAgent then initiates actions to fulfill the user request. Before each action selection step,\\nUFO captures screenshots of the current application’s UI window with all available controls\\nannotated. Additionally, UFO records information about each control for ActAgent’s\\nobservation. ActAgent is tasked with choosing a control to operate and subsequently\\nselecting a specific action to execute on the chosen control via a control interaction module.\\nThis decision is based on ActAgent’s observation, its prior plan, and its operation memory.\\nFollowing the execution, UFO constructs a local plan for future steps, and proceeds to\\nthe next action selection step. This recursive process continues until the user request is\\nsuccessfully completed within the selected application. This concludes one phase of the\\nuser request.\\nIn scenarios where the user request spans multiple applications, the ActAgent will delegate\\nthe task to the AppAgent for the purpose of switching to a different application once\\nActAgent completes its tasks on the current one, initiating the second phase of the request.\\nThis iterative process continues until all aspects of the user request are fully completed.\\nUsers have the option to introduce new requests interactively, prompting UFO to process\\nthe new request by repeating the aforementioned process. Upon the successful completion\\nof all user requests, UFO concludes its operation. In the subsequent sections, we delve into\\nthe intricate details of each component within the UFO framework.\\n3.2 AppAgent\\nThe AppAgent bears the responsibility of selecting an active application to fulfill user\\nrequests or switching to a new application when necessary. Additionally, AppAgent con-\\nstructs a comprehensive global plan to orchestrate the entire task, and its architecture is\\nillustrated in Figure 3. AppAgent takes the following information as input:\\n•User Request : The original user query submitted to UFO.\\n•Desktop Screenshots : Screenshots of the current desktop, where multiple screens are\\nconcatenated into a single image.\\n•Application Information : A listing of available active application details, encompassing\\ntheir names and types.\\n5\", metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 4}),\n",
       " Document(page_content=\"Previous Screenshot\\nClean Screenshot Annotated ScreenshotActAgent\\nObservation\\nI observe that  the editing \\nwindow is open for \\ncomposing an new email, and \\nthe previous action has taken \\neffect …Thought\\nTo complete  the user's \\nrequest, I need to Input the \\nrecipient’s email address in \\nthe “To” block …\\nSelected Control\\n{Label: 2, Name: To}Status\\nCONTINUE\\nLocal Plan\\n(1) Enter the email title ...\\n(2) Compose the email body, \\nI need to input...\\n(3) ...Comment\\nI am proceeding to enter the \\nemail address, than I will \\ncompose the email title and \\nbody.Output\\nUser Request\\nUser Request\\nMemory\\nMemory\\n Examples\\n ExamplesInput\\nControl Information\\n{Label: 1, Name: Send,  Type: Button }\\n{Label: 2, Name: To,       Type: Edit }\\n{Label: 3, Name: Home, Type: MenuItem }\\n...Control Information\\n{Label: 1, Name: Send,  Type: Button }\\n{Label: 2, Name: To,       Type: Edit }\\n{Label: 3, Name: Home, Type: MenuItem }\\n...Selected Control\\nset_edit_text (text=“…” )Function\\nControl AnnotationsFigure 4: An illustration of the ActAgent.\\n•Memory : Comprising previous thoughts, comments, actions, and execution results.\\n•Examples : Textual examples for application selection, serving as demonstrations for the\\ntask.\\nThe provided information, including Desktop Screenshots, Application Information, and\\nMemory, collectively equips AppAgent with a comprehensive set of data to facilitate\\ndecision-making. Desktop Screenshots and Application Information enable AppAgent\\nto comprehend the current status and constrain its scope for application selection. On\\nthe other hand, Memory serves as a historical record of past request completions, aiding\\nAppAgent in making informed decisions based on prior experiences. This multifaceted\\ninput framework enhances AppAgent’s ability to select the most suitable application for\\nfulfilling user requests.\\nUpon collecting all pertinent information, AppAgent employs GPT-V to generate the fol-\\nlowing outputs:\\n•Observation : Detailed descriptions of the screenshots of the current desktop window.\\n•Thoughts : The logical next step required to fulfill the given task, adhering to the Chain-\\nof-thought (CoT) paradigm Wei et al. (2022).\\n•Selected Application : The label and name of the chosen application.\\n•Status : The task status, denoted as either “CONTINUE” or “FINISH”.\\n•Global Plan : A subsequent plan of action to complete the user request, typically a global\\nand coarse-grained plan.\\n•Comment : Additional comments or information to provide, including a brief progress\\nsummary and points to be highlighted.\\nPrompting AppAgent to provide its Observation and Thoughts serves a dual purpose.\\nFirstly, it encourages AppAgent to meticulously analyze the current status, offering a\\ndetailed explanation for its logic and decision-making process. This not only enhances the\\nlogical coherence of its decisions Wei et al. (2022); Ding et al. (2023), but also contributes to\\nimproving the overall interpretability of UFO . Secondly, AppAgent determines the status\\nof the task, outputting “FINISH” if it deems the task completed. AppAgent may also leave\\ncomments for the user, reporting progress, highlighting potential issues, or addressing\\nany user queries. Once AppAgent identifies the selected application, UFO proceeds to\\ntake specific actions within this application to fulfill the user request, with the ActAgent\\nresponsible for executing these actions.\\n3.3 ActAgent\\nThe ActAgent functions as a downstream entity following AppAgent, responsible for\\nexecuting specific actions on the selected application to fulfill the user request. Its input and\\noutput also exhibit variations compared to AppAgent, as depicted in Figure 4. ActAgent\\naccepts the following input:\\n6\", metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 5}),\n",
       " Document(page_content='•User Request : The original user query submitted to UFO, identical to the AppAgent.\\n•Screenshots : The screenshots encompass three parts: (i)Previous Screenshot; (ii)Clean\\nscreenshot; and (iii)Annotated screenshot.\\n•Control Information : A listing of names and types of controls that are enabled for\\noperations in the selected application.\\n•Memory : Previous thoughts, comments, actions, and execution results, mirroring the\\nAppAgent.\\n•Examples : Textual examples for action selection, serving as demonstrations.\\nIn contrast to the AppAgent, UFO provides ActAgent with three types of screenshots to\\naid in its decision-making process. The previous screenshot with the last selected control\\nhighlighted in a red rectangle ( i.e., ) helps comprehend the operation execution in the last\\nstep and analyze the impact of the action. The clean screenshot allows for an understanding\\nof the application’s status without annotation obstructions, and the annotated screenshot,\\nlabeling each control with a number ( e.g.,36), facilitates a better understanding of the\\nfunction and location of UI elements. Different types of controls are labeled with distinct\\ncolors for differentiation.\\nMoreover, the memory fed into ActAgent serves two key purposes. Firstly, it acts as a\\nreminder for the agent, enabling ActAgent to analyze past actions and reduce the likelihood\\nof repeating actions that proved ineffective. Secondly, it establishes a crucial channel for\\ncross-application communication. Execution results, such as text extracted from a document\\nor a description of an image, are stored in the memory module. ActAgent can selectively\\nincorporate this information for actions that require it, such as composing an email with text\\nfrom different sources. This augmentation significantly extends the capabilities of UFO.\\nGiven this comprehensive input, ActAgent meticulously analyzes all the information and\\noutputs the following:\\n•Observation : Detailed descriptions of the screenshots of the current application window,\\nalong with an analysis of whether the last action has taken effect.\\n•Thoughts : The logical thinking and rationale process behind the current action decision.\\n•Selected Control : The label and name of the chosen control for the operation.\\n•Function : The specific function and its arguments applied to the control.\\n•Status : The task status, indicated as either “CONTINUE” if further action is needed,\\n“FINISH” if the task is completed, “PENDING” if the current action requires user confir-\\nmation, “SCREENSHOT” if the agent believes a further screenshot is needed to annotate\\na smaller set of controls, and “APP SELECTION” when the task is completed on the\\ncurrent application and a switch to a different one is required.\\n•Local Plan : A more precise and fine-grained plan for future actions to completely fulfill\\nthe user request.\\n•Comment : Additional comments or information, including a brief progress summary,\\nhighlighted points, or changes in the plan, similar to what AppAgent provides.\\nWhile some of its output fields may share similarities with AppAgent, UFO determines the\\nnext step based on the outputted Status of the task. If the task is not finished, it applies the\\nFunction to the selected control, triggering the next state of the application after execution.\\nActAgent iteratively repeats this process of observing and reacting to the selected application\\nuntil the user request is fully completed or a switch to a different application is required.\\n3.4 Control Interaction\\nTo execute and ground the selected action from ActAgent on the application controls, UFO\\nfirst detects and translates the action into an executable operation. The Python package\\npywinauto Bim & Min-shuai (2014) offers valuable tools for inspecting UI controls and\\nperforming actions on them. It provides a range of functions and classes for automating the\\n7', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 6}),\n",
       " Document(page_content='Figure 5: An example of the annotated PowerPoint GUI with information provided by\\npywinauto . Different colors of annotations represent different control types.\\nWindows GUI, allowing for programmatic interaction and control of Windows applications.\\nThis capability is particularly useful for tasks such as automated testing, scripting, and the\\nautomation of repetitive tasks. As the backend, we choose the Windows UI Automation\\nAPI Dinh et al. (2018) for its robust support in UI inspection and interaction through code.\\nUFO utilizes pywinauto to inspect all actionable controls of an application, retrieving their\\nprecise location and boundary box to facilitate annotations. An example of annotated\\ncontrols on a PowerPoint GUI is presented in Figure 5, with control information provided\\nbypywinauto . Different colors various control types. Additionally, pywinauto provides rich\\ncontext for each control, including its name, type, and title, which are crucial information\\nfor control and action selection.\\nUFO focuses on the following 10 constrained control types with high relevance, as de-\\ntermined by our analysis. These include Button ,Edit ,TabItem ,Document ,ListItem ,\\nMenuItem ,TreeItem ,ComboBox ,Hyperlink ,ScrollBar . We show a detailed descriptions\\nof these control types in Table 1. These set can covers majority of relevant controls in the\\napplications, and it is also extendable per request.\\nFor the specific functions applied to the control, we have chosen common and widely used\\nmouse operations supported by pywinauto , as well as developed customized actions. These\\nactions include1:\\n•Click : Clicking the control item with the mouse, with options for left or right clicks,\\nsingle or double clicks.\\n•SetText : Inputting text into an editable control, mimicking the keyboard behaviors.\\n•Annotate : Capturing a screenshot of the current application window and annotating the\\ncontrol item on the GUI.\\n•Summary : Summarizing the observation of the current application window based on the\\nclean screenshot.\\n•GetText : Retrieving the textual information of the control.\\n•Scroll : Scrolling the control item vertically or horizontally to make hidden content\\nvisible.\\n1Note that each of these actions can only be executed on specific and different types of controls.\\n8', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 7}),\n",
       " Document(page_content='Table 1: The detailed descriptions of control types supported by UFO.\\nControl Type Description\\nButton A button is a user interface element that users can interact with to trigger\\nan action. Clicking the button typically initiates a specific operation or\\ncommand.\\nEdit An edit control allows users to input and edit text or numeric data. It\\nis commonly used for fields where users can type information, such as\\ntextboxes or search bars.\\nTabItem A tab item is part of a tab control, organizing content into multiple pages.\\nUsers can switch between different tab items to access distinct sets of\\ninformation or functionalities.\\nDocument A document control represents a document or a page in a document-\\nview architecture. It is often used to display and manage documents or\\nlarge blocks of text.\\nListItem A list item is an element within a list control, presenting data in a list\\nformat. Users can select and interact with individual items within the\\nlist.\\nMenuItem A menu item is part of a menu control, providing a list of commands\\nor options. Users can click on menu items to trigger specific actions or\\nnavigate through application features.\\nTreeItem A tree item is a node within a tree control, organizing information in\\na hierarchical structure. Users can expand or collapse tree items to\\nnavigate through a hierarchical set of data.\\nComboBox A ComboBox is a combination of a text box and a drop-down list. It\\nallows users to either type a value directly into the text box or choose\\nfrom a predefined list by opening the drop-down menu.\\nHyperlink A Hyperlink enables users to navigate to another location or resource.\\nThey are often used to provide easy access to external websites, docu-\\nments, or specific sections within an application.\\nScrollBar A scroll bar allows users to scroll through content that is larger than\\nthe visible area. It provides a way to navigate vertically or horizontally\\nwithin a window or control.\\nClick ,SetText ,GetText , and Scroll are common functions originally supported by\\npywinauto , covering most daily operations on GUI. Annotate andSummary are customized\\noperations to fulfill special requests for UFO . The former allows re-annotation of the GUI\\nwith a more concise list of controls (details in Section 3.5.3), and the latter enables UFO\\nto describe its visual observation in text to fulfill user requests. At every step, ActAgent\\nwill choose an action from this available list to execute on the selected UI control in the\\napplication. With the control interaction module, UFO evolves into a LAM capable of\\nmaking tangible impacts on the system.\\n3.5 Special Design Consideration\\nUFO incorporates a series of dedicated design elements specifically tailored to the Windows\\nOS. These enhancements are aimed at facilitating more effective, automated, and secure\\ninteractions with UI controls, thereby enhancing its capacity to address users’ requests. Key\\nfeatures encompass interactive mode, action customization, control filtering, plan reflection,\\nand safeguard, each of which is elaborated upon in the subsequent subsections.\\n3.5.1 Interactive Mode\\nUFO offers users the capability to engage in interactive and iterative interactions, rather than\\ninsisting on one-shot completions. Upon the completion of a task, users have the flexibility\\nto request UFO to enhance the previous task, propose an entirely new task for UFO to\\nundertake, or perform operations to assist UFO in tasks where it may lack proficiency,\\nsuch as providing a password input. This user-friendly approach not only distinguishes\\n9', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 8}),\n",
       " Document(page_content='UFO from other existing UI agents in the market but also allows it to absorb user feedback,\\nfacilitating the completion of longer and more intricate tasks.\\n3.5.2 Action Customization\\nUFO currently facilitates operations on controls or the UI, as detailed in Section 3.4. How-\\never, it is imperative to note that this list is not exhaustive and can be highly extended and\\ncustomized beyond the constraints of Windows UI Automation. This extensibility is crucial\\nfor tailoring the framework to meet specific user needs, allowing for the incorporation\\nof functionalities such as keyboard shortcuts, macro commands, plugins, and more. An\\nillustrative example is the utilization of summary() , which leverages UFO ’s multimodal\\ncapabilities to observe and describe screenshots as required.\\nTo achieve this level of customization, users can register their bespoke operations. This\\ninvolves specifying the purpose, arguments, return values, and, if necessary, providing\\nillustrative examples for demonstration purposes. This information is then incorporated into\\nthe prompt for UFO ’s reference. Once the registration process is completed, the customized\\noperation becomes available for execution by UFO . This inherent flexibility renders UFO a\\nhighly extendable framework, enabling it to fulfill more intricate and user-specific requests\\nwithin the Windows system.\\n3.5.3 Control Filtering\\nIn the GUI of an application, Windows UI Automation can detect hundreds of control\\nitems, each available for operations. However, annotating all these controls can clutter\\nthe application UI screenshots, obstructing the view of individual items and generating\\nan extensive list that may pose challenges for UFO in making optimal choice decisions.\\nNotably, some of these controls may prove unlikely to be useful or relevant for fulfilling\\nuser requests. Consequently, the implementation of filtering mechanisms becomes crucial\\nto exclude certain controls and streamline the decision-making process for UFO.\\nTo address this challenge, UFO employs a dual-level control filtering approach, comprising\\nthe hard level and the soft level. At the hard level, candidate controls are constrained based\\non specific control types with high relevance and popularity, as detailed in Section 3.4. More-\\nover, we incorporate a soft filter mechanism that empowers UFO to dynamically determine\\nwhether to re-select a more concise list of specified controls. This adaptive filtering approach\\nis triggered when UFO perceives an excessive number of controls, potentially cluttering the\\ncurrent screenshot and obstructing the visibility of the required control. In such scenarios,\\nUFO intelligently returns a refined list of control candidates of interest. Subsequently, it\\ncaptures a new screenshot annotated only with these controls, facilitating a more focused\\nand effective filtering process. This feature enhances the automation capabilities of the\\nframework, enabling UFO to make intelligent decisions for optimal performance.\\n3.5.4 Plan Reflection\\nWhile both the application selection agent and the action selection agent are responsible\\nfor initiating plans to fulfill user requests, the actual state of the application’s UI may not\\nalways align with the anticipated conditions. For instance, if UFO initially plans to click a\\n“New Email” button in the next step, but this button is not visible in the current UI, UFO\\nmay need to first navigate to the ”Main Page” and subsequently locate the “New Email”\\nbutton. Consequently, both the plan and action should adapt accordingly.\\nTo address this dynamic nature of the UI, we prompt UFO to continuously revise its plan at\\nevery decision step, allowing it to deviate from the original course as needed. This adaptive\\napproach enhances UFO ’s responsiveness to the evolving application status based on its\\nobservations. The efficacy of such reflective mechanisms has been substantiated in various\\nLLM frameworks and agent architectures Qiao et al. (2023); Ding et al. (2023); Shinn et al.\\n(2023). Moreover, the integration of plan reflection significantly contributes to the enhanced\\nperformance of UFO in navigating and interacting with diverse application UIs.\\n10', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 9}),\n",
       " Document(page_content='Table 2: An incomplete list of sensitive actions considered in UFO.\\nSensitive Action Description\\nSend Action for a Message\\nor EmailInitiating the “Send” action, such as clicking the send but-\\nton, is considered sensitive as the dispatched message or\\nemail becomes irretrievable.\\nDeleting or Modifying\\nFiles and FoldersOperations involving the deletion or modification of files\\nand folders, especially those situated in critical system di-\\nrectories or containing vital user data.\\nClosing a Window or Ap-\\nplicationClosing a window or application is flagged as sensitive due\\nto the potential for data loss or system crashes.\\nAccessing Webcam or Mi-\\ncrophoneAccessing the webcam or microphone without explicit user\\nconsent is identified as sensitive to address privacy con-\\ncerns.\\nInstalling or Uninstalling\\nSoftwareActions related to installing or uninstalling software ap-\\nplications are marked as sensitive due to their impact on\\nsystem configuration and potential security risks.\\nBrowser History or Pass-\\nword RetrievalRetrieving sensitive user data such as browser history or\\nstored passwords is identified as a sensitive action, posing\\npotential privacy leaks.\\n3.5.5 Safeguard\\nLastly, we acknowledge the sensitivity of certain actions within the system, such as irre-\\nversible changes resulting from operations like file deletion. In recognition of these potential\\nrisks, UFO incorporates a safeguard mechanism to seek user confirmation before executing\\nsuch actions. The safeguard functionality is not limited to the following enumerated list in\\nTable 2, as UFO intelligently assesses the sensitivity of each action. With the deployment\\nof this safeguard, UFO establishes itself as a significantly safer and trustworthy agent,\\nmitigating the risk of compromising the system or jeopardizing user files and privacy.\\n4 Experiment\\nIn this section, we comprehensively evaluate UFO ’s performance in completing user re-\\nquests on the Windows OS. The evaluation is conducted through a combination of quantita-\\ntive analysis and case studies, encompassing diverse tasks.\\n4.1 Benchmark & Baselines & Metrics\\nTo comprehensively evaluate the performance across various Windows applications, we\\ndeveloped a benchmark called WindowsBench. This benchmark comprises 50 user requests,\\nencompassing 9 popular Windows applications commonly used in daily tasks. The selected\\napplications include Outlook ,Photos ,PowerPoint ,Word ,Adobe Acrobat ,File Explorer ,\\nVisual Studio Code ,WeChat , and Edge Browser . These applications cater to different\\npurposes such as work, communication, coding, reading, and web browsing, ensuring the\\ndiversity and comprehensiveness of the evaluation. For each application, we designed\\n5 distinct requests, and an additional 5 requests involve interactions spanning multiple\\napplications. This setup results in a total of 50 requests, with each application having at\\nleast one request linked to a follow-up request, providing a comprehensive evaluation of\\nUFO ’s interactive mode. We present the detailed list of requests utilized in WindowsBench\\nin Table 5, 6, and 7 in Appdendix Section A. Requests involving follow-up interactions are\\norganized numerically within each category.\\nGiven the absence of existing Windows agents, we selected GPT-3.5 and GPT-4 as baseline\\nmodels. Since these models lack the capability to directly interact with applications, we\\ninstructed them to provide step-by-step instructions to complete the user requests. A human\\nthen acted as their surrogate to carry out the operations. When visual abilities were required,\\nwe allowed the baselines to pause, as they couldn’t perform these tasks independently.\\n11', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 10}),\n",
       " Document(page_content='Table 3: Performance comparison achieved by UFO on WindowsBench.\\nFramework Success Step Completion Rate Safeguard Rate\\nGPT-3.5 (Human Surrogate) 24% 7.86 31.6% 50%\\nGPT-4 (Human Surrogate) 42% 8.44 47.8% 57.1%\\nUFO 86% 5.48 89.6% 85.7%\\nTable 4: The detailed performance breakdown across applications achieved by UFO on\\nWindowsBench.\\nApplication Success Step Completion Rate Safeguard Rate\\nOutlook 100.0% 6.8 94.0% 100.0%\\nPhotos 80.0% 4.0 96.7% 100.0%\\nPowerPoint 80.0% 5.6 88.8% 50.0%\\nWord 100.0% 5.4 92.7% -\\nAdobe Acrobat 60.0% 4.2 78.7% 100.0%\\nFile Explorer 100.0% 4.8 88.7% 100.0%\\nVisual Studio Code 80.0% 4.0 84.0% -\\nWeChat 100.0% 5.0 98.0% 66.7%\\nEdge Browser 80.0% 5.2 92.0% 100.0%\\nCross-Application 80.0% 9.8 83.0% 100.0%\\nIn terms of evaluation metrics, we assess UFO from three perspectives for each request:\\nsuccess, step, completion rate, and safeguard rate. The success metric determines if the\\nagent successfully completes the request. The step refers to the number of actions the agent\\ntakes to fulfill a task, serving as an indicator of efficiency. The completion rate is the ratio of\\nthe number of correct steps to the total number of steps. Lastly, the safeguard rate measures\\nhow often UFO requests user confirmation when the request involves sensitive actions.\\nGiven the potential instability of GPT-V , which may generate different outputs each time,\\nwe conduct three tests for each request and select the one with the highest completion rate.\\nThis approach is consistent for the other baselines as well.\\n4.2 Performance Evaluation\\nFirstly, we present a comprehensive quantitative comparison of various frameworks using\\nthe WindowsBench dataset, as illustrated in Table 3. Remarkably, our UFO achieves\\nan impressive 86% success rate across the benchmark, surpassing the best-performing\\nbaseline (GPT-4) by more than double. This result underscores the sophistication of UFO\\nin successfully executing tasks on the Windows OS, positioning it as a highly effective\\nagent. Moreover, UFO exhibits the highest completion rate, suggesting its capability to take\\nsignificantly more accurate actions. We also observe that UFO completes tasks with the\\nfewest steps, showcasing its efficiency as a framework, while GPT-3.5 and GPT-4 tend to\\nprovide more steps but are less effective for the tasks. From the safety perspective, UFO\\nattains the highest safeguard rate at 85.7%, which proves that it can classify sensitive requests\\naccurately, affirming its status as a secure agent that actively seeks user confirmation for\\nthese requests.\\nThe inferior performance of the baselines compared to UFO can be attributed to two primary\\nfactors. Firstly, both baselines lack the ability to directly interact with the real application\\nenvironment, relying on human surrogates for action execution. This limitation results in\\nan inability to adapt to changes and reflections in the environment, leading to decreased\\naccuracy. Secondly, the baselines solely accept textual input, neglecting the importance\\nof visual capabilities for GUI interaction. This weakness hinders their effectiveness in\\ncompleting user requests on Windows, where visual information is often crucial. Notably,\\nGPT-4 outperforms GPT-3.5, highlighting its greater potential in these tasks. In summary,\\nconsidering all the results above, we show that UFO excels across all four evaluation metrics,\\noutperforming other baselines to a great extent, establishing itself as a versatile and potent\\nframework for interacting with the Windows OS.\\n12', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 11}),\n",
       " Document(page_content='We present the detailed breakdown of UFO ’s performance across different applications\\nin Table 4 (with the breakdowns for GPT-3.5 and GPT-4 available in the Appendix, Sec-\\ntion B). The “-” symbol in the safeguard rate column denotes that all requests related to the\\napplication are not sensitive. Notably, UFO demonstrates strong performance across all\\napplications, showcasing its versatility and effectiveness in interacting with and operating\\non diverse software. However, there is an exception in the case of Adobe Acrobat , where\\nUFO achieves a 60% success rate and a 78.7% completion rate. This can be attributed\\nto the fact that many control types in Adobe Acrobat are not supported by Windows UI\\nAutomation, posing challenges for UFO in operating within this application. Importantly,\\nwhen tasked with completing requests spanning multiple applications, UFO maintains\\na high level of performance. Despite requiring more steps (average of 9.8) to fulfill such\\nrequests, UFO achieves an 80% success rate, an 83% completion rate, and a 100% safeguard\\nrate. This underscores UFO ’s sophistication in navigating across different applications to\\naccomplish long-term and complex tasks, solidifying its position as an omnipotent agent for\\nWindows interactions.\\n4.3 Case Study\\nTo demonstrate the exceptional capabilities of UFO , we provide two case studies illus-\\ntrating how UFO efficiently fulfills user requests, particularly focusing on tasks involving\\nPowerPoint and spanning across multiple applications2.\\n4.3.1 Deleting All Notes on a PowerPoint Presentation\\nIn Figure 6, we tasked UFO with the request: “Help me quickly remove all notes in the slide\\nof the ufo testing.”. This request is a common scenario when users want a clean version of\\na slide without any accompanying notes. Traditional methods involve manually deleting\\nnotes page by page, a tedious and time-consuming process for lengthy slides.\\nHowever, UFO efficiently identifies a shortcut, simplifying the entire procedure. Analyzing\\nits initial plan, UFO proposes the use of the ‘Remove All Presentation Notes’ function,\\nwhich is a feature often overlooked by PowerPoint users due to its hidden placement. The\\nautomation process commences with UFO navigating to the “File” tab, providing access\\nto the backstage view. Subsequently, it smoothly transitions to the “Info” menu, where\\nthe desired function might be located. To examine the document for notes, it clicks the\\n‘Check for Issues’ button and selects ‘Inspect Document’. Once inspected, the hidden gem,\\n‘Remove All Presentation Notes’ is revealed at the bottom of the menu. UFO identifies\\nthis, scrolls down, locates the function, and initiates the click. Given the sensitive nature of\\ndeleting notes, UFO implements its safeguard feature, seeking user confirmation. Upon\\nconfirmation, all notes vanish with a single click. This demonstration highlights how UFO\\nempowers users to work smarter, not harder, establishing itself as an intelligent assistant.\\n4.3.2 Composing an Emails with Information Gathered from Multiple Applications\\nWe make a more complex request shown in Figure 7, “My name is Zac. Please read the\\nmeeting note to identify all action items, and include the detailed description LLM training\\nworkflow in the LLM-training.png, to compose a new email of these contents. Send the\\nfull email to our leader Hidan via email address to ask for his review”. This request puts a\\nsignificant demand on UFO ’s ability to navigate and interact across multiple applications\\nswiftly.\\nIn response, UFO formulates a dynamic plan for the task, deftly navigating between Word,\\nPhotos, and Outlook to achieve the overarching objective. Initially, it activates the required\\ndocument file in Word, utilizing the GetText API to extract text from the main window, which\\nis then logged into its memory. Subsequently, UFO switches to the LLM-training image file\\nin Photos, observing and generating a detailed description for future email composition.\\nWith all the necessary information gathered, UFO opens the Outlook application, accessing\\nthe “New Email” button to initiate the editing block. The agent then repeats the screenshot\\n2For additional cases, please consult the Appendix Section D.\\n13', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 12}),\n",
       " Document(page_content='capturing, annotation, and action process, autonomously inputting the email recipient,\\ndrafting a subject, and composing the email body, including all required information. Prior\\nto sending, the safeguard feature prompts the user for confirmation due to the sensitive\\nnature of the action. Once confirmed, the email is sent. We show the email composed by\\nUFO in Appendix Section C.\\nThe result is a meticulously composed and dispatched email by UFO , skillfully capturing\\ninformation from both the meeting notes and the details of the LLM pipeline image. This\\nillustrates the remarkable ability of UFO in completing complex tasks that require long-term\\nplanning and memory, even when spanning multiple applications.\\n5 Limitations & Lessons Learned\\nWe acknowledge several limitations in the current UFO framework. Firstly, the available UI\\ncontrols and actions are currently limited by those supported by pywinauto and Windows\\nUI Automation. Applications and controls that deviate from this standard and backend\\nare not currently supported by UFO . To broaden UFO ’s capabilities, we plan to expand its\\nscope by supporting alternative backends, such as Win32 API, or incorporating dedicated\\nGUI models for visual detection, as demonstrated by CogAgent Hong et al. (2023b). This\\nenhancement will enable UFO to operate across a broader range of applications and handle\\nmore complex actions.\\nSecondly, we recognize the challenge UFO faces when exploring unfamiliar application\\nUIs, which may be niche or uncommon. In such cases, UFO may require substantial time to\\nnavigate and identify the correct action. To address this, we propose leveraging knowledge\\nfrom online search engines as an external knowledge base for UFO . Analyzing both textual\\nand image-based guidelines in search results will empower UFO to distill a more precise and\\ndetailed plan for completing requests on unfamiliar applications, enhancing its adaptability\\nand generality.\\n6 Conclusion\\nWe introduce UFO\\n , an innovative UI-focused agent designed to fulfill user requests\\nin natural language through intelligent interactions with applications on the Windows\\nOS. Leveraging GPT-Vision, UFO analyzes application GUI screenshots and controls in-\\nformation to dynamically select the optimal application and control for executing actions\\nand satisfying user queries. The grounding of actions on the application is facilitated by a\\ncontrol interaction module, minimizing human intervention and enabling full automation,\\nestablishing it as a comprehensive LAM framework. With the dual-agent design, namely,\\nAppAgent and ActAgent, UFO intelligently switches between applications, allowing for\\nthe completion of long and complex tasks that span different applications. Additionally, we\\nincorporate key features such as Action Customization and Safeguard to enhance UFO ’s\\nextendibility and safety. Evaluation results based on 50 requests across 9 popular Windows\\nOS applications demonstrate its remarkable versatility and generalizability. To the best of\\nour knowledge, UFO represents the pioneering effort in developing a UI automation agent\\nspecifically tailored for the Windows OS environment.\\n14', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 13}),\n",
       " Document(page_content=\"Step 7\\nStep 7\\nStep 8\\nStep 8\\nStep 2\\nStep 4\\nStep 3\\nHelp me quickly remove all \\nnotes in the slide of the ufo  \\ntesting.\\nStep 6\\nStep 6\\n Step 5\\nStep 5\\nStep 1\\nStep 1\\n(1) Open and set focus the ufo_testing - PowerPoint.\\n (2) Click the 'File' button at the upper left corner, to open a menu.\\n(3) Click the 'Info' ListItem, to access the 'Info' page.\\n (4) Click the 'Check for Issues' MenuItem, to open a menu for checking. \\n (5) Click the 'Inspect Document', to open a document inspector. \\n (6) Click the 'Inspect' button, to inspect the ppt for notes. \\n (7) I should now find and click the 'Remove All Presentation Notes'. \\n (8) Once 'Remove All Presentation Notes' is visible, I should click it to remove all notes in the \\nufo_testing.pptx file. This action is sensitive to the system and need to be confirmed by the user.\\nInitial Plan\\nUFO\\n<Continue>\\n<Continue>\\n<Continue> <Continue>\\n<Continue> <Continue>\\n<Pending> <Finish>\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'Inspect Document' \\ndialog open. The 'Remove All Document Properties \\nand Personal Information' and 'Remove All Revision \\nTracking Data' buttons are visible, but the 'Remove \\nAll Presentation Notes' button is not visible. The \\nscrollbar on the right indicates that there are more \\noptions available below the current view. The last \\naction of clicking the 'Inspect' button has taken \\neffect, as evidenced by the open 'Inspect Document' \\ndialog.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to scroll down \\nusing the ScrollBar to make the 'Remove All \\nPresentation Notes' button visible. This is in line with \\nthe steps needed to remove all notes from the \\npresentation quickly and efficiently.\\nSelected Control \\n : Inspect\\nFunction\\n : wheel_mouse_input(wheel_dist= -20)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'Inspect Document' \\ndialog open. The 'Remove All Document Properties \\nand Personal Information' and 'Remove All Revision \\nTracking Data' buttons are visible, but the 'Remove \\nAll Presentation Notes' button is not visible. The \\nscrollbar on the right indicates that there are more \\noptions available below the current view. The last \\naction of clicking the 'Inspect' button has taken \\neffect, as evidenced by the open 'Inspect Document' \\ndialog.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to scroll down \\nusing the ScrollBar to make the 'Remove All \\nPresentation Notes' button visible. This is in line with \\nthe steps needed to remove all notes from the \\npresentation quickly and efficiently.\\nSelected Control \\n : Inspect\\nFunction\\n : wheel_mouse_input(wheel_dist= -20)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'Inspect Document' \\ndialog open. The 'Remove All Presentation Notes' \\nbutton is highlighted, indicating that the previous \\naction to remove all notes has been taken. The \\n'Close' button is visible and ready to be clicked to \\nexit the dialog.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the final step is to click the \\n'Close' button to exit the 'Inspect Document' dialog. \\nThis will complete the task of removing all notes \\nfrom the presentation as requested by the user.\\nSelected Control \\n : Close\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'Inspect Document' \\ndialog open. The 'Remove All Presentation Notes' \\nbutton is highlighted, indicating that the previous \\naction to remove all notes has been taken. The \\n'Close' button is visible and ready to be clicked to \\nexit the dialog.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the final step is to click the \\n'Close' button to exit the 'Inspect Document' dialog. \\nThis will complete the task of removing all notes \\nfrom the presentation as requested by the user.\\nSelected Control \\n : Close\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'ufo_testing' file open \\nin the backstage view, specifically on the 'Info' page. \\nThe 'Check for Issues' menu has been expanded, \\nrevealing the 'Inspect Document' option, which is \\nannotated with label 48. This matches the next step in \\nthe previous plan to access the document inspector. \\nThe application appears to be in the correct view to \\nproceed with the removal of all notes from the \\npresentation.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to click the \\n'Inspect Document' MenuItem to open the document \\ninspector . This is necessary to reach the 'Remove All \\nPresentation Notes' function.\\nSelected Control \\n : Inspect Document\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'ufo_testing' file open \\nin the backstage view, specifically on the 'Info' page. \\nThe 'Check for Issues' menu has been expanded, \\nrevealing the 'Inspect Document' option, which is \\nannotated with label 48. This matches the next step in \\nthe previous plan to access the document inspector. \\nThe application appears to be in the correct view to \\nproceed with the removal of all notes from the \\npresentation.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to click the \\n'Inspect Document' MenuItem to open the document \\ninspector . This is necessary to reach the 'Remove All \\nPresentation Notes' function.\\nSelected Control \\n : Inspect Document\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'File' tab open, \\ndisplaying various options such as 'Home', 'New', \\n'Open', etc. The 'Info' ListItem is visible, which is \\nnecessary for the next step in the process of removing \\nall notes from the presentation. The application is in \\nthe backstage view, which is where I need to be to \\naccess the 'Check for Issues' and 'Inspect Document' \\noptions.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to click the 'Info' \\nListItem to access the 'Info' page  where I can find \\nthe 'Check for Issues' option. This is in line with the \\nsteps needed to remove all notes from the \\npresentation quickly.\\nSelected Control \\n : Info\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'File' tab open, \\ndisplaying various options such as 'Home', 'New', \\n'Open', etc. The 'Info' ListItem is visible, which is \\nnecessary for the next step in the process of removing \\nall notes from the presentation. The application is in \\nthe backstage view, which is where I need to be to \\naccess the 'Check for Issues' and 'Inspect Document' \\noptions.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to click the 'Info' \\nListItem to access the 'Info' page  where I can find \\nthe 'Check for Issues' option. This is in line with the \\nsteps needed to remove all notes from the \\npresentation quickly.\\nSelected Control \\n : Info\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'ufo_testing.pptx' \\nfile open. The 'File Tab' button is visible, which is \\nneeded to access the backstage view where the \\n'Check for Issues' option is located. This matches \\nthe next step in the previous plan. The application \\nappears to be in the normal editing view, and no \\nbackstage view or other menus are open at this \\ntime.\\nThoughts\\n : Following the previous plan, the next \\nstep is to click the 'File Tab' to access the backstage \\nview . This is necessary to proceed with the steps to \\nremove all notes from the presentation quickly.\\nSelected Control \\n : File Tab\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'ufo_testing.pptx' \\nfile open. The 'File Tab' button is visible, which is \\nneeded to access the backstage view where the \\n'Check for Issues' option is located. This matches \\nthe next step in the previous plan. The application \\nappears to be in the normal editing view, and no \\nbackstage view or other menus are open at this \\ntime.\\nThoughts\\n : Following the previous plan, the next \\nstep is to click the 'File Tab' to access the backstage \\nview . This is necessary to proceed with the steps to \\nremove all notes from the presentation quickly.\\nSelected Control \\n : File Tab\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'ufo_testing' file \\nopen in the backstage view on the 'Info' page. The \\n'Check for Issues' MenuItem is visible and \\naccessible, which aligns with the next step in the \\nprevious plan to remove all notes from the \\npresentation quickly. The application is in the \\ncorrect state to proceed with the removal of notes.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to click the \\n'Check for Issues' MenuItem  to access the options \\nfor inspecting the document. This is necessary to \\nreach the 'Remove All Presentation Notes' \\nfunction.\\nSelected Control \\n : Check for Issues\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'ufo_testing' file \\nopen in the backstage view on the 'Info' page. The \\n'Check for Issues' MenuItem is visible and \\naccessible, which aligns with the next step in the \\nprevious plan to remove all notes from the \\npresentation quickly. The application is in the \\ncorrect state to proceed with the removal of notes.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to click the \\n'Check for Issues' MenuItem  to access the options \\nfor inspecting the document. This is necessary to \\nreach the 'Remove All Presentation Notes' \\nfunction.\\nSelected Control \\n : Check for Issues\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'Inspect \\nDocument' dialog open. The 'Inspect' button is \\nvisible and ready to be clicked, which aligns with \\nthe next step in the previous plan to inspect the \\npresentation for notes. The scrollbar (label 57) is \\nalso visible, indicating that there may be additional \\noptions below the current view.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to click the \\n'Inspect' button  to inspect the presentation for \\nnotes. After this, I will need to scroll down to find \\nthe 'Remove All Presentation Notes' option, which \\nis not currently visible on the screen.\\nSelected Control \\n : Inspect\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'Inspect \\nDocument' dialog open. The 'Inspect' button is \\nvisible and ready to be clicked, which aligns with \\nthe next step in the previous plan to inspect the \\npresentation for notes. The scrollbar (label 57) is \\nalso visible, indicating that there may be additional \\noptions below the current view.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to click the \\n'Inspect' button  to inspect the presentation for \\nnotes. After this, I will need to scroll down to find \\nthe 'Remove All Presentation Notes' option, which \\nis not currently visible on the screen.\\nSelected Control \\n : Inspect\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'Inspect \\nDocument' dialog open. The 'Remove All \\nPresentation Notes' button is now visible, which \\nwas the target control item according to the \\nprevious plan. The scrollbar has been used to \\nreveal this button, as indicated by the scrollbar \\nposition and the last step taken.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to click the \\n'Remove All Presentation Notes' button to remove \\nall notes  from the presentation. This action is \\nsensitive as it will permanently delete the notes, \\nand thus, it requires user confirmation  before \\nproceeding.\\nSelected Control \\n :Remove All Presentation \\nNotes\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nPowerPoint application with the 'Inspect \\nDocument' dialog open. The 'Remove All \\nPresentation Notes' button is now visible, which \\nwas the target control item according to the \\nprevious plan. The scrollbar has been used to \\nreveal this button, as indicated by the scrollbar \\nposition and the last step taken.\\nThoughts\\n : Following the previous plan and the \\ncurrent screenshot, the next step is to click the \\n'Remove All Presentation Notes' button to remove \\nall notes  from the presentation. This action is \\nsensitive as it will permanently delete the notes, \\nand thus, it requires user confirmation  before \\nproceeding.\\nSelected Control \\n :Remove All Presentation \\nNotes\\nFunction\\n : click_input(button='left', double=False)\\nFigure 6: An detailed example of UFO completing user request: “Help me quickly remove\\nall notes in the slide of the ufo testing.”.\\n15\", metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 14}),\n",
       " Document(page_content=\"Step 7Step 7Step 2\\nStep 4Step 3\\nMy name is Zac. Please read the meeting \\nnote in meeting_note.docx to identify all \\naction items,  and include the detailed \\ndescription LLM training workflow in the \\nLLM -training.png, to compose an new \\nemail of these contents. Sent the full \\nemail to our leader Hidan via {email \\naddress}  to ask for his review.\\nStep 6 Step 5Step 5Step 1\\n(1) Open the Word application window for 'meeting_note  -  Compatibility Mode - Word' to read the \\nmeeting notes and identify all action items.\\n(2) After identifying the action items, open the 'LLM -training.png  - Photos' to describe the LLM training \\nworkflow.\\n(3) Once both contents are ready, open the 'Mail - Outlook' application to compose a new email.\\n(4) Include the identified action items from the meeting notes and the detailed description of the LLM \\ntraining workflow in the email body.\\n(5) Set the recipient of the email to {email address}  and the subject as per the user's request.\\n(6) Send the email to Hidan for his review. This action is sensitive and needs to be confirmed by the \\nuser .\\nInitial Plan\\nUFO\\n<APP_SELECTION>\\n<Continue><Continue>\\n<Continue> <Continue>\\n<Pending>\\nObservation\\n : The current screenshot shows that the \\nOutlook application is open with the new email \\ncomposition window visible. The 'To' field is already filled \\nwith the email address, and the subject line 'Review of \\nLLM Training Project Action Items and Workflow' is set. \\nThe email body is ready for content to be inputted. \\nComparing with the last screenshot, it is evident that the \\nprevious actions of opening the new email window and \\nsetting the recipient and subject have taken effect.\\nThoughts\\n : Based on the current screenshot and the \\ncontrol item list, the next step is to input the identified \\naction items from 'meeting_note.docx' and the  detailed \\ndescription of the LLM training workflow from 'LLM -\\ntraining.png' into the email body . This aligns with the \\nuser's request to compose a new email with these \\ncontents for Hidan's review. \\nSelected Control \\n : Message body\\nFunction\\n : set_edit_text (text={email body})\\nDear Hidan,\\nPlease find below the action items identified from our recent meeting, as well as a detailed \\ndescription of the LLM training workflow.\\nAction Items:\\n- Zac Yang: Develop a comprehensive project plan and share it with the team by 2024.03.06.\\n- Kevin Cheung: Conduct thorough research on potential risks associated with LLM training \\nand present findings by 2024.04.01. \\n- Tom James: Formulate a detailed proposal for resource allocation, including computing \\nresources and software tools. Submit it for review by 2024.01.03.\\n- Sophia Chow: Compile and distribute meeting minutes, including action items and \\ndeadlines, within 24 hours.\\nLLM Training Workflow Description:\\nThe LLM training framework consists of four main components: 'Pre -training' with no \\nhuman involvement using proprietary and unlabeled data, 'Self -supervised learning' where \\nthe base model is trained with narrow datasets, 'Fine -tuning' which involves human \\nfeedback to fine -tune the model, and 'Prompt -based learning' where the augmented model \\nis developed through prompting with human involvement and specialized knowledge.\\nPlease review the above information and provide your feedback at your earliest \\nconvenience.\\nBest regards,\\nZac\\nObservation\\n : The current screenshot shows that the \\nOutlook application is open and the 'New Email' window \\nis active. The 'To', 'Cc', 'Add a subject', and 'Message \\nbody' fields are visible and ready for input. The 'New \\nEmail' button is not visible because the new email \\ncomposition window is already open. The last action of \\nclicking the 'New Email' button has taken effect as seen \\nin the screenshot.\\nThoughts\\n : According to the previous plan and the \\ncurrent screenshot, the next step is to input the email \\naddress  of the receiver, Hidan, into the 'To' field. This \\naction aligns with the user's request to send an email to \\nHidan for his review.\\nSelected Control \\n : To\\nFunction\\n : set_edit_text (Text= {email address 1})\\nObservation\\n : The current screenshot displays the \\nimage 'LLM -training.png' opened in the Photos \\napplication. The image shows a diagram of a workflow \\nprocess, which includes steps like 'Pre -training', 'Self -\\nsupervised learning', 'Fine -tuning', and 'Prompt -based \\nlearning'. There are no annotations obstructing the view \\nof the workflow diagram. The last action taken was \\nsetting focus to the 'LLM -training.png - Photos' \\napplication.\\nThoughts\\n : According to the previous plan and the \\ncurrent screenshot, my next step is to summarize the \\nworkflow depicted in the 'LLM -training.png' image . This \\ninvolves describing the steps and processes shown in the \\ndiagram. After summarizing this information, I will need to \\nswitch to the Outlook application to compose and send the \\nemail to Hidan.\\nSelected Control \\n : LLM -training\\nFunction\\n : summary(text= {image summary} )\\nObservation\\n : The current screenshot shows the \\nWord application window with the document \\n'meeting_note - Compatibility Mode' open. The \\ndocument content is visible, and it appears to be a \\nmeeting note with sections such as Date, Time, \\nLocation, Attendees, and Agenda. The previous \\naction of setting focus to the Word application \\nwindow was successful, as indicated by the last \\nscreenshot.\\nThoughts\\n : According to the user's request and \\nthe previous plan, my next step is to extract the \\ntext from the 'meeting_note.docx' document to \\nidentify all action items . The 'meeting_note - \\nCompatibility Mode' document is already open, and \\nI can proceed to get the text from the document \\nusing the 'GetText' action on the Document control \\nitem.\\nSelected Control \\n : meeting_note \\nFunction\\n : texts( )\\nObservation\\n : The current screenshot shows the \\nOutlook application with the 'New Email' button \\nvisible and ready to be clicked. The application \\nwindow is the same as the one from the last step, \\nindicating that the previous action to set focus on \\nthe Outlook application was successful.\\nThoughts\\n : According to the step history and the \\nprevious plan, the next action is to click on the \\n'New Email' button  to open a new email \\ncomposition window. Once the window is open, I \\nwill input the email address, subject, and body \\ncontent as per the user's request.\\nSelected Control \\n : New mail\\nFunction\\n : click_input(button='left', double=False)\\nObservation\\n : The current screenshot shows the \\nOutlook application with a new email composition \\nwindow open. The 'To' field is already filled with \\nthe email address {email address 1} . The 'Subject' \\nfield is empty, and the 'Message body' is ready for \\ntext input. The previous action of setting the \\nrecipient's email address in the 'To' field has taken \\neffect as seen in the screenshot.\\nThoughts\\n : Following the previous plan, the next \\nstep is to input the subject of the email. Since the \\n'To' field is already filled, I can proceed to set the \\nsubject in the 'Add a subject' Edit control . After \\nthat, I will include the identified action items from \\n'meeting_note.docx' and the detailed description \\nof the LLM training workflow from 'LLM -\\ntraining.png' in the email body.\\nSelected Control \\n : Add a subject\\nFunction\\n : set_edit_text(text='Review of LLM \\nTraining Project Action Items and Workflow')\\nObservation\\n : The current screenshot shows the \\nOutlook application with an email composition \\nwindow open. The 'To' field is filled, and the \\nsubject line reads 'Review of LLM Training Project \\nAction Items and Workflow'. The email body \\ncontains the action items and a detailed \\ndescription of the LLM training workflow, as per \\nthe user's request. The 'Send' button is visible and \\nready to be clicked to send the email. The last \\naction of inputting the email body content has \\ntaken effect as seen in the screenshot.\\nThoughts\\n : The email composition is complete \\nwith all the necessary information as per the user's \\nrequest. The next and final step is to send the \\nemail to Hidan for his review. Clicking the 'Send' \\nbutton is a sensitive action that requires user \\nconfirmation  before execution.\\nSelected Control \\n :Send\\nFunction\\n : click_input(button='left', double=False)\\n[Email Address]\\n[Email Address]\\n[Email Address]<APP_SELECTION>\\n Review of LLM Training Project Action Items and Workflow\\nFigure 7: A detailed example of UFO completing user request: “My name is Zac. Please\\nread the meeting note to identify all action items, and include the detailed description LLM\\ntraining workflow in the LLM-training.png, to compose an new email of these contents.\\nSent the full email to our leader Hidan via {email address }to ask for his review.”.\\n16\", metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 15}),\n",
       " Document(page_content='References\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\\nGPT-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.\\nAkinlolu Adekotujo, Adedoyin Odumabo, Ademola Adedokun, and Olukayode Aiyeniko.\\nA comparative study of operating systems: Case of windows, unix, linux, mac, android\\nand ios. International Journal of Computer Applications , 176(39):16–23, 2020.\\nHE Bim and WANG Min-shuai. Application of pywinauto in software performance test.\\nComputer and Modernization , (8):135, 2014.\\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof\\nChoromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:\\nVision-language-action models transfer web knowledge to robotic control. arXiv preprint\\narXiv:2307.15818 , 2023.\\nHarrison Chase. LangChain, October 2022. URL https://github.com/langchain-ai/\\nlangchain .\\nGuangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, B ¨orje F Karlsson, Jie Fu,\\nand Yemin Shi. AutoAgents: A framework for automatic agent generation. arXiv preprint\\narXiv:2309.17288 , 2023a.\\nYinfang Chen, Huaibing Xie, Minghua Ma, Yu Kang, Xin Gao, Liu Shi, Yunjie Cao, Xuedong\\nGao, Hao Fan, Ming Wen, et al. Empowering practical root cause analysis by large\\nlanguage models for cloud incidents. arXiv preprint arXiv:2305.15778 , 2023b.\\nYuhang Chen, Chaoyun Zhang, Minghua Ma, Yudong Liu, Ruomeng Ding, Bowen Li,\\nShilin He, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Imdiffusion: Im-\\nputed diffusion models for multivariate time series anomaly detection. arXiv preprint\\narXiv:2307.00754 , 2023c.\\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin,\\nSaravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying\\nthe law of penrose triangle for thought generation. arXiv preprint arXiv:2311.04254 , 2023.\\nDuong Tran Dinh, Pham Ngoc Hung, and Tung Nguyen Duy. A method for automated\\nuser interface testing of windows-based applications. In Proceedings of the 9th International\\nSymposium on Information and Communication Technology , pp. 337–343, 2018.\\nZane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar,\\nRohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, et al. Agent AI: Surveying\\nthe horizons of multimodal interaction. arXiv preprint arXiv:2401.03568 , 2024.\\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and\\nZhiting Hu. Reasoning with language model is planning with world model. arXiv preprint\\narXiv:2305.14992 , 2023.\\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili\\nWang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. MetaGPT: Meta programming\\nfor multi-agent collaborative framework. arXiv preprint arXiv:2308.00352 , 2023a.\\nWenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang,\\nZihan Wang, Yuxiao Dong, Ming Ding, et al. CogAgent: A visual language model for gui\\nagents. arXiv preprint arXiv:2312.08914 , 2023b.\\nYuxuan Jiang, Chaoyun Zhang, Shilin He, Zhihao Yang, Minghua Ma, Si Qin, Yu Kang,\\nYingnong Dang, Saravan Rajmohan, Qingwei Lin, et al. Xpert: Empowering incident\\nmanagement with query recommendations via large language models. arXiv preprint\\narXiv:2312.11988 , 2023.\\n17', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 16}),\n",
       " Document(page_content='Pengxiang Jin, Shenglin Zhang, Minghua Ma, Haozhe Li, Yu Kang, Liqun Li, Yudong\\nLiu, Bo Qiao, Chaoyun Zhang, Pu Zhao, et al. Assess and summarize: Improve outage\\nunderstanding with large language models. arXiv preprint arXiv:2305.18084 , 2023.\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu.\\nGPTeval: Nlg evaluation using GPT-4 with better human alignment. arXiv preprint\\narXiv:2303.16634 , 2023.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language\\nmodels to follow instructions with human feedback. Advances in Neural Information\\nProcessing Systems , 35:27730–27744, 2022.\\nBo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang\\nDong, Jue Zhang, Lu Wang, et al. TaskWeaver: A code-first agent framework. arXiv\\npreprint arXiv:2311.17541 , 2023.\\nRudolf Ramler, Thomas Wetzlmaier, and Robert Hoschek. Gui scalability issues of windows\\ndesktop applications and how to find them. In Companion Proceedings for the ISSTA/ECOOP\\n2018 Workshops , pp. 63–67, 2018.\\nYunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-LLM: A trainable agent for\\nrole-playing. arXiv preprint arXiv:2310.10158 , 2023.\\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366 , 2023.\\nSignificant Gravitas. AutoGPT. URL https://github.com/Significant-Gravitas/\\nAutoGPT .\\nWilliam Stallings. The windows operating system. Operating Systems: Internals and Design\\nPrinciples , 2005.\\nYashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power\\nof intelligent LLM agents. arXiv preprint arXiv:2306.03314 , 2023.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\\nNaoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi.\\nGPT-4V (ision) for robotics: Multimodal task planning from human demonstration. arXiv\\npreprint arXiv:2311.12015 , 2023.\\nJunyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and\\nJitao Sang. Mobile-Agent: Autonomous multi-modal mobile device agent with visual\\nperception. arXiv preprint arXiv:2401.16158 , 2024.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan\\nChen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based\\nautonomous agents. arXiv preprint arXiv:2308.11432 , 2023.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V\\nLe, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language\\nmodels. Advances in Neural Information Processing Systems , 35:24824–24837, 2022.\\nChaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao\\nZhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can GPT-4V (ision) serve medical\\napplications? case studies on GPT-4V for multimodal medical diagnosis. arXiv preprint\\narXiv:2310.09909 , 2023a.\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\\nLi Jiang, Xiaoyun Zhang, and Chi Wang. AutoGen: Enabling next-gen LLM applications\\nvia multi-agent conversation framework. arXiv preprint arXiv:2308.08155 , 2023b.\\n18', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 17}),\n",
       " Document(page_content='Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,\\nJunzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model\\nbased agents: A survey. arXiv preprint arXiv:2309.07864 , 2023.\\nAn Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang,\\nYiwu Zhong, Julian McAuley, Jianfeng Gao, et al. GPT-4V in wonderland: Large multi-\\nmodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562 ,\\n2023.\\nZhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu.\\nAppAgent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771 ,\\n2023a.\\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\\nLijuan Wang. The dawn of lmms: Preliminary explorations with GPT-4V (ision). arXiv\\npreprint arXiv:2309.17421 , 9(1):1, 2023b.\\nXinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan,\\nWilliam Yang Wang, and Linda Ruth Petzold. GPT-4V (ision) as a generalist evaluator for\\nvision-language tasks. arXiv preprint arXiv:2311.01361 , 2023.\\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. GPT-4V (ision) is a generalist\\nweb agent, if grounded. arXiv preprint arXiv:2401.01614 , 2024.\\nA Requests in WindoesBench and Detailed Evaluations\\nIn Table 5, 6, and 7, we present the complete user requests included in WindowsBench, along\\nwith the detailed results achieved by UFO . These requests span across nine different popular\\nWindows applications, incorporating various commonly used functions. Requests with\\nfollow-up tasks are sequentially numbered for clarity. In the Safeguard column, “-” denotes\\nthat the request is not sensitive and does not require user confirmation. “ ✓” indicates that\\nthe request is sensitive, and UFO successfully activates the safeguard for user confirmation,\\nwhile “ ✗” signifies that the safeguard fails to trigger for a sensitive request. In the success\\ncolumn, “ ✓” denotes that UFO completes the request successfully, while “ ✗” indicates a\\nfailure to fulfill the request.\\n19', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 18}),\n",
       " Document(page_content='Table 5: The requests in WindowsBench and detailed results achieved by UFO (Part-I).\\nRequest Application Safeguard Step Success Completion\\nRate\\nI am Zac. Draft an email, to [email address 1]\\nand cc [email address 2], to thanks for his con-\\ntribution on the VLDB paper on using diffu-\\nsion models for time series anomaly detection.\\nDon’t send it out.Outlook - 6✓100%\\nSearch ’Spring Festival’ in my mail box, and\\nopen the second returned email.Outlook - 4✓100%\\nDelete the first junk email. Outlook✓4✓100%\\nPlease download and save the pdf attachment\\nto the local in the first sent email.Outlook - 10✓70%\\n(1) Draft an email, to {email address 1 }and to\\nask him if he can come to the meeting at 3:00\\npm. Don’t send it out.\\n(2) Add some content to the email body to\\ntell him the meeting is very important, and cc\\n{email address 2 }as well.\\n(3) Send the email now.Outlook✓10✓100%\\nGive a detailed description of the components\\nand workflows of the TaskWeaver architecture\\nin the image.Photos - 2✓100%\\nOpen the image of the LLM training and rotate\\nit for 2 times.Photos - 3✓100%\\nOpen the image of the LLM training and\\ndelete it.Photos✓4✓100%\\nOpen the mouse image, then loop over the\\nnext two images, without closing them. Sum-\\nmarize them one by one.Photos - 5✗83.3%\\n(1) Zoom in the image of the autogen for 2\\ntimes to make it clearer, then describe it with\\ndetailed workflow and components.\\n(2) It looks too big. Just zoom it to fit the\\nscreen.Photos - 6✓100%\\nHelp me quickly remove all notes in the slide\\nof the ufo testing, without looping through\\neach slide one-by-one.PowerPoint✓8✓100%\\nClear the recording on current page for ppt. PowerPoint✗3✓100%\\nPlease add Morph transition to the current\\npage and the next page of the ufo testing.pptPowerPoint - 6✓83.3%\\nSummarize scripts for all pages in the ppt one\\nby one to help with my presetation.PowerPoint - 7✗85.7%\\n(1) Apply the first format generated by the\\nDesigner to the current ppt.\\n(2) This format is not beautiful. Select a better\\nformat and explain why you select it.PowerPoint - 8✓75%\\nCheck spelling and grammar of the current\\nmeeting note.Word - 2✓100%\\nPlease change the theme of the meeting note\\ninto ’Organic’.Word - 4✓100%\\nSave meeting note as an Adobe PDF to the\\nlocal.Word - 6✓83.3%\\nAdd a cover page to the meeting note. Choose\\none you think is beautiful.Word - 5✓100%\\n(1) Please change to a better-looking format\\nand color to beautify the current page of the\\nmeeting note.\\n(2) Do more to make it look even better.Word - 10✓80%\\n20', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 19}),\n",
       " Document(page_content='Table 6: The requests in WindowsBench and detailed results achieved by UFO (Part-II).\\nRequest Application Safeguard Step Success Completion\\nRate\\nClose all pdf that are currently open. Adobe Acrobat✓3✓100%\\nCascade the current pdf windows and close\\nthe first one.Adobe Acrobat✓3✗60%\\nVisually find and present to me the timeline\\nand milestones in the current meeting note.Adobe Acrobat - 4✗50%\\n(1) Read the imdiffuion paper. What is the\\nmain contribution of it?\\n(2) Find the first figure of the paper. What\\ndoes it show?Adobe Acrobat - 5✓100%\\n(1) Comprehend the current pdf, what is the\\ndifference between phase 1 and phase 2?\\n(2) What is the difference of the Application Se-\\nlection Agent and the Action Selection Agent?\\n(3) Base on the workflow pdf, is there anything\\ncan be improved for the agents?\\n(4) Thanks, this is actually the workflow of\\nyourself. Do you think there are other things\\ncan be improved for you? I will implement\\nyour thought on it.\\n(5) Great. Do you think there are other things\\nthat can be improved visually for the figure\\nitself in the pdf, to provide better illustration\\nfor other readers?Adobe Acrobat - 6✓83.3%\\nNavigate to /Desktop/mix/screen and open\\ns0.png by only double clicking.File Explorer - 6✓83.3%\\nDelete all files individually in the current fig-\\nure folder.File Explorer✓6✓100%\\nCopy the actagent file in the folder to the Doc-\\nument folder.File Explorer - 5✓60%\\nCreate a new txt file in the current folder. File Explorer - 3✓100%\\n(1) Open all images that are related to dragon\\nin the test folder.\\n(2) Summarize in detailed what you have seen\\nin the image d4.File Explorer - 4✓100%\\nTell me the changelog of the {code base name }\\nrepo in their readme base on your observation.Visual Studio Code - 5✓80%\\nFind the use of ’print with color’ in the script\\nfolder in the {code base name }repo.Visual Studio Code - 4✓100%\\nDownload the Docker extension in the {code\\nbase name }repo.Visual Studio Code - 4✓100%\\nCreate a new terminal at the {code base name }\\nrepo.Visual Studio Code - 2✓100%\\n(1) Read through carefully and review the\\ncode in the utils.py in the {code base name }\\nand identify any potential bugs or aspects can\\nbe improved.\\n(2) What about in the draw bbox multi func-\\ntion?Visual Studio Code - 5✗40%\\nPlease send a ’smile’ emoticon to the current\\nchatbox at Wechat.WeChat✗4✓100%\\nDelete the current chatbox of ’File Transfer’ on\\nWechat.WeChat✓3✓100%\\nPlease like {user name }’s post at Moment. WeChat - 3✓100%\\nBring the first picture to the front in my\\n”favourite” at WeChat and decribe it.WeChat - 5✓100%\\n(1) Open the {account name }Official Account\\nat WeChat.\\n(2) Send a message to ’File Transfer’ to intro-\\nduce this account.WeChat✓10✓90%\\n21', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 20}),\n",
       " Document(page_content='Table 7: The requests in WindowsBench and detailed results achieved by UFO (Part-III).\\nRequest Application Safeguard Step Success Completion\\nRate\\nHow many total citations does Geoffrey Hin-\\nton have currently?Edge Browser - 4✓100%\\nPost ”It’s a good day.” on my Twitter. Edge Browser✓6✓100%\\nDownload the Imdiffusion repo as zip. Edge Browser - 7✓100%\\nChange the theme of the browser to the icy\\nmint.Edge Browser - 5✓100%\\n(1) Find and navigate to {name }’s homepage\\nat Microsoft.\\n(2) Print this page in color.Edge Browser - 5✗60%\\nMy name is Zac. Please read the meeting\\nnote in meeting note.docx to identify all ac-\\ntion items, and include the detailed descrip-\\ntion LLM training workflow in the LLM-\\ntraining.png, to compose an new email of\\nthese contents. Sent the full email to our leader\\nHidan via {email address }to ask for his re-\\nview.Word, Photos,\\nOutlook- 10✓100%\\nSearch for and read through the latest news\\nabout Microsoft and summarize it, then send\\nit to ’File Transfer’ on WeChat.Edge Browser,\\nWeChat✓9✓100%\\nOpen the image of ufo rv at UFO-\\nwindows/logo/, summarize its content\\nand use the summary to search for a similar\\nimage on Google.File Explorer,\\nEdge Browser- 10✓90%\\nObserve the overview.pdf, then download the\\nmost similar paper base on this architecture\\nfrom the Internet base on your understanding,\\nand sent it to the ’File Transfer’ on WeChat.Adobe Acrobat,\\nEdge Browser,\\nWeChat- 10✗25%\\n(1) Read the paper.pptx, summarize the paper\\ntitle presented on the slide, then search and\\nopen the paper on the Internet.\\n(2) Can you summarize this paper and down-\\nload its PDF version for me?PowerPoint,\\nEdge Broswer- 10✓100%\\n22', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 21}),\n",
       " Document(page_content='Table 8: The detailed performance breakdown achieved by GPT-3.5 (Human Surrogate) on\\nWindowsBench.\\nApplication Success Step Completion Rate Safeguard Rate\\nOutlook 60.0% 7.6 76.2% 100.0%\\nPhotos 60.0% 5.6 35.7% 100.0%\\nPowerPoint 20.0% 11.2 20.6% 0.0%\\nWord 20.0% 7.6 25.0% -\\nAdobe Acrobat 0.0% 7.4 5.3% 50.0%\\nFile Explorer 40.0% 7.0 53.3% 0.0%\\nVisual Studio Code 0.0% 7.6 0.0% -\\nWeChat 40.0% 6.0 48.4% 33.3%\\nEdge Browser 0.0% 7.6 32.0% 100.0%\\nCross-Application 0.0% 11.0 18.8% 50.0%\\nTable 9: The detailed performance breakdown achieved by GPT-4 (Human Surrogate) on\\nWindowsBench.\\nApplication Success Step Completion Rate Safeguard Rate\\nOutlook 100.0% 8.4 73.9% 0.0%\\nPhotos 40.0% 7.0 32.7% 100.0%\\nPowerPoint 40.0% 10.4 35.2% 50.0%\\nWord 20.0% 9.2 15.3% -\\nAdobe Acrobat 0.0% 7.6 40.2% 50.0%\\nFile Explorer 80.0% 6.2 63.4% 100.0%\\nVisual Studio Code 40.0% 7.4 40.3% -\\nWeChat 40.0% 6.2 68.0% 66.7%\\nEdge Browser 60.0% 8.2 58.8% 100.0%\\nCross-Application 0.0% 13.8 49.7% 50.0%\\nB Performance Breakdown of GPT-3.5 and GPT-4\\nIn Tables 8 and 9, we present the detailed performance breakdown for GPT-3.5 and GPT-4,\\nrespectively. It is evident that GPT-4 significantly outperforms GPT-3.5, aligning with our\\nexpectations. Notably, GPT-4 exhibits superior performance, particularly in tasks related to\\nVisual Studio Code , where GPT-3.5 frequently struggles to select the correct application\\ninitially, leading to its overall lower performance. Another noteworthy instance is seen in\\nEdge Browser , where GPT-3.5 often overlooks crucial steps, resulting in request failures. In\\ngeneral, both baselines exhibit inconsistent performance across different applications, and\\ntheir overall efficacy falls significantly short when compared to UFO.\\nC Email Composed by UFO in Section 4.3.2\\nBelow is the email composed by UFO , as detailed in Section 4.3.2. The content of the\\nemail is synthesized by gathering information from a Word document and incorporating a\\ndescription extracted from an image for the writing.\\n23', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 22}),\n",
       " Document(page_content='Title: Review of LLM Training Project Action Items and Workflow\\nDear Hidan,\\nPlease find below the action items identified from our recent meeting, as well as a\\ndetailed description of the LLM training workflow.\\nAction Items:\\n- Zac Yang: Develop a comprehensive project plan and share it with the team by\\n2024.03.06.\\n- Kevin Cheung: Conduct thorough research on potential risks associated with LLM\\ntraining and present findings by 2024.04.01.\\n- Tom James: Formulate a detailed proposal for resource allocation, including\\ncomputing resources and software tools. Submit it for review by 2024.01.03.\\n- Sophia Chow: Compile and distribute meeting minutes, including action items and\\ndeadlines, within 24 hours.\\nLLM Training Workflow Description:\\nThe LLM training framework consists of four main components: ’Pre-training’ with\\nno human involvement using proprietary and unlabeled data, ’Self-supervised\\nlearning’ where the base model is trained with narrow datasets, ’Fine-tuning’ which\\ninvolves human feedback to fine-tune the model, and ’Prompt-based learning’ where\\nthe augmented model is developed through prompting with human involvement\\nand specialized knowledge.\\nPlease review the above information and provide your feedback at your earliest\\nconvenience.\\nBest regards,\\nZac\\nD Additional Case Study\\nIn this section, we present six additional case studies to illustrate the efficacy of UFO in per-\\nforming diverse tasks across the Windows operating system. These case studies encompass\\nboth single-application scenarios and instances where UFO seamlessly transitions between\\nmultiple applications.\\nD.1 Reading a PDF\\nUFO also demonstrates its effectiveness in reading and comprehending a PDF file. In\\nFigure 8, we request UFO to “(1) Read the imdiffusion paper. What is the main contribution\\nof it? (2) Find the first figure of the paper. What does it show?”. The imdiffusion paper Chen\\net al. (2023c) delves into complex technology related to anomaly detection and diffusion\\nmodels. Remarkably, UFO showcases its understanding of the paper by summarizing\\nits abstract using OCR in vision input, accurately capturing its main contribution. In\\na follow-up request, we ask UFO to find and explain the first figure. UFO navigates\\nthrough the PDF, locates the initial figure, and delivers a detailed explanation, comparing\\nthe forecasting, reconstruction, and imputation approaches in anomaly detection. This\\nshowcases the remarkable OCR ability of UFO in understanding both text and figures,\\nmaking it a valuable assistant for paper reading. Below shows UFO ’s summary on the\\npaper and the description of the first figure:\\n24', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 23}),\n",
       " Document(page_content=\"(1) Read the imdiffuion paper. What is the main contribution of it?\\n (2) Find the first figure of the paper. What does it show? \\nThe 'ImDiffusion' paper's abstract discusses the importance of anomaly \\ndetection in multivariate time series data and the challenges associated \\nwith existing approaches. The paper introduces 'ImDiffusion', a novel \\napproach that combines time series imputation and diffusion models to \\nachieve accurate and robust anomaly detection. The method is designed \\nto handle complex multivariate time series data, enabling the imputation \\nof missing and irregular values in the time series. This approach \\nreportedly improves the accuracy of anomaly detection by leveraging \\ndiffusion models as a preprocessing step to accurately reconstruct the \\nanomaly detection time series.\\nFigure 1: Examples of reconstruction, forecasting and imputation modeling of time series for anomaly \\ndetection. To address the challenges and overcome the limitations of existing approaches, we propose a \\nnovel anomaly detection method - ImDiffusion. This detector combines the use of time series imputation and \\ndiffusion models to achieve accurate and robust anomaly detection. ImDiffusion employs dedicated gating \\ndata that makes the diffusion models extra cautiously model the time points, helping utilities see more \\ndetail, to recognize valued and serious issues of anomalies through the challenges of detection. The paper \\nintroduces a novel framework based on the combination of time series imputation and diffusion models that \\neffectively leverages neighboring values in the time series as additional conditional information, enabling a \\nmore accurate modeling of the temporal and inter -related dependencies present in multivariate data. \\nSecondly, the reference performance threshold of the calibration test of the counter prediction from then \\nexchanging values helps to focus the detection in outliers, Figure 1 presents an example in which forecasting, \\nreconstruction, and imputation models are employed to predict a time series using diffusion models. \\nObserve that the imputation -based methods exhibit the highest prediction performance especially on \\nnormal periods, thus only the imputation method successfully identifies the period of anomaly. We therefore \\nemploy time series imputation for accurate self -supervised modeling of time series, which forms the \\nfoundation of our proposed ImDiffusion framework.\\nUser\\nUFO\\nFigure 8: UFO completes user request: “(1) Read the imdiffuion paper. What is the main\\ncontribution of it? (2) Find the first figure of the paper. What does it show? ”\\nPaper Summary: The ’ImDiffusion’ paper’s abstract discusses the importance of\\nanomaly detection in multivariate time series data and the challenges associated\\nwith existing approaches. The paper introduces ’ImDiffusion’, a novel approach\\nthat combines time series imputation and diffusion models to achieve accurate and\\nrobust anomaly detection. The method is designed to handle complex multivariate\\ntime series data, enabling the imputation of missing and irregular values in the time\\nseries. This approach reportedly improves the accuracy of anomaly detection by\\nleveraging diffusion models as a preprocessing step to accurately reconstruct the\\nanomaly detection time series.\\nFigure Description: Figure 1: Examples of reconstruction, forecasting and impu-\\ntation modeling of time series for anomaly detection. To address the challenges\\nand overcome the limitations of existing approaches, we propose a novel anomaly\\ndetection method - ImDiffusion. This detector combines the use of time series im-\\nputation and diffusion models to achieve accurate and robust anomaly detection.\\nImDiffusion employs dedicated gating data that makes the diffusion models extra\\ncautiously model the time points, helping utilities see more detail, to recognize val-\\nued and serious issues of anomalies through the challenges of detection. The paper\\nintroduces a novel framework based on the combination of time series imputation\\nand diffusion models that effectively leverages neighboring values in the time series\\nas additional conditional information, enabling a more accurate modeling of the\\ntemporal and inter-related dependencies present in multivariate data. Secondly, the\\nreference performance threshold of the calibration test of the counter prediction from\\nthen exchanging values helps to focus the detection in outliers, Figure 1 presents an\\nexample in which forecasting, reconstruction, and imputation models are employed\\nto predict a time series using diffusion models. Observe that the imputation-based\\nmethods exhibit the highest prediction performance especially on normal periods,\\nthus only the imputation method successfully identifies the period of anomaly. We\\ntherefore employ time series imputation for accurate self-supervised modeling of\\ntime series, which forms the foundation of our proposed ImDiffusion framework.\\n25\", metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 24}),\n",
       " Document(page_content=\"By clicking on the first design \\nsuggestion in the 'Designer' pane, I \\nhave applied the desired format to \\nthe current slide, completing the \\nuser's request.\\nI have selected the design suggestion \\nlabeled 53 because it provides a clear \\nvisual hierarchy by placing the \\ndescriptive picture at the top, drawing \\nimmediate attention. The title and text \\nare positioned at the bottom, making it \\neasy for the audience to follow the \\nflow of information. The contrasting \\nbackground colors and font colors \\nenhance readability and focus on the \\ncontent.\\n(1) Apply the first format generated \\nby the  Designer to the current ppt.\\n(2) This format is not beautiful. \\nSelect a better format and explain \\nwhy you select it.\\nUser\\nUser\\nUser\\nUser\\nUFO\\nUFO\\nUFO\\nUFO\\nFigure 9: UFO completes user request: “(1) Apply the first format generated by the Designer\\nto the current ppt. (2) This format is not beautiful. Select a better format and explain why\\nyou select it.”\\nD.2 Designing a PowerPoint Slide\\nIn Figure 9, we present another example where we make two requests to UFO : “(1) Apply\\nthe first format generated by the Designer to the current ppt. (2) This format is not beautiful.\\nSelect a better format and explain why you selected it.” The first step is relatively straight-\\nforward, where UFO successfully navigates to the ‘Designer’ Pane and applies the desired\\nformat to the slide. The second request, however, depends on UFO ’s aesthetic judgment.\\nUFO performs this task effectively and provides a reasonable explanation: “I have selected\\nthe design suggestion because it provides a clear visual hierarchy by placing the descriptive\\npicture at the top, drawing immediate attention. The title and text are positioned at the\\nbottom, making it easy for the audience to follow the flow of information. The contrasting\\nbackground colors and font colors enhance readability and focus on the content”. This\\nshowcases the remarkable visual ability of UFO , as it can complete open-ended requests\\nbased on its own aesthetic sense.\\nD.3 Downloading an Extension for VSCode\\nIn Figure 10, we present an example where UFO effectively fulfills a user request in Visual\\nStudio Code: “Download the Docker extension in the repo.”. Although this is a relatively\\nsimple task, we aimed to test UFO ’s performance in operating on a dark mode GUI.\\nRemarkably, UFO completes this request effortlessly by clicking the Extension button,\\ninputting “Docker” into the search box, and accurately clicking the “Install” button for the\\nDocker Extension. With just one click, the task is successfully completed. This showcases\\nUFO ’s capability to operate on less popular applications, even when they are in dark mode.\\nD.4 Post a Twitter\\nIn Figure 11, we shift our focus to the Edge Web browser to assess UFO ’s capability in\\noperating over one of the most popular application types on Windows OS. Specifically, we\\n26\", metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 25}),\n",
       " Document(page_content='Download the Docker extension in the repo.\\nUser\\n UFO\\nFigure 10: UFO completes user request: “Download the Docker extension in the repo.”.\\nPost \"It\\'s a good day.\" on my Twitter. \\nUser\\n UFO\\nUser \\nConfirmation\\nFigure 11: UFO completes user request: “Post ‘It’s a good day.’ on my Twitter. ”.\\nrequest UFO to perform the task of “Post ‘It’s a good day.’ on my Twitter.”, a common yet\\nintricate request for an agent. The observation reveals UFO’s seamless execution, where it\\ninputs the Twitter address into the browser’s address bar, navigates to the link perceived\\nas the main Twitter page, identifies the Post button on the Twitter page, and clicks it. This\\ndecision, although not necessarily straightforward, allows UFO to input the required text\\nfor the tweet. The safeguard activates before sending, prompting user confirmation. The\\nsuccessful posting of the tweet demonstrates UFO ’s sophisticated ability to operate on web\\nbrowsers, a category of applications widely used on Windows OS.\\nD.5 Sending the News\\nIn a cross-application example, we illustrate how UFO can gather news from the Internet\\nand share it on WeChat in Figure 12. Specifically, we issue the command: “Search for and\\nread through the latest news about Microsoft and summarize it, then send it to ’File Transfer’\\non WeChat”. UFO adeptly inputs the query “latest news about Microsoft” into the Google\\nsearch bar on Edge Browser, initiates the search, and opens the first link in the search results.\\nThe page contains multiple news panes, and UFO skillfully summarizes the content using\\nits visual OCR ability, logging the information into its memory. Subsequently, UFO opens\\nWeChat, locates the designated chatbox, and inputs the summarized news retrieved from\\n27', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 26}),\n",
       " Document(page_content=\"User Confirmation\\nUFO\\nSearch for and read through the latest news about Microsoft and \\nsummarize it, then send it to 'File Transfer' on WeChat. \\nUserUser\\nFigure 12: UFO completes user request: “Search for and read through the latest news about\\nMicrosoft and summarize it, then send it to ’File Transfer’ on WeChat.”.\\nits memory. Upon user confirmation triggered by the safeguard, UFO sends the news. We\\nshow the news sent below.\\nThe Microsoft News page displays several articles. The headlines include: ’Excited\\nto announce the general availability of Copilot for Sales and Copilot for Service, as\\nwe continue to extend Copilot to every role and function.’, ’Microsoft announces\\nquarterly earnings release date’, ’With Copilot Pro, we’re helping even more people\\nsupercharge their creativity and productivity by unlocking Copilot in Microsoft 365\\napps, providing access to the very latest models 2014 and more.’, and ’Microsoft\\nunveils new generative AI and data solutions across the shopper journey, offering\\ncopilot experiences through Microsoft Cloud for Retail’.\\nThis example once again underscores UFO ’s ability to seamlessly transition between differ-\\nent applications, allowing it to effectively and safely complete long-term, complex tasks.\\nSuch demonstrations position UFO as an advanced and compelling agent for Windows OS.\\nD.6 Search Paper in a PowerPoint Slide and Summarize\\nLastly, in Figure 13, we present an intricate cross-application example with multiple sub-\\nrequests where UFO is tasked with finding a paper presented in a slide on the Internet,\\nsummarizing the paper, and downloading it. The user’s request is as follows: “(1) Read\\nthe paper.pptx, summarize the paper title presented on the slide, then search and open the\\npaper on the Internet. (2) Can you summarize this paper and download its PDF version for\\nme?” This request involves multiple instances of OCR usage by UFO.\\nTo fulfill the first sub-request, UFO carefully observes the title page of the slide, successfully\\nidentifies the paper titled ”Xpert: Empowering Incident Management with Query Recom-\\nmendations via Large Language Models.” Subsequently, it opens a Google page in the Edge\\n28\", metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 27}),\n",
       " Document(page_content=\"(1) Read the paper.pptx, summarize the paper title presented on the \\nslide, then search and open the paper on the Internet. \\nUser\\nUFO\\n(2) Can you summarize \\nthis paper and download \\nits PDF version for me?\\nUser\\nUFO\\nThe paper titled 'Xpert: Empowering Incident \\nManagement with Query Recommendations via \\nLarge Language Models' discusses the role of large -\\nscale cloud systems in modern IT infrastructure and \\nthe impact of incidents occurring within these \\nsystems. The paper introduces Xpert, a machine \\nlearning framework that automates Kusto Query \\nLanguage (KQL) recommendations, tailored to new \\nincidents. By leveraging historical incident data and \\nlarge language models, Xpert generates customized \\nKQL queries, enabling a thorough evaluation of query \\nquality from three comprehensive perspectives. The \\npaper details the deployment of Xpert in the real \\nproduction environment of a large -scale incident \\nmanagement system in Microsoft, demonstrating its \\neffectiveness in supporting incident management.Summary\\nFigure 13: UFO completes user request: “(1) Read the paper.pptx, summarize the paper\\ntitle presented on the slide, then search and open the paper on the Internet. (2) Can you\\nsummarize this paper and download its PDF version for me?”.\\nBrowser, inputs the title into the search bar, and locates the paper as the second result by\\nskipping the first ad link. UFO clicks and opens the link on arXiv, completing the first\\nrequest.\\nUpon receiving the second sub-request to summarize the paper and download the PDF\\nversion, UFO clicks the “Download as PDF” option and uses its OCR ability to generate the\\nfollowing summary:\\nThe paper titled ‘Xpert: Empowering Incident Management with Query Recommen-\\ndations via Large Language Models’ discusses the role of large-scale cloud systems in\\nmodern IT infrastructure and the impact of incidents occurring within these systems.\\nThe paper introduces Xpert, a machine learning framework that automates Kusto\\nQuery Language (KQL) recommendations, tailored to new incidents. By leveraging\\nhistorical incident data and large language models, Xpert generates customized KQL\\nqueries, enabling a thorough evaluation of query quality from three comprehensive\\nperspectives. The paper details the deployment of Xpert in the real production envi-\\nronment of a large-scale incident management system in Microsoft, demonstrating\\nits effectiveness in supporting incident management.\\n29\", metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 28}),\n",
       " Document(page_content='This summary aligns perfectly with the paper’s contribution as outlined in Jiang et al.\\n(2023), showcasing UFO ’s excellent ability to operate across different applications and its\\nremarkable OCR capabilities.\\n30', metadata={'source': 'https://arxiv.org/pdf/2402.07939v3.pdf', 'page': 29}),\n",
       " Document(page_content='Cleaner Pretraining Corpus Curation with Neural Web Scraping\\nZhipeng Xu1*, Zhenghao Liu1*, Yukun Yan2, Zhiyuan Liu2, Chenyan Xiong3and Ge Yu1\\n1Department of Computer Science and Technology, Northeastern University, China\\n2Department of Computer Science and Technology, Institute for AI, Tsinghua University, China\\nBeijing National Research Center for Information Science and Technology, China\\n3Language Technologies Institute, Carnegie Mellon University, United States\\nAbstract\\nThe web contains large-scale, diverse, and\\nabundant information to satisfy the information-\\nseeking needs of humans. Through meticulous\\ndata collection, preprocessing, and curation,\\nwebpages can be used as a fundamental data\\nresource for language model pretraining. How-\\never, when confronted with the progressively\\nrevolutionized and intricate nature of webpages,\\nrule-based/feature-based web scrapers are be-\\ncoming increasingly inadequate. This paper\\npresents a simple, fast, and effective Neural\\nwebScraper (NeuScraper ) to help extract pri-\\nmary and clean text contents from webpages.\\nExperimental results show that NeuScraper\\nsurpasses the baseline scrapers by achieving\\nmore than a 20% improvement, demonstrat-\\ning its potential in extracting higher-quality\\ndata to facilitate the language model pretrain-\\ning. All of the code is available at https:\\n//github.com/OpenMatch/NeuScraper .\\n1 Introduction\\nLarge Language Models (LLMs) have shown im-\\npressive performance in various NLP tasks as the\\nsize of models scaling up (Chowdhery et al., 2023;\\nTouvron et al., 2023; Achiam et al., 2023; Zhao\\net al., 2023). However, recent findings in scaling\\nlaws indicate that both model size and training data\\nshould be scaled proportionally (Hoffmann et al.,\\n2022), posing a significant challenge in acquiring\\nsufficiently large pretraining datasets or even rais-\\ning the concerns about data scarcity (Penedo et al.,\\n2023; Villalobos et al., 2022).\\nTo curate more data for pretraining, researchers\\npay more attention to collecting more valuable data\\nfrom the Web. The web-crawled datasets, such as\\nCommon Crawl, have been widely used for pre-\\ntraining, facilitating the development of language\\nmodels (Wenzek et al., 2020; Radford et al., 2019;\\n*indicates equal contribution.Raffel et al., 2020; Penedo et al., 2023). Neverthe-\\nless, prior research has demonstrated that, even af-\\nter aggressive cleaning, the quality of pre-extracted\\ntext provided by Common Crawl still fails to reach\\nthe expected (Raffel et al., 2020; Gao et al., 2021;\\nPenedo et al., 2023). The reason lies in that ad-\\nvertisements, banners, hyperlinks, and other harm-\\nful content are usually mixed within the primary\\ncontent of the page, thereby only extracting these\\nprimary contents brings lots of noise to pretrain-\\ning (Gibson et al., 2005; V ogels et al., 2018).\\nThe web scrapers provide opportunities to\\nextract valuable content from the raw HTML\\npages (Barbaresi, 2021). However, rule-based and\\nheuristic scrapers have notable limitations. On the\\none hand, web pages are becoming increasingly\\nsophisticated, requiring more intricate underlying\\ncode to deal with the page layout (Butkiewicz et al.,\\n2011). In this case, maintaining the scraper rules\\nis time-consuming and requires much human ef-\\nfort. On the other hand, HTML functions as a\\nmarkup language, enabling web designers to cus-\\ntomize web pages according to individual prefer-\\nences. Consequently, web pages frequently lack\\ncomplete standardization, which may mislead the\\nrule-based web scrapers (Hantke and Stock, 2022).\\nIn this paper, we present a simple, fast, and\\neffective Neural Web Scraper ( NeuScraper ) de-\\nsigned to extract primary content from webpages.\\nNewScraper employs a shallow neural architecture\\nand integrates layout information for efficient pars-\\ning. Our experiments demonstrate that NewScraper\\nsurpasses baseline scrapers, achieving a 20% im-\\nprovement in performance and generating a higher-\\nquality corpus for language model pretraining. No-\\ntably, NeuScraper shows potential of high process-\\ning speeds when utilized on GPU. The easy-to-use\\nand open-source tool, NeuScraper , can facilitate\\nthe creation of large-scale corpora for pretraining.arXiv:2402.14652v1  [cs.CL]  22 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'page': 0}),\n",
       " Document(page_content='<html><head><title>Macintosh-Wikipedia<body><div><span><h1>Macintosh-128K<span>From Wikipedia …<li><div>27 Languages<span><span>Article12345Textual Sequence Modeling of Webpages\\nMacinto…127 Lang…2Article3Macinto…4From…5……nWeb Scraping with Neural MethodNeuScraper…1102031415…n…\\nFigure 1: The Pipeline of Primary Content Extraction\\nUsing NeuScraper (Neural Web Scraper).\\n2 Related Work\\nLeveraging web scrapers for extraction provides\\na promising way to extract high-quality content\\nfrom webpages. Such a web scraping task is usu-\\nally defined as text extraction, boilerplate removal,\\ntemplate removal, or generic web extraction in dif-\\nferent webpage processing pipelines (Finn et al.,\\n2001; Rahman et al., 2001; Vieira et al., 2006),\\nwhich is distinguished from the web information\\nextraction task that extracts the entities from web-\\npages (Li et al., 2021; Wang et al., 2022). The\\nweb scrapers can be divided into rule-based and\\nfeature-based methods.\\nRule-based web scrapers start from web wrap-\\npers, which often need manual designs or a wrap-\\nper induction system for producing (Muslea et al.,\\n1999; Crescenzi et al., 2001). The web wrap-\\npers usually need to be tailored for each webpage,\\nwhich is not feasible to process large-scale web-\\npages (Guo et al., 2010). A more conventional\\napproach is to create a Document Object Model\\n(DOM) tree, which assists in building a rule-based\\nscraper (Gupta et al., 2003; Guo et al., 2010) or help\\nthe comparison of webpages (Yi et al., 2003). Addi-\\ntionally, the work also incorporates tag cumulative\\ndistributions (Finn et al., 2001), text density (Sun\\net al., 2011), and tag ratios (Weninger et al., 2010)\\nto benefit the content extraction from webpages.\\nExcept for these rule-based methods, some scrap-\\ners use feature-based approaches to better extract\\nthe primary contents from webpages. Specifically,they divide the webpage into several blocks us-\\ning rules that are built based on the HTML tags\\nor DOM tree. Then they extract dozens to hun-\\ndreds of hand-crafted features from these blocks,\\nsuch as markup, text/document features (Spousta\\net al., 2008), linguistic, structural & visual fea-\\ntures (Bauer et al., 2007) and DOM tree-based fea-\\ntures (V ogels et al., 2018). These features can be\\nfed into SVM (Bauer et al., 2007; Kohlschütter\\net al., 2010), conditional random fields (Spousta\\net al., 2008), logistic regressions (Peters and\\nLecocq, 2013) or convolutional neural network (V o-\\ngels et al., 2018) to classify whether the texts in the\\nblock are the primary content of the webpages.\\n3 Neural Web Scraper\\nThis section introduces our Neural Web Scraper\\n(NeuScraper ) to extract primary contents from\\nwebpages. We first introduce the sequence mod-\\neling method of webpages (Sec. 3.1) and then de-\\nscribe our neural-based web scraper (Sec. 3.2).\\n3.1 Textual Sequence Modeling of Webpages\\nAs shown in Figure 1, the primary content extrac-\\ntion task aims to extract the content from the high-\\nlighted areas, which consists of clean texts and\\nrepresents the main information of the webpage.\\nTo facilitate the web scraping with NeuScraper ,\\nwe convert the HTML code into textual sequences.\\nPrevious work (Bauer et al., 2007) has demon-\\nstrated the effectiveness of both structural and vi-\\nsual features in helping to identify primary con-\\ntents. Thus, to preserve webpage layout informa-\\ntion, we rely on the DOM tree structure to trans-\\nform webpages into textual sequences. Specifically,\\nwe employ the BeautifulSoup41toolkit to build the\\nDOM tree for each webpage, conduct the depth-\\nfirst traversal on the tree and regard the visited order\\nas additional location information to represent the\\nnodes. During this process, only the nodes that con-\\ntain plaint texts, table nodes (tagged with <table> ),\\nand list nodes (tagged with <ol> ,<ul> or<dl> )\\nare reserved to produce the final textual sequences\\nX={x1, x2, ..., x n}, where ndenotes the number\\nof the reserved DOM nodes. After processing, the\\nweb scraping task primarily involves determining\\nwhether the node xicontains the primary content\\nof the webpage for evaluation.\\n1https://pypi.org/project/beautifulsoup4/', metadata={'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'page': 1}),\n",
       " Document(page_content='3.2 Web Scraping with the Neural Method\\nIn this subsection, we introduce our neural model-\\ning method to build the web scraper. To process the\\ntextual sequences X={x1, x2, ..., x n}, we build a\\nhierarchical architecture for node-level prediction.\\nSpecifically, to guarantee the efficiency of\\nNeuScraper , we use the first layer of the XLM-\\nRoberta (Conneau et al., 2020) model to encode\\nthe text representation xiof the i-th DOM node as\\nthe 768-dimensional node representation hi:\\nhi=XLMRoberta-Layer1(xi), (1)\\nwhere hiis the representation of the “[CLS]” to-\\nken. Then we feed these node representations\\nH={h1, h2, ..., h n}into a 3-layer transformer\\nmodel (Vaswani et al., 2017) with 8 attention heads\\nto get the encoded node representations:\\nˆhi=Transformer (Linear (hi)), (2)\\nwhere the linear layer projects hito 256-\\ndimensional embeddings for efficient modeling.\\nFollowing previous work (Overwijk et al., 2022),\\nthe DOM nodes can be categorized into six kinds\\nof labels yk, including title, primary content, head-\\ning, paragraph, table and list. Then we calculate\\nthe label prediction probability P(yk\\ni= 1|xi)of\\nthek-th category label yk\\niof the i-th node:\\nP(yk\\ni= 1|xi) =Sigmoid (MLP(ˆhi)) (3)\\nFinally, NeuScraper is trained using the loss L:\\nL=6X\\nk=1nX\\ni=1CrossEntropy (P(yk\\ni|xi),Yk\\ni), (4)\\nwhere Yk\\niis the ground truth label. Yk\\niis a binary\\nlabel and Yk\\ni= 1 indicates that the i-th DOM\\nnode belongs to the k-th label category. During\\ninference, we only consider the primary content\\nlabel to extract the texts from webpages.\\n4 Experimental Methodology\\nIn this section, we describe the datasets, baselines,\\nevaluation metrics and implementation details.\\nDataset. We use ClueWeb22 (Overwijk et al.,\\n2022) dataset in experiments. The content extrac-\\ntion labels of ClueWeb22 were generated from the\\nproduction system of a commercial search engine.\\nThe labels are not available for general web scrap-\\ning tools, because they are annotated with more\\nexpensive signals of page rendering and visualiza-\\ntion. We used 8.28 million pages for training andMethodEvaluation Metrics Latency\\nAcc. Prec. Rec. F1 (ms)\\nhtmlparser 40.94 40.92 98.95 57.90 16.42\\nbs4 41.07 41.05 99.94 58.20 10.16\\nhtml2text 40.09 39.40 85.40 53.92 13.13\\nboilerpipe 66.28 66.89 35.52 46.40 8.77\\njusText 62.67 72.49 27.06 39.41 8.37\\nlxml 65.45 61.54 37.82 46.84 8.24\\ninscriptis 45.06 42.53 96.43 59.03 12.08\\nreadability 68.26 72.08 37.01 48.91 11.25\\ntrafilatura 70.57 66.60 56.77 61.30 9.95\\nNeuScraper 86.66 81.15 88.30 84.58 6.18\\nTable 1: Overall Performance. We use ClueWeb22 to\\nevaluate the content extraction effectiveness of different\\nweb scrapers. More details are shown in Appendix A.2.\\n19,013 English pages for evaluation. More details\\nare shown in Appendix A.2.\\nBaseline. The baselines consists of\\nnine open-sourced scrapers, including ba-\\nsic HTML manipulators ( html2text and\\ninscriptis (Weichselbraun, 2021)), generic\\nwebpage parsers ( beautifulsoup4 ,lxml and\\nhtmlparser ), rule-based scrapers ( jusText\\nand readability ) and machine learning-based\\nscraper ( boilerpipe (Kohlschütter et al., 2010)).\\ntrafilatura (Barbaresi, 2021) is our main\\nbaseline, which combines different rules and\\nheuristic methods.\\nEvaluation Metrics. The accuracy, precision,\\nrecall, and F1, are used to evaluate the effectiveness\\nin extracting primary contents. Besides, we follow\\nthe Pythia (Biderman et al., 2023) and evaluate the\\neffectiveness of pretrained language models trained\\nwith the web data using 8 standard tasks.\\nImplementation Details. NeuScraper is\\ntrained for 30 epochs using the AdamW optimizer\\nwith batch size=1024. Learning rate adjustments\\nfollowed a cosine decay schedule, with a warm-up\\nphase spanning the initial 5% of iterations and a\\npeak rate fixed at 6e-4. To accommodate memory\\nand computational speed limitations, the maximum\\nlength of node sequences was truncated to 384.\\n5 Evaluation Result\\nIn this section, we first show the effectiveness of\\ndifferent scrapers in extracting primary contents\\nfrom the raw HTML pages. Subsequently, we eval-\\nuate the quality of the extracted data and utilize it\\nto pretrain language models of varying scales.\\n5.1 Overall Performance\\nThe effectiveness of rule-based scrapers and our\\nNeuScraper in extracting primary contents from', metadata={'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'page': 2}),\n",
       " Document(page_content='Size Method BLIMP ARC-e ARC-c SWAG WinoG SciQ Lambada LogiQA A VG\\n70Mhtmlparser 64.80 34.30 16.80 30.20 51.85 58.60 12.41 24.42 36.67\\ntrafilatura 67.66 36.11 18.08 31.37 51.88 63.50 13.85 22.58 38.13\\nNeuScraper 67.90 37.54 17.40 31.16 49.72 63.90 13.66 24.73 38.25\\n160Mhtmlparser 70.87 41.16 17.23 32.24 49.88 66.10 16.96 22.58 39.63\\ntrafilatura 73.46 42.46 18.25 34.08 48.61 69.20 18.10 22.11 40.78\\nNeuScraper 74.01 42.84 18.43 34.14 51.46 69.00 17.58 21.50 41.12\\n410Mhtmlparser 74.24 42.63 18.77 34.45 49.80 70.80 22.35 22.42 41.93\\ntrafilatura 77.84 45.28 20.56 37.29 52.32 72.90 23.77 21.96 43.99\\nNeuScraper 76.71 47.34 20.47 37.00 50.74 74.40 26.76 24.42 44.73\\n1Bhtmlparser 76.41 46.59 18.68 36.78 52.40 73.60 27.03 19.04 43.82\\ntrafilatura 79.83 48.19 20.90 39.95 51.53 77.90 31.26 21.35 46.36\\nNeuScraper 78.70 50.16 20.90 39.19 50.67 79.30 31.88 22.58 46.67\\nTable 2: Effectiveness of Pythia Pretraining Using the Extracted Data from Different Scrapers.\\nthe raw HTML pages is shown in Table 1.\\nAmong all baseline models, the trafilatura\\nscraper exhibits the highest performance, showcas-\\ning its effectiveness in content extraction through\\nits cascade of rule-based filters and content heuris-\\ntic methods. Our NeuScraper surpasses all tradi-\\ntional rule-based web scrapers and achieves over a\\n20% improvement. It illustrates the effectiveness of\\nourNeuScraper in learning the schemes of the pri-\\nmary contents, generalizing its advances to handle\\nvarious HTML layouts and extracting high-quality\\ntexts from raw pages. Notably, with the GPU sup-\\nport and distributed computation, NeuScraper sig-\\nnificantly reduces the scraper latency.\\n5.2 Effectiveness of the Cleaned Web Data in\\nLanguage Model Pretraining\\nThis part evaluates the effectiveness of language\\nmodels pretrained on the web data.\\nAs shown in Table 2, we utilize different scrapers\\nto handle the webpages sourced from ClueWeb22\\nand leverage the extracted data to pretrain Pythia\\nmodels of varying scales (Biderman et al., 2023).\\nThe evaluation results demonstrate that employing\\ntheNeuScraper for webpage processing enhances\\nthe performance of language models in downstream\\ntasks. It is noteworthy that the NeuScraper repre-\\nsents a data-driven scraping approach, circumvent-\\ning the need for building sophisticated rules and\\nconducting intricate feature engineering to deal\\nwith the continuously evolving HTML layouts.\\n5.3 Evaluation on the Quality of Extracted\\nData Using NeuScraper\\nIn this subsection, we aim to estimate the quality of\\nextracted data using NeuScraper . The evaluation\\nresults are shown in Figure 2.\\nIt is apparent that if two corpora are of com-\\nparable quality, their n-gram distributions should\\n(a) 410M Models.\\n (b) 1B Models.\\nFigure 2: The Effectiveness of Language Models\\nTrained on Web Data to Reproduce the Target Corpora.\\nexhibit similarity. Thus, we use the language mod-\\nels pretrained on web data (the same as Sec. 5.2) to\\nask these language models to reproduce the target\\ncorpora, such as Wikitext (Merity et al., 2017) and\\nLambada (Radford et al., 2019). The perplexity is\\nused to evaluate the effectiveness of the language\\nmodels pretrained on web data in replicating the\\ntarget corpora. The lower perplexity indicates the\\nlanguage model is more proficient to the target cor-\\npora, showing the pretrained data and target data\\nhave more overlaps and are more similar.\\nThe evaluation results reveal that the utilization\\nof extracted content from some simple scrapers,\\nsuch as htmlparser , significantly impacts the ef-\\nfectiveness of language models, which causes an\\nincrease of more than 20 points in perplexity due to\\nthe noise derived from webpages. Compared with\\nthetrafilatura scraper, NeuScraper decreases\\nthe perplexity by over 4 points, showing its capa-\\nbility to yield higher-quality data for pretraining\\nthrough learning to extract primary content.\\n6 Conlusion\\nThis paper proposes NeuScraper (Neural web\\nScraper ), which employs a shallow neural archi-', metadata={'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'page': 3}),\n",
       " Document(page_content='tecture to clean the webpages. The experimental\\nresults show the effectiveness of NeuScraper . The\\nopen-sourced and easy-used web scraper may facil-\\nitate the research on language model pretraining.\\nLimitation\\nTo guarantee efficiency, NeuScraper needs the\\npowerful parallelism of GPUs to achieve high-\\nspeed web scraping. In addition, for large-scale\\npretraining corpus processing, a high throughput\\nstorage medium is required to ensure inference effi-\\nciency due to the frequent data swapping between\\nthe storage medium and GPU.\\nReferences\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\\nArXiv preprint .\\nAdrien Barbaresi. 2021. Trafilatura: A web scraping\\nlibrary and command-line tool for text discovery and\\nextraction. In Proceedings of ACL , pages 122–131.\\nDaniel Bauer, Judith Degen, Xiaoye Deng, Priska\\nHerger, Jan Gasthaus, Eugenie Giesbrecht, Lina\\nJansen, Christin Kalina, Thorben Kräger, Robert\\nMärtin, et al. 2007. Fiasco: Filtering the internet\\nby automatic subtree classification, osnabruck. In\\nBuilding and Exploring Web Corpora: Proceedings\\nof the 3rd Web as Corpus Workshop, incorporating\\nCleanEval , pages 111–121.\\nStella Biderman, Hailey Schoelkopf, Quentin Gregory\\nAnthony, Herbie Bradley, Kyle O’Brien, Eric Hal-\\nlahan, Mohammad Aflah Khan, Shivanshu Purohit,\\nUSVSN Sai Prashanth, Edward Raff, et al. 2023.\\nPythia: A suite for analyzing large language mod-\\nels across training and scaling. In International\\nConference on Machine Learning , pages 2397–2430.\\nPMLR.\\nMichael Butkiewicz, Harsha V Madhyastha, and Vyas\\nSekar. 2011. Understanding website complexity:\\nmeasurements, metrics, and implications. In Pro-\\nceedings of the 2011 ACM SIGCOMM conference on\\nInternet measurement conference , pages 313–328.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\\ntian Gehrmann, et al. 2023. Palm: Scaling language\\nmodeling with pathways. Journal of Machine Learn-\\ning Research , (240):1–113.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\\nVishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\\nmoyer, and Veselin Stoyanov. 2020. Unsupervisedcross-lingual representation learning at scale. In Pro-\\nceedings of ACL , pages 8440–8451.\\nValter Crescenzi, Giansalvatore Mecca, Paolo Merialdo,\\net al. 2001. Roadrunner: Towards automatic data\\nextraction from large web sites. In VLDB , pages\\n109–118.\\nAidan Finn, Nicholas Kushmerick, and Barry Smyth.\\n2001. Fact or fiction: Content classification for digi-\\ntal libraries. In DELOS .\\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\\nrace He, Anish Thite, Noa Nabeshima, et al. 2021.\\nThe pile: An 800gb dataset of diverse text for lan-\\nguage modeling. ArXiv preprint .\\nDavid Gibson, Kunal Punera, and Andrew Tomkins.\\n2005. The volume and evolution of web page tem-\\nplates. In Special interest tracks and posters of the\\n14th international conference on World Wide Web ,\\npages 830–839.\\nYan Guo, Huifeng Tang, Linhai Song, Yu Wang, and\\nGuodong Ding. 2010. Econ: an approach to extract\\ncontent from web news page. In 2010 12th Interna-\\ntional Asia-Pacific Web Conference , pages 314–320.\\nIEEE.\\nSuhit Gupta, Gail E. Kaiser, David Neistadt, and Pe-\\nter Grimm. 2003. Dom-based content extraction of\\nHTML documents. In Proceedings of the Twelfth\\nInternational World Wide Web Conference, WWW\\n2003, Budapest, Hungary, May 20-24, 2003 , pages\\n207–214.\\nFlorian Hantke and Ben Stock. 2022. Html violations\\nand where to find them: a longitudinal analysis of\\nspecification violations in html. In Proceedings of\\nthe 22nd ACM Internet Measurement Conference ,\\npages 358–373.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\\nWelbl, Aidan Clark, et al. 2022. Training compute-\\noptimal large language models. ArXiv preprint .\\nChristian Kohlschütter, Peter Fankhauser, and Wolfgang\\nNejdl. 2010. Boilerplate detection using shallow text\\nfeatures. In Proceedings of the Third International\\nConference on Web Search and Web Data Mining,\\nWSDM 2010, New York, NY, USA, February 4-6,\\n2010 , pages 441–450.\\nJunlong Li, Yiheng Xu, Lei Cui, and Furu Wei. 2021.\\nMarkuplm: Pre-training of text and markup language\\nfor visually-rich document understanding. ArXiv\\npreprint .\\nStephen Merity, Caiming Xiong, James Bradbury, and\\nRichard Socher. 2017. Pointer sentinel mixture mod-\\nels. In Proceedings of ICLR .', metadata={'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'page': 4}),\n",
       " Document(page_content='Ion Muslea, Steve Minton, and Craig Knoblock. 1999.\\nA hierarchical approach to wrapper induction. In\\nProceedings of the third annual conference on Au-\\ntonomous Agents , pages 190–197.\\nArnold Overwijk, Chenyan Xiong, Xiao Liu, Cameron\\nVandenBerg, and Jamie Callan. 2022. Clueweb22:\\n10 billion web documents with visual and semantic\\ninformation. ArXiv preprint .\\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\\nand Julien Launay. 2023. The refinedweb dataset for\\nfalcon llm: outperforming curated corpora with web\\ndata, and web data only. ArXiv preprint .\\nMatthew E Peters and Dan Lecocq. 2013. Content\\nextraction using diverse feature sets. In Proceedings\\nof the 22nd international conference on world wide\\nweb, pages 89–90.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, Ilya Sutskever, et al. 2019. Language\\nmodels are unsupervised multitask learners.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. The Journal of Machine Learning Research ,\\n(1):5485–5551.\\nAFR Rahman, H Alam, R Hartono, et al. 2001. Content\\nextraction from html documents. In 1st Int. Workshop\\non Web Document Analysis (WDA2001) , pages 1–4.\\nMiroslav Spousta, Michal Marek, and Pavel Pecina.\\n2008. Victor: the web-page cleaning tool. In 4th Web\\nas Corpus Workshop (WAC4)-Can we beat Google ,\\npages 12–17.\\nFei Sun, Dandan Song, and Lejian Liao. 2011. DOM\\nbased content extraction via text density. In Proceed-\\nings of SIGIR , pages 245–254.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al. 2023. Llama: Open and efficient foun-\\ndation language models. ArXiv preprint .\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Proceedings of NeurIPS , pages 5998–\\n6008.\\nKarane Vieira, Altigran S Da Silva, Nick Pinto,\\nEdleno S De Moura, Joao MB Cavalcanti, and Ju-\\nliana Freire. 2006. A fast and robust method for web\\npage template detection and removal. In Proceedings\\nof the 15th ACM international conference on Infor-\\nmation and knowledge management , pages 258–267.Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay\\nBesiroglu, Marius Hobbhahn, and Anson Ho. 2022.\\nWill we run out of data? an analysis of the limits of\\nscaling datasets in machine learning. ArXiv preprint .\\nThijs V ogels, Octavian-Eugen Ganea, and Carsten Eick-\\nhoff. 2018. Web2text: Deep structured boilerplate\\nremoval. In Advances in Information Retrieval: 40th\\nEuropean Conference on IR Research, ECIR 2018,\\nGrenoble, France, March 26-29, 2018, Proceedings\\n40, pages 167–179. Springer.\\nQifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiao-\\njun Quan, and Dongfang Liu. 2022. Webformer: The\\nweb-page transformer for structure information ex-\\ntraction. In Proceedings of the ACM Web Conference\\n2022 , pages 3124–3133.\\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel\\nBowman. 2020. Blimp: The benchmark of linguistic\\nminimal pairs for english. Transactions of the Asso-\\nciation for Computational Linguistics , 8:377–392.\\nAlbert Weichselbraun. 2021. Inscriptis–a python-based\\nhtml to text conversion library optimized for knowl-\\nedge extraction from the web. ArXiv preprint .\\nTim Weninger, William H. Hsu, and Jiawei Han. 2010.\\nCETR: content extraction via tag ratios. In Proceed-\\nings of the 19th International Conference on World\\nWide Web, WWW 2010, Raleigh, North Carolina,\\nUSA, April 26-30, 2010 , pages 971–980.\\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\\nmand Joulin, and Edouard Grave. 2020. CCNet: Ex-\\ntracting high quality monolingual datasets from web\\ncrawl data. In Proceedings of the 12th Language\\nResources and Evaluation Conference , pages 4003–\\n4012.\\nLan Yi, Bing Liu, and Xiaoli Li. 2003. Eliminating\\nnoisy information in web pages for data mining. In\\nProceedings of the ninth ACM SIGKDD international\\nconference on Knowledge discovery and data mining ,\\npages 296–305.\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\\nChoi. 2018. Swag: A large-scale adversarial dataset\\nfor grounded commonsense inference. In Proceed-\\nings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing , pages 93–104.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\\nsurvey of large language models. ArXiv preprint .', metadata={'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'page': 5}),\n",
       " Document(page_content='A Appendix\\nA.1 License\\nClueWeb22 shows its terms of use at website2. All\\nof these licenses and agreements allow their data\\nfor academic use.\\nA.2 More Experimental Details of Overall\\nEvaluation\\nIn this subsection, we describe further details of\\nthe implementation of overall evaluation.\\nDataset. We randomly selected about 8.28 mil-\\nlion webpages from ClueWeb22-B as the training\\nset. To evaluate content extraction performance, we\\nutilized a snapshot extracted from ClueWeb22-B,\\nidentified as en0001-01 . This particular snapshot\\ncomprises 19,013 English web pages along with\\ntheir respective annotations. Notably, it’s imper-\\native to highlight that en0001-01 was excluded\\nfrom both the training and validation datasets.\\nMetrics. In our experiments, we convert the web\\nscanning task into a binary classification problem,\\nso we can compute relevant metrics at the node\\nlevel. However, some previous web scrapers would\\ndirectly return the primary content without node\\ninformation. Therefore, we directly check whether\\nthe reserved plain text contains the text spans of\\nDOM tree nodes, which are annotated as ground\\ntruths in the benchmark. Latency is obtained by\\ncalculating the average processing time of 19,013\\npages.\\nComputing Platform. We conducted the train-\\ning of NeuScraper on a server equipped with 8\\nNVIDIA A100-40G GPUs, with the training pro-\\ncess spanning approximately 40 hours. For the\\nevaluation of baseline scrapers, we utilized a setup\\ncomprising 2 ×Intel Xeon Gold-6348@2.60GHz\\nCPUs with multiprocessing. In contrast, the evalu-\\nation of NeuScraper was carried out using a single\\nNVIDIA A100 40 GB GPU, employing an infer-\\nence batch size of 256.\\nA.3 More Experimental Details on Using\\nCleaned Web Data for Language Model\\nPretraining\\nIn this subsection, we describe additional details of\\nthe evaluation of the effectiveness of the cleaned\\nweb data in language model pretraining.\\nPretraining Corpus. We continue to utilize\\nClueWeb22-B as the source corpus for our pretrain-\\ning endeavors. To be specific, we employ various\\n2https://lemurproject.org/clueweb22/scrapers to acquire the corpus while ensuring an\\nequivalent number of tokens, thereby pretraining\\nthe language model to mirror the performance of\\neach scraper. For different sizes of Pythia models,\\nthe corpus size consistently remains fixed at 13\\nbillion tokens.\\nPretraining Details. Our pretraining framework\\nextends from the Lit-GPT3and we evaluate the\\nperformance of pretrained models using the stan-\\ndard lm-evaluation-harness toolkit4. Specifically,\\nfor all Pythia models, we employed the AdamW\\noptimizer with a peak learning rate in line with\\nBiderman et al. (2023). The total batch size was\\nset to 480 (with the batch size of 12 per GPU and\\ngradient accumulation being set to 10). We adopt\\nthe infinite pretraining strategy, i.e., only one epoch\\nis trained to ensure that the model reads all data\\nonly once. All of the models were trained on 4 ×\\nNIVIDIA A100-40G GPUs.\\nDatasets for Evaluation. We selected 8 stan-\\ndard datasets to evaluate the performance of pre-\\ntrained language models. Some of them are from\\nthe Pythia standard benchmark (Biderman et al.,\\n2023), supplemented by SWAG (Zellers et al.,\\n2018) and BLIMP (Warstadt et al., 2020).\\nBaselines. In this experiments, we chose to use\\nhtmlparser5andtrafilatura (Barbaresi, 2021)\\nas the main baselines for comparison. htmlparser\\nserves as the text pre-extraction tool for Common-\\nCrawl6WET file (containing pre-extracted text),\\nwhile trafilatura has become the state-of-the-art\\nweb scraper.\\nA.4 Case Study\\nIn this subsection, we show additional case stud-\\nies of NeuScraper andtrafilatura , our neural\\nweb scraper and a previously state-of-the-art web\\nscraper.\\nWe first analyze the case in Figure 3, where\\nwe use red boxes to indicate the content extracted\\nby the scraper. This is a college course page that\\ncontains some expertise in electrical engineering.\\nWhen scraping this page, trafilatura loses a lot\\nof textual content compared to our NeuScraper .\\nBy checking the raw HTML code, we found that\\nthere is an error caused by insufficient standardiza-\\ntion of web pages: the paragraph tag “ <p>” is used\\n3https://github.com/Lightning-AI/lit-gpt\\n4https://github.com/EleutherAI/\\nlm-evaluation-harness\\n5https://htmlparser.sourceforge.net\\n6https://commoncrawl.org/overview', metadata={'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'page': 6}),\n",
       " Document(page_content='for headings on this page instead of the standard\\n“<h>” tag. This page is readable for humans, but\\nthe HTML tag conveys an error that seriously af-\\nfects the extraction performance of trafilatura .\\nIn contrast, our NeuScraper shows great adapt-\\nability. It not only extracts most of the paragraph\\ncontent, but also removes useless information such\\nas phone numbers, e-mails, dates, and so on.\\nAnother typical case is interleaved boilerplate\\nand body text, as shown in Figure 4. We use\\nblue boxes to indicate the content extracted by\\nthe scraper. In this case, the boilerplate and body\\ntext are written in the same way. The boilerplate\\nalso uses “ <h>” to identify headings and “ <p>”\\nfor paragraphs, instead of the list surrounded by\\n“<li> ” in most cases. Recognizing it is difficult for\\ntrafilatura .NeuScraper leverages its ability to\\nrecognize latent semantic information to success-\\nfully remove the boilerplate in such pages.', metadata={'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'page': 7}),\n",
       " Document(page_content='(a) Trafilatura.\\n(b) NeuScraper.\\nFigure 3: Case#1 of the Primary Content Extraction Results Using Different Scrapers. The extracted parts are\\nhighlighted with red boxes.', metadata={'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'page': 8}),\n",
       " Document(page_content='(a) Trafilatura.\\n(b) NeuScraper.\\nFigure 4: Case#2 of the Primary Content Extraction Results Using Different Scrapers. The extracted parts are\\nhighlighted with blue boxes.', metadata={'source': 'https://arxiv.org/pdf/2402.14652v1.pdf', 'page': 9}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\nZeyu Lu1 2 *Zidong Wang1 3 *Di Huang1 4Chengyue Wu5Xihui Liu5Wanli Ouyang1Lei Bai1\\nFigure 1: Selected samples from FiT-XL/2 models at resolutions of 256×256,224×448and448×224. FiT is capable of\\ngenerating images at unrestricted resolutions and aspect ratios.\\nAbstract\\nNature is infinitely resolution-free . In the con-\\ntext of this reality, existing diffusion models, such\\nas Diffusion Transformers, often face challenges\\nwhen processing image resolutions outside of\\ntheir trained domain. To overcome this limita-\\ntion, we present the Flexible Vision Transformer\\n(FiT), a transformer architecture specifically de-\\nsigned for generating images with unrestricted\\nresolutions and aspect ratios . Unlike traditional\\nmethods that perceive images as static-resolution\\n*Equal contribution1Shanghai Artificial Intelligence Labora-\\ntory2Shanghai Jiao Tong University3Tsinghua University4Sydney\\nUniversity5The University of Hong Kong. Correspondence to:\\nLei Bai <baisanshi@gmail.com >.\\nWork in progess.grids, FiT conceptualizes images as sequences of\\ndynamically-sized tokens. This perspective en-\\nables a flexible training strategy that effortlessly\\nadapts to diverse aspect ratios during both train-\\ning and inference phases, thus promoting reso-\\nlution generalization and eliminating biases in-\\nduced by image cropping. Enhanced by a metic-\\nulously adjusted network structure and the inte-\\ngration of training-free extrapolation techniques,\\nFiT exhibits remarkable flexibility in resolution\\nextrapolation generation. Comprehensive experi-\\nments demonstrate the exceptional performance\\nof FiT across a broad range of resolutions, show-\\ncasing its effectiveness both within and beyond\\nits training resolution distribution. Repository\\navailable at https://github.com/whlzy/FiT.\\n1arXiv:2402.12376v1  [cs.CV]  19 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 0}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\n1. Introduction\\nCurrent image generation models struggle with generaliz-\\ning across arbitrary resolutions. The Diffusion Transformer\\n(DiT) (Peebles & Xie, 2023) family, while excelling within\\ncertain resolution ranges, falls short when dealing with im-\\nages of varying resolutions. This limitation stems from\\nthe fact that DiT can not utilize dynamic resolution images\\nduring its training process, hindering its ability to adapt to\\ndifferent token lengths or resolutions effectively.\\nTo overcome this limitation, we introduce the Flexible Vi-\\nsion Transformer (FiT) , which is adept at generating im-\\nages at unrestricted resolutions and aspect ratios . The key\\nmotivation is a novel perspective on image data modeling:\\nrather than treating images as static grids of fixed dimen-\\nsions, FiT conceptualizes images as sequences of variable-\\nlength tokens. This approach allows FiT to dynamically\\nadjust the sequence length, thereby facilitating the gener-\\nation of images at any desired resolution without being\\nconstrained by pre-defined dimensions. By efficiently man-\\naging variable-length token sequences and padding them\\nto a maximum specified length, FiT unlocks the potential\\nfor resolution-independent image generation. FiT repre-\\nsents this paradigm shift through significant advancements\\ninflexible training pipeline ,network architecture , and\\ninference processes .\\nFlexible Training Pipeline. FiT uniquely preserves the\\noriginal image aspect ratio during training, by viewing the\\nimage as a sequence of tokens. This unique perspective\\nallows FiT to adaptively resize high-resolution images to\\nfit within a predefined maximum token limit, ensuring that\\nno image, regardless of its original resolution, is cropped\\nor disproportionately scaled. This method ensures that the\\nintegrity of the image resolution is maintained, as shown\\nin Figure 2, facilitating the ability to generate high-fidelity\\nimages at various resolutions. To the best of our knowl-\\nedge, FiT is the first transformer-based generation model to\\nmaintain diverse image resolutions throughout training.\\nNetwork Architecture. The FiT model evolves from the\\nDiT architecture but addresses its limitations in resolution\\nextrapolation. One essential network architecture adjust-\\nment to handle diverse image sizes is the adoption of 2D Ro-\\ntary Positional Embedding (RoPE) (Su et al., 2024), inspired\\nby its success in enhancing large language models (LLMs)\\nfor length extrapolation (Liu et al., 2023). We also intro-\\nduce Swish-Gated Linear Unit (SwiGLU) (Shazeer, 2020)\\nin place of the traditional Multilayer Perceptron (MLP)\\nand replace DiT’s Multi-Head Self-Attention (MHSA) with\\nMasked MHSA to efficiently manage padding tokens within\\nour flexible training pipeline.\\nInference Process. While large language models employ\\ntoken length extrapolation techniques (Peng et al., 2023;\\nFixed Resolution\\nFlexible ResolutionDiTFiT\\nCenter Crop\\nResize\\nResize\\n(a)\\n(b)Data PreprocessGenerateFigure 2: Pipeline comparison between (a) DiT and (b) FiT.\\nLocalLLaMA) for generating text of arbitrary lengths, a\\ndirect application of these technologies to FiT yields sub-\\noptimal results. We tailor these techniques for 2D RoPE,\\nthereby enhancing FiT’s performance across a spectrum of\\nresolutions and aspect ratios.\\nOur highest Gflop FiT-XL/2 model, after training for only\\n1.8 million steps on ImageNet-256 (Deng et al., 2009)\\ndataset, outperforms all state-of-the-art CNN and trans-\\nformer models by a significant margin across resolutions of\\n160×320,128×384,320×320,224×448, and160×480.\\nThe performance of FiT-XL/2 significantly advances further\\nwith our training-free resolution extrapolation method. Com-\\npared to the baseline DiT-XL/2 training for 7 million steps,\\nFiT-XL/2 lags slightly at the resolution of 256×256but\\nsignificantly surpasses it at all other resolutions.\\nIn summary, our contributions lie in the novel introduction\\nof FiT, a flexible vision transformer tailored for diffusion\\nmodels, capable of generating images at any resolution and\\naspect ratio. We present three innovative design features in\\nFiT, including a flexible training pipeline that eliminates the\\nneed for cropping, a unique transformer architecture for dy-\\nnamic token length modeling, and a training-free resolution\\nextrapolation method for arbitrary resolution generation.\\nStrict experiments demonstrate that the FiT-XL/2 model\\nachieves state-of-the-art performance across a variety of\\nresolution and aspect ratio settings.\\n2. Related Work\\nDiffusion Models. Denoising Diffusion Probabilistic Mod-\\nels (DDPMs) (Ho et al., 2020; Saharia et al., 2022; Rad-\\nford et al., 2021) and score-based models (Hyv ¨arinen &\\nDayan, 2005; Song et al., 2020b) have exhibited remark-\\nable progress in the context of image generation tasks. The\\nDenoising Diffusion Implicit Model (DDIM) Song et al.\\n(2020a), offers An accelerated sampling procedure. Latent\\nDiffusion Models (LDMs) (Rombach et al., 2022) estab-\\nlishes a new benchmark of training deep generative models\\nto reverse a noise process in the latent space, through the\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 1}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\nuse of VQ-V AE (Esser et al., 2021).\\nTransformer Models. The Transformer model (Vaswani\\net al., 2017), has successfully supplanted domain-specific\\narchitectures in a variety of fields including, but not limited\\nto, language (Brown et al., 2020; Chowdhery et al., 2023a),\\nvision (Dosovitskiy et al., 2020), and multi-modality (Team\\net al., 2023). In vision perception research, most ef-\\nforts (Touvron et al., 2019; 2021; Liu et al., 2021; 2022)\\nthat focus on resolution are aimed at accelerating pretraining\\nusing a fixed, low resolution. On the other hand, NaViT (De-\\nhghani et al., 2023) implements the ’Patch n’ Pack’ tech-\\nnique to train ViT using images at their natural, ’native’\\nresolution. Notably, transformers have been also explored\\nin the denoising diffusion probabilistic models (Ho et al.,\\n2020) to synthesize images. DiT (Peebles & Xie, 2023)\\nis the seminal work that utilizes a vision transformer as\\nthe backbone of LDMs and can serve as a strong baseline.\\nBased on DiT architecture, MDT (Gao et al., 2023) intro-\\nduces a masked latent modeling approach, which requires\\ntwo forward-runs in training and inference. U-ViT (Bao\\net al., 2023) treats all inputs as tokens and incorporates\\nU-Net architectures into the ViT backbone of LDMs. Dif-\\nfiT (Hatamizadeh et al., 2023) introduces a time-dependent\\nself-attention module into the DiT backbone to adapt to dif-\\nferent stages of the diffusion process. We follow the LDM\\nparadigm of the above methods and further propose a novel\\nflexible image synthesis pipeline.\\nLength Extrapolation in LLMs. RoPE (Rotary Position\\nEmbedding) (Su et al., 2024) is a novel positional embed-\\nding that incorporates relative position information into\\nabsolute positional embedding. It has recently become the\\ndominant positional embedding in a wide range of LLM\\n(Large Language Model) designs (Chowdhery et al., 2023b;\\nTouvron et al., 2023a;b). Although RoPE enjoys valuable\\nproperties, such as the flexibility of sequence length, its per-\\nformance drops when the input sequence surpasses the train-\\ning length. Many approaches have been proposed to solve\\nthis issue. PI (Position Interpolation) (Chen et al., 2023)\\nlinearly down-scales the input position indices to match the\\noriginal context window size, while NTK-aware (LocalL-\\nLaMA) changes the rotary base of RoPE. YaRN (Yet another\\nRoPE extensioN) (Peng et al., 2023) is an improved method\\nto efficiently extend the context window. RandomPE (Ruoss\\net al., 2023) sub-samples an ordered set of positions from\\na much larger range of positions than originally observed\\nin training or inference. xPos (Sun et al., 2022) incorpo-\\nrates long-term decay into RoPE and uses blockwise causal\\nattention for better extrapolation performance. Our work\\ndelves deeply into the implementation of RoPE in vision\\ngeneration and on-the-fly resolution extrapolation methods.3. Flexible Vision Transformer for Diffusion\\n3.1. Preliminary\\n1-D RoPE (Rotary Positional Embedding) (Su et al., 2024)\\nis a type of position embedding that unifies absolute and\\nrelative PE, exhibiting a certain degree of extrapolation\\ncapability in LLMs. Given the m-th key and n-th query\\nvector as qm,kn∈R|D|, 1-D RoPE multiplies the bias to\\nthe key or query vector in the complex vector space:\\nfq(qm, m) =eimΘqm, f k(kn, n) =einΘkn(1)\\nwhere Θ = Diag( θ1,···, θ|D|/2)is rotary frequency matrix\\nwithθd=b−2d/|D|and rotary base b= 10000 . In real\\nspace, given l=|D|/2, the rotary matrix eimΘequals to:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0cosmθ1−sinmθ1··· 0 0\\nsinmθ1cosmθ1··· 0 0\\n...............\\n0 0 ··· cosmθl−sinmθl\\n0 0 ··· sinmθlcosmθl\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb(2)\\nThe attention score with 1-D RoPE is calculated as:\\nAn= Re⟨fq(qm, m), fk(kn, n)⟩ (3)\\nNTK-aware Interpolation (LocalLLaMA) is a training-\\nfree length extrapolation technique in LLMs. To handle the\\nlarger context length Ltestthan maximum training length\\nLtrain, it modifies the rotary base of 1-D RoPE as follows:\\nb′=b·s|D|\\n|D|−2, (4)\\nwhere the scale factor sis defined as:\\ns= max(Ltest\\nLtrain,1.0). (5)\\nYaRN (Yet another RoPE extensioN) Interpolation (Peng\\net al., 2023) introduces the ratio of dimension dasr(d) =\\nLtrain/(2πb2d/|D|), and modifies the rotary frequency as:\\nθ′\\nd= (1−γ(r(d)))θd\\ns+γ(r(d))θd, (6)\\nwhere sis the aforementioned scale factor, and γ(r(d))is a\\nramp function with extra hyper-parameters α, β:\\nγ(r) =\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f30, ifr < α\\n1, ifr > β\\nr−α\\nβ−α,otherwise .(7)\\nBesides, it incorporates a 1D-RoPE scaling term as:\\nf′\\nq(qm, m) =1√\\ntfq(qm, m), f′\\nk(kn, n) =1√\\ntfk(kn, n),\\n(8)\\nwhere1√\\nt= 0.1 ln(s) + 1 .\\n3', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 2}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\n(a) Flexible Training Pipeline2D Pos.Emb.\\nFlexible Vision Transformer\\nPadding to Maximum TokensLatent TokensResizeCalculate Loss\\nEncode+Patchify0,00,10,20,31,01,11,21,32,02,12,22,3TokensDenoised TokensGT Tokens\\n(b) Flexible Inference Pipeline\\nFlexible Vision TransformerUnPatchify+Decode\\nReshapeKiter2D Pos.Emb.Padding to Maximum TokensNoisy Tokens0,00,10,20,31,01,11,21,32,02,12,22,3TokensDenoised Tokens\\nDefine Generated Image Position0,00,10,20,31,01,11,21,3FlattenMaskedMHSASwiGLULinearLayerNormScale, ShiftScale\\nLayerNormScale, ShiftScale2D RoPE(c) FiTBlock\\nFigure 3: Overview of (a) flexible training pipeline, (b) flexible inference pipeline, and (c) FiT block.\\n3.2. Flexible Training and Inference Pipeline\\nModern deep learning models, constrained by the charac-\\nteristics of GPU hardware, are required to pack data into\\nbatches of uniform shape for parallel processing. Due to\\nthe diversity in image resolutions, as shown in Fig. 4, DiT\\nresizes and crops the images to a fixed resolution 256×256.\\nWhile resizing and cropping as a means of data augmenta-\\ntion is a common practice, this approach introduces certain\\nbiases into the input data. These biases will directly affect\\nthe final images generated by the model, including blurring\\neffects from the transition from low to high resolution and\\ninformation lost due to the cropping (more failure samples\\ncan be found in Appendix D).\\nTo this end, we propose a flexible training and inference\\npipeline, as shown in Fig. 3 (a, b). In the preprocessing\\nphase , we avoid cropping images or resizing low-resolution\\nimages to a higher resolution. Instead, we only resize high-\\nresolution images to a predetermined maximum resolution\\nlimit, HW⩽2562.In the training phase , FiT first en-\\ncodes an image into latent codes with a pre-trained V AE\\nencoder. By patchfiying latent codes to latent tokens, we\\ncan get sequences with different lengths L. To pack these\\nsequences into a batch, we pad all these sequences to the\\nmaximum token length Lmax using padding tokens. Here\\nwe set Lmax= 256 to match the fixed token length of DiT.\\nThe same as the latent tokens, we also pad the positional\\nembeddings to the maximum length for packing. Finally,\\nwe calculate the loss function only for the denoised output\\ntokens, while discarding all other padding tokens. In the\\ninference phase , we firstly define the position map of the\\ngenerated image and sample noisy tokens from the Gaussian\\ndistribution as input. After completing Kiterations of the\\ndenoising process, we reshape and unpatchfiy the denoised\\ntokens according to the predefined position map to get the\\nfinal generated image.3.3. Flexible Vision Transformer Architecture\\nBuilding upon the flexible training pipeline, our goal is\\nto find an architecture that can stably train across various\\nresolutions and generate images with arbitrary resolutions\\nand aspect ratios, as shown in Figure 3 (c). Motivated\\nby some significant architectural advances in LLMs, we\\nconduct a series of experiments to explore architectural\\nmodifications based on DiT, see details in Section 4.2.\\nReplacing MHSA with Masked MHSA. The flexible train-\\ning pipeline introduces padding tokens for flexibly pack-\\ning dynamic sequences into a batch. During the forward\\nphase of the transformer, it is crucial to facilitate interac-\\ntions among noised tokens while preventing any interaction\\nbetween noised tokens and padding tokens. The Multi-Head\\nSelf-Attention (MHSA) mechanism of original DiT is inca-\\npable of distinguishing between noised tokens and padding\\ntokens. To this end, we use Masked MHSA to replace\\nthe standard MHSA. We utilize the sequence mask Mfor\\nMasked Attention, where noised tokens are assigned the\\nvalue of 0, and padding tokens are assigned the value of\\nnegative infinity ( -inf), which is defined as follows:\\nMasked Attn. (Qi, Ki, Vi) =Softmax\\x12QiKT\\ni√dk+M\\x13\\nVi\\n(9)\\nwhere Qi,Ki,Viare the query, key, and value matrices for\\nthei-th head.\\nReplacing Absolute PE with 2D RoPE. We observe that\\nvision transformer models with absolute positional embed-\\nding fail to generalize well on images beyond the training\\nresolution, as in Sections 4.3 and 4.5. Inspired by the suc-\\ncess of 1D-RoPE in LLMs for length extrapolation (Liu\\net al., 2023), we utilize 2D-RoPE to facilitate the resolution\\n4', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 3}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\ngeneralization in vision transformer models. Formally, we\\ncalculate the 1-D RoPE for the coordinates of height and\\nwidth separately. Then such two 1-D RoPEs are concate-\\nnated in the last dimension. Given 2-D coordinates of width\\nand height as {(w, h)\\x0c\\x0c\\x0c1⩽w⩽W,1⩽h⩽H}, the 2-D\\nRoPE is defined as:\\nfq(qm, hm, wm) = [eihmΘqm∥eiwmΘqm],\\nfk(kn, hn, wn) = [eihnΘkn∥eiwnΘkn],(10)\\nwhere Θ = Diag( θ1,···, θ|D|/4), and∥denotes concate-\\nnate two vectors in the last dimension. Note that we divide\\nthe|D|-dimension space into |D|/4-dimension subspace to\\nensure the consistency of dimension, which differs from\\n|D|/2-dimension subspace in 1-D RoPE. Analogously, the\\nattention score with 2-D RoPE is:\\nAn= Re⟨fq(qm, hm, wm), fk(kn, hn, wn)⟩. (11)\\nIt is noteworthy that there is no cross-term between hand\\nwin 2D-RoPE and attention score An, so we can further\\ndecouple the rotary frequency as ΘhandΘw, resulting in the\\ndecoupled 2D-RoPE, which will be discussed in Section 3.4\\nand more details can be found in Appendix B.\\nReplacing MLP with SwiGLU. We follow recent LLMs\\nlike LLaMA (Touvron et al., 2023a;b), and replace the MLP\\nin FFN with SwiGLU, which is defined as follows:\\nSwiGLU (x, W, V ) =SiLU (xW)⊗(xV)\\nFFN(x) =SwiGLU (x, W 1, W2)W3(12)\\nwhere ⊗denotes Hadmard Product, W1,W2, and W3are\\nthe weight matrices without bias, SiLU (x) =x⊗σ(x).\\nHere we will use SwiGLU as our choice in each FFN block.\\n3.4. Training Free Resolution Extrapolation\\nWe denote the inference resolution as ( Htest,Wtest). Our\\nFiT can handle various resolutions and aspect ratios during\\ntraining, so we denote training resolution as Ltrain=√Lmax.\\nBy changing the scale factor in Equation (5) to s=\\nmax(max( Htest, W test)/Ltrain,1.0), we can directly imple-\\nment the positional interpolation methods in large language\\nmodel extrapolation on 2D-RoPE, which we call vanilla\\nNTK and YaRN implementation. Furthermore, we propose\\nvision RoPE interpolation methods by using the decoupling\\nattribute in decoupled 2D-RoPE. We modify Equation (10)\\nto:\\nˆfq(qm, hm, wm) = [eihmΘhqm∥eiwmΘwqm],\\nˆfk(kn, hn, wn) = [eihnΘhkn∥eiwnΘwkn],(13)\\nwhere Θh={θh\\nd=b−2d/|D|\\nh,1⩽d⩽|D|\\n2}andΘw=\\n{θw\\nd=b−2d/|D|\\nw ,1⩽d⩽|D|\\n2}are calculated separately.Accordingly, the scale factor of height and width is defined\\nseparately as\\nsh= max(Htest\\nLtrain,1.0), s w= max(Wtest\\nLtrain,1.0).(14)\\nDefinition 3.1. The Definition of VisionNTK Interpolation\\nis a modification of NTK-aware Interpolation by using Equa-\\ntion(13) with the following rotary base.\\nbh=b·s|D|\\n|D|−2\\nh, b w=b·s|D|\\n|D|−2\\nw , (15)\\nwhere b= 10000 is the same with Equation (1)\\nDefinition 3.2. The Definition of VisionYaRN Interpola-\\ntion is a modification of YaRN Interpolation by using Equa-\\ntion(13) with the following rotary frequency.\\nθh\\nd= (1−γ(r(d))θd\\nsh+γ(r(d))θd,\\nθw\\nd= (1−γ(r(d))θd\\nsw+γ(r(d))θd,(16)\\nwhere γ(r(d))is the same with Equation (6).\\nIt is worth noting that VisionNTK and VisionYaRN are\\ntraining-free positional embedding interpolation approaches,\\nused to alleviate the problem of position embedding out of\\ndistribution in extrapolation. When the aspect ratio equals\\none, they are equivalent to the vanilla implementation of\\nNTK and YaRN. They are especially effective in generating\\nimages with arbitrary aspect ratios, see Section 4.3.\\n4. Experiments\\n4.1. FiT Implementation\\nWe present the implementation details of FiT, including\\nmodel architecture, training details, and evaluation metrics.\\nModel architecture. We follow DiT-B and DiT-XL to\\nset the same layers, hidden size, and attention heads for\\nbase model FiT-B and xlarge model FiT-XL. As DiT re-\\nveals stronger synthesis performance when using a smaller\\npatch size, we use a patch size p=2, denoted by FiT-B/2\\nand FiT-XL/2. FiT adopts the same off-the-shelf pre-trained\\nV AE (Esser et al., 2021) as DiT provided by the Stable\\nDiffusion (Rombach et al., 2022) to encode/decode the im-\\nage/latent tokens. The V AE encoder has a downsampling\\nratio of 1/8and a feature channel dimension of 4. An im-\\nage of size 160×320×3is encoded into latent codes of\\nsize20×40×4. The latent codes of size 20×40×4are\\npatchified into latent tokens of length L= 10×20 = 200 .\\nTraining details. We train class-conditional latent FiT mod-\\nels under predetermined maximum resolution limitation,\\nHW⩽2562(equivalent to token length L≤256), on the\\nImageNet (Deng et al., 2009) dataset. We down-resize the\\n5', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 4}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\nMethod Train Cost256×256 (1:1) 160×320 (1:2) 128×384 (1:3)\\nFID↓sFID↓ IS↑ Prec.↑Rec.↑FID↓sFID↓ IS↑ Prec.↑Rec.↑ FID↓ sFID↓ IS↑ Prec.↑Rec.↑\\nBigGAN-deep - 6.95 7.36 171.4 0.87 0.28 - - - - - - - - - -\\nStyleGAN-XL - 2.30 4.02 265.12 0.78 0.53 - - - - - - - - - -\\nMaskGIT 1387k×256 6.18 - 182.1 0.80 0.51 - - - - - - - - - -\\nCDM - 4.88 - 158.71 - - - - - - - - - - - -\\nU-ViT-H/2-G (cfg=1.4) 500k×1024 2.35 5.68 265.02 0.82 0.57 6.93 12.64 175.08 0.67 0.63 196.84 95.90 7.54 0.06 0.27\\nADM-G,U 1980k×256 3.94 6.14 215.84 0.83 0.53 10.26 12.28 126.99 0.67 0.59 56.52 43.21 32.19 0.30 0.50\\nLDM-4-G (cfg=1.5) 178k×1200 3.60 5.12 247.67 0.87 0.48 10.04 11.47 119.56 0.65 0.61 29.67 26.33 57.71 0.44 0.61\\nMDT-G†(cfg=3.8,s=4) 6500k×256 1.79 4.57 283.01 0.81 0.61 135.6 73.08 9.35 0.15 0.20 124.9 70.69 13.38 0.13 0.42\\nDiT-XL/2-G (cfg=1.50) 7000k×256 2.27 4.60 278.24 0.83 0.57 20.14 30.50 97.28 0.49 0.67 107.2 68.89 15.48 0.12 0.52\\nFiT-XL/2-G∗(cfg=1.50) 1800k×256 4.27 9.99 249.72 0.84 0.51 5.74 10.05 190.14 0.74 0.55 16.81 20.62 110.93 0.57 0.52\\nTable 1: Benchmarking class-conditional image generation with in-distribution resolution on ImageNet dataset. “-G” denotes\\nthe results with classifier-free guidance.†: MDT-G adpots an improved classifier-free guidance strategy (Gao et al., 2023):\\nwt= (1−cosπ(t\\ntmax)s)w/2, where w= 3.8is the maximum guidance scale and s= 4 is the controlling factor.∗:\\nFiT-XL/2-G adopts VisionNTK for resolution extrapolation.\\nMethod Train Cost320×320 (1:1) 224×448 (1:2) 160×480 (1:3)\\nFID↓sFID↓ IS↑ Prec.↑Rec.↑FID↓sFID↓ IS↑ Prec.↑Rec.↑FID↓sFID↓ IS↑ Prec.↑Rec.↑\\nU-ViT-H/2-G (cfg=1.4) 500k×1024 7.65 16.30 208.01 0.72 0.54 67.10 42.92 45.54 0.30 0.49 95.56 44.45 24.01 0.19 0.47\\nADM-G,U 1980k×256 9.39 9.01 161.95 0.74 0.50 11.34 14.50 146.00 0.71 0.49 23.92 25.55 80.73 0.57 0.51\\nLDM-4-G (cfg=1.5) 178k×1200 6.24 13.21 220.03 0.83 0.44 8.55 17.62 186.25 0.78 0.44 19.24 20.25 99.34 0.59 0.50\\nMDT-G†(cfg=3.8,s=4) 6500k×256 383.5 136.5 4.24 0.01 0.04 365.9 142.8 4.91 0.01 0.05 276.7 138.1 7.20 0.03 0.09\\nDiT-XL/2-G (cfg=1.50) 7000k×256 9.98 23.57 225.72 0.73 0.48 94.94 56.06 35.75 0.23 0.46 140.2 79.60 14.70 0.094 0.45\\nFiT-XL/2-G∗(cfg=1.50) 1800k×256 5.42 15.41 252.65 0.81 0.47 7.90 19.63 215.29 0.75 0.47 15.72 22.57 132.76 0.62 0.47\\nTable 2: Benchmarking class-conditional image generation with out-of-distribution resolution on ImageNet dataset. “-G”\\ndenotes the results with classifier-free guidance.†: MDT-G adopts an aforementioned improved classifier-free guidance\\nstrategy.∗: FiT-XL/2-G adopts VisionNTK for resolution extrapolation. Our FiT model achieves state-of-the-art performance\\nacross all the resolutions and aspect ratios, demonstrating a strong extrapolation capability.\\nhigh-resolution images to meet the HW⩽2562limitation\\nwhile maintaining the aspect ratio. We follow DiT to use\\nHorizontal Flip Augmentation. We use the same training\\nsetting as DiT: a constant learning rate of 1×10−4using\\nAdamW (Loshchilov & Hutter, 2017), no weight decay, and\\na batch size of 256. Following common practice in the gen-\\nerative modeling literature, we adopt an exponential moving\\naverage (EMA) of model weights over training with a decay\\nof 0.9999. All results are reported using the EMA model.\\nWe retain the same diffusion hyper-parameters as DiT.\\nEvaluation details and metrics. We evaluate models with\\nsome commonly used metrics, i.e.Fre’chet Inception Dis-\\ntance (FID) (Heusel et al., 2017), sFID (Nash et al., 2021),\\nInception Score (IS) (Salimans et al., 2016), improved Pre-\\ncision and Recall (Kynk ¨a¨anniemi et al., 2019). For fair\\ncomparisons, we follow DiT to use the TensorFlow eval-\\nuation from ADM (Dhariwal & Nichol, 2021) and report\\nFID-50K with 250 DDPM sampling steps. FID is used as\\nthe major metric as it measures both diversity and fidelity.\\nWe additionally report IS, sFID, Precision, and Recall as\\nsecondary metrics. For FiT architecture experiment (Sec-\\ntion 4.2) and resolution extrapolation ablation experiment\\n(Section 4.3), we report the results without using classifier-\\nfree guidance (Ho & Salimans, 2021).\\nEvaluation resolution. Unlike previous work that mainly\\nconducted experiments on a fixed aspect ratio of 1 : 1, we\\nconducted experiments on different aspect ratios, which are1 : 1 ,1 : 2 , and 1 : 3 , respectively. On the other hand, we\\ndivide the experiment into resolution within the training dis-\\ntribution and resolution out of the training distribution. For\\nthe resolution in distribution, we mainly use 256×256(1:1),\\n160×320(1:2), and 128×384(1:3) for evaluation, with\\n256,200,192latent tokens respectively. All token lengths\\nare smaller than or equal to 256, leading to respective resolu-\\ntions within the training distribution. For the resolution out\\nof distribution, we mainly use 320×320(1:1), 224×448\\n(1:2), and 160×480(1:3) for evaluation, with 400,392,\\n300latent tokens respectively. All token lengths are larger\\nthan 256, resulting in the resolutions out of training distri-\\nbution. Through such division, we holistically evaluate the\\nimage synthesis and resolution extrapolation ability of FiT\\nat various resolutions and aspect ratios.\\n4.2. FiT Architecture Design\\nIn this part, we conduct an ablation study to verify the ar-\\nchitecture designs in FiT. We report the results of various\\nvariant FiT-B/2 models at 400K training steps and use FID-\\n50k, sFID, IS, Precision, and Recall as the evaluation met-\\nrics. We conduct experiments at three different resolutions:\\n256×256,160×320, and 224×448. These resolutions\\nare chosen to encompass different aspect ratios, as well as\\nto include resolutions both in and out of the distribution.\\nFlexible training vs. Fixed training. Flexible training\\npipeline significantly improves the performance across var-\\n6', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 5}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\nArch. Pos. Embed. FFN Train256×256 (i.d.) 160×320 (i.d.) 224×448 (o.o.d.)\\nFID↓sFID↓ IS↑ Prec.↑Rec.↑FID↓sFID↓ IS↑ Prec.↑Rec.↑FID↓sFID↓ IS↑ Prec.↑Rec.↑\\nDiT-B Abs. PE MLP Fixed 44.83 8.49 32.05 0.48 0.63 91.32 66.66 14.02 0.21 0.45 109.1 110.71 14.00 0.18 0.31\\nConfig A Abs. PE MLP Flexible 43.34 11.11 32.23 0.48 0.61 50.51 10.36 25.26 0.42 0.60 52.55 16.05 28.69 0.42 0.58\\nConfig B Abs. PE SwiGLU Flexible 41.75 11.53 34.55 0.49 0.61 48.66 10.65 26.76 0.41 0.60 52.34 17.73 30.01 0.41 0.57\\nConfig C Abs. PE + 2D RoPE MLP Flexible 39.11 10.79 36.35 0.51 0.61 46.71 10.32 27.65 0.44 0.61 46.60 15.84 33.99 0.46 0.58\\nConfig D 2D RoPE MLP Flexible 37.29 10.62 38.34 0.53 0.61 45.06 9.82 28.87 0.43 0.62 46.16 23.72 35.28 0.46 0.55\\nFiT-B 2D RoPE SwiGLU Flexible 36.36 11.08 40.69 0.52 0.62 43.96 10.26 30.45 0.43 0.62 44.67 24.09 37.10 0.49 0.53\\nTable 3: Ablation results from DiT-B/2 to FiT-B/2 at 400K training steps without using classifier-free guidance.\\nMethod320×320 (1:1) 224×448 (1:2) 160×480 (1:3)\\nFID↓sFID↓ IS↑ Prec.↑Rec.↑ FID↓ sFID↓ IS↑ Prec.↑Rec.↑ FID↓ sFID↓ IS↑ Prec.↑Rec.↑\\nDiT-B 95.47 108.68 18.38 0.26 0.40 109.1 110.71 14.00 0.18 0.31 143.8 122.81 8.93 0.073 0.20\\nDiT-B + EI 81.48 62.25 20.97 0.25 0.47 133.2 72.53 11.11 0.11 0.29 160.4 93.91 7.30 0.054 0.16\\nDiT-B + PI 72.47 54.02 24.15 0.29 0.49 133.4 70.29 11.73 0.11 0.29 156.5 93.80 7.80 0.058 0.17\\nFiT-B 61.35 30.71 31.01 0.41 0.53 44.67 24.09 37.1 0.49 0.52 56.81 22.07 25.25 0.38 0.49\\nFiT-B + PI 65.76 65.45 29.32 0.32 0.45 175.42 114.39 8.45 0.14 0.06 224.83 123.45 5.89 0.02 0.06\\nFiT-B + YaRN 44.76 38.04 44.70 0.51 0.51 82.19 75.48 29.68 0.40 0.29 104.06 72.97 20.76 0.21 0.31\\nFiT-B + NTK 57.31 31.31 33.97 0.43 0.55 45.24 29.38 38.84 0.47 0.52 59.19 26.54 26.01 0.36 0.49\\nFiT-B + VisionYaRN 44.76 38.04 44.70 0.51 0.51 41.92 42.79 45.87 0.50 0.48 62.84 44.82 27.84 0.36 0.42\\nFiT-B + VisionNTK 57.31 31.31 33.97 0.43 0.55 43.84 26.25 39.22 0.48 0.52 56.76 24.18 26.40 0.37 0.49\\nTable 4: Benchmarking class-conditional image generation with out-of-distribution resolution on ImageNet. The FiT-B/2 and\\nDiT-B/2 at 400K training steps are adopted in this experiment. Metrics are calculated without using classifier-free guidance.\\nYaRN and NTK mean the vanilla implementation of such two methods. Our FiT-B/2 demonstrates stable extrapolation\\nperformance, which can be further improved combined with VisionNTK and VisionYaRN methods.\\nious resolutions. This improvement is evident not only\\nwithin the in-distribution resolutions but also extends to res-\\nolutions out of the training distribution, as shown in Tab. 3.\\nConfig A is the original DiT-B/2 model only with flexible\\ntraining, which slightly improves the performance (-1.49\\nFID) compared with DiT-B/2 with fixed resolution training\\nat256×256resolution. Config A demonstrates a signif-\\nicant performance improvement through flexible training.\\nCompared to DiT-B/2, FID scores are reduced by 40.81 and\\n56.55 at resolutions 160×320and224×448, respectively.\\nSwiGLU vs. MLP. SwiGLU slightly improves the perfor-\\nmance across various resolutions, compared to MLP . Config\\nB is the FiT-B/2 flexible training model replacing MLP with\\nSwiGLU. Compared to Config A, Config B demonstrates\\nnotable improvements across various resolutions. Specifi-\\ncally, for resolutions of 256×256,160×320, and224×448,\\nConfig B reduces the FID scores by 1.59, 1.85, and 0.21 in\\nTab. 3, respectively. So FiT uses SwiGLU in FFN.\\n2D RoPE vs. Absolute PE. 2D RoPE demonstrates greater\\nefficiency compared to absolute position encoding, and it\\npossesses significant extrapolation capability across various\\nresolutions. Config D is the FiT-B/2 flexible training model\\nreplacing absolute PE with 2D RoPE. For resolutions within\\nthe training distribution, specifically 256×256and160×\\n320, Config D reduces the FID scores by 6.05, and 5.45 in\\nTab. 3, compared to Config A. For resolution beyond the\\ntraining distribution, 224×448, Config D shows significant\\nextrapolation capability (-6.39 FID) compared to Config A.\\nConfig C retains both absolute PE and 2D RoPE. However,\\nin a comparison between Config C and Config D, we observethat Config C performs worse. For resolutions of 256x256,\\n160x320, and 224x448, Config C increases FID scores of\\n1.82, 1.65, and 0.44, respectively, compared to Config D.\\nTherefore, only 2D RoPE is used for positional embedding\\nin our implementation.\\nPutting it together. FiT demonstrates significant and com-\\nprehensive superiority across various resolution settings,\\ncompared to original DiT. FiT has achieved state-of-the-art\\nperformance across various configurations. Compared to\\nDiT-B/2, FiT-B/2 reduces the FID score by 8.47 on the most\\ncommon resolution of 256×256in Tab. 3. Furthermore,\\nFiT-B/2 has made significant performance gains at resolu-\\ntions of 160×320and224×448, decreasing the FID scores\\nby 47.36 and 64.43, respectively.\\n4.3. FiT Resolution Extrapolation Design\\nIn this part, we adopt the DiT-B/2 and FiT-B/2 models at\\n400K training steps to evaluate the extrapolation perfor-\\nmance on three out-of-distribution resolutions: 320×320,\\n224×448and160×480. Direct extrapolation does not\\nperform well on larger resolution out of training distribution.\\nSo we conduct a comprehensive benchmarking analysis\\nfocused on positional embedding interpolation methods.\\nPI and EI. PI (Position Interpolation) and EI (Embedding\\nInterpolation) are two baseline positional embedding inter-\\npolation methods for resolution extrapolation. PI linearly\\ndown-scales the inference position coordinates to match the\\noriginal coordinates. EI resizes the positional embedding\\n7', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 6}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\nwith bilinear interpolation1. Following ViT (Dosovitskiy\\net al., 2020), EI is used for absolute positional embedding.\\nNTK and YaRN. We set the scale factor to s=\\nmax(max( Htest, W test)/√\\n256) and adopt the vanilla imple-\\nmentation of the two methods, as in Section 3.1. For YaRN,\\nwe set α= 1, β= 32 in Equation (7).\\nVisionNTK and VisionYaRN. These two methods are de-\\nfined detailedly in Definitions 3.1 and 3.2. Note that when\\nthe aspect ratio equals one, the VisionNTK and VisionYaRN\\nare equivalent to NTK and YaRN, respectively.\\nAnalysis. We present in Tab. 4 that our FiT-B/2 shows\\nstable performance when directly extrapolating to larger\\nresolutions. When combined with PI, the extrapolation\\nperformance of FiT-B/2 at all three resolutions decreases.\\nWhen combined with YaRN, the FID score reduces by 16.77\\non320×320, but the performance on 224×448and168×\\n480descends. Our VisionYaRN solves this dilemma and\\nreduces the FID score by 40.27 on 224×448and by 41.22\\nat160×480 compared with YaRN. NTK interpolation\\nmethod demonstrates stable extrapolation performance but\\nincreases the FID score slightly at 224×448and160×480\\nresolutions. Our VisionNTK method alleviates this problem\\nand exceeds the performance of direct extrapolation at all\\nthree resolutions. In conclusion, our FiT-B/2 has a strong\\nextrapolation ability, which can be further enhanced when\\ncombined with VisionYaRN and VisionNTK methods.\\nHowever, DiT-B/2 demonstrates poor extrapolation ability.\\nWhen combined with PI, the FID score achieves 72.47 at\\n320×320resolution, which still falls behind our FiT-B/2. At\\n224×448and160×480resolutions, PI and EI interpolation\\nmethods cannot improve the extrapolation performance.\\n4.4. FiT In-Distribution Resolution Results\\nFollowing our former analysis, we train our highest Gflops\\nmodel, FiT-XL/2, for 1.8M steps. We conduct experiments\\nto evaluate the performance of FiT at three different in distri-\\nbution resolutions: 256×256,160×320, and128×384. We\\nshow samples from the FiT in Fig 1, and we compare against\\nsome state-of-the-art class-conditional generative models:\\nBigGAN (Brock et al., 2018), StyleGAN-XL (Sauer et al.,\\n2022), MaskGIT (Chang et al., 2022), CDM (Ho et al.,\\n2022), U-ViT (Bao et al., 2023), ADM (Dhariwal & Nichol,\\n2021), LDM (Rombach et al., 2022), MDT (Gao et al.,\\n2023), and DiT (Peebles & Xie, 2023). When generating\\nimages of 160×320and128×384resolution, we adopt PI\\non the positional embedding of the DiT model, as stated in\\nSection 4.3. EI is employed in the positional embedding of\\nU-ViT and MDT models, as they use learnable positional\\nembedding. ADM and LDM can directly synthesize images\\nwith resolutions different from the training resolution.\\n1torch.nn.functional.interpolate(pe, (h,w), method=’bilinear’)As shown in Tab. 1, FiT-XL/2 outperforms all prior diffu-\\nsion models, decreasing the previous best FID-50K of 6.93\\nachieved by U-ViT-H/2-G to 5.74 at 160×320resolution.\\nFor128×384resolution, FiT-XL/2 shows significant su-\\nperiority, decreasing the previous SOTA FID-50K of 29.67\\nto 16.81. The FID score of FiT-XL/2 increases slightly at\\n256×256resolution, compared to other models that have\\nundergone longer training steps.\\n4.5. FiT Out-Of-Distribution Resolution Results\\nWe evaluate our FiT-XL/2 on three different out-of-\\ndistribution resolutions: 320×320,224×448, and160×480\\nand compare against some SOTA class-conditional gener-\\native models: U-ViT, ADM, LDM-4, MDT, and DiT. PI is\\nemployed in DiT, while EI is adopted in U-ViT and MDT,\\nas in Section 4.4. U-Net-based methods, such as ADM and\\nLDM-4 can directly generate images with resolution out of\\ndistribution. As shown in Table 2, FiT-XL/2 achieves the\\nbest FID-50K and IS, on all three resolutions, indicating its\\noutstanding extrapolation ability. In terms of other metrics,\\nas sFID, FiT-XL/2 demonstrates competitive performance.\\nLDMs with transformer backbones are known to have diffi-\\nculty in generating images out of training resolution, such\\nas DiT, U-ViT, and MDT. More seriously, MDT has almost\\nno ability to generate images beyond the training resolution.\\nWe speculate this is because both learnable absolute PE and\\nlearnable relative PE are used in MDT. DiT and U-ViT show\\na certain degree of extrapolation ability and achieve FID\\nscores of 9.98 and 7.65 respectively at 320x320 resolution.\\nHowever, when the aspect ratio is not equal to one, their\\ngeneration performance drops significantly, as 224×448\\nand160×480resolutions. Benefiting from the advantage of\\nthe local receptive field of the Convolution Neural Network,\\nADM and LDM show stable performance at these out-of-\\ndistribution resolutions. Our FiT-XL/2 solves the problem\\nof insufficient extrapolation capabilities of the transformer\\nin image synthesis. At 320×320,224×448, and160×480\\nresolutions, FiT-XL/2 exceeds the previous SOTA LDM on\\nFID-50K by 0.82, 0.65, and 3.52 respectively.\\n5. Conclusion\\nIn this work, we aim to contribute to the ongoing research\\non flexible generating arbitrary resolutions and aspect ratio.\\nWe propose Flexible Vision Transformer (FiT) for diffu-\\nsion model, a refined transformer architecture with flexible\\ntraining pipeline specifically designed for generating images\\nwith arbitrary resolutions and aspect ratios. FiT surpasses all\\nprevious models, whether transformer-based or CNN-based,\\nacross various resolutions. With our resolution extrapola-\\ntion method, VisionNTK, the performance of FiT has been\\nsignificantly enhanced further.\\n8', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 7}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\nReferences\\nBao, F., Nie, S., Xue, K., Cao, Y ., Li, C., Su, H., and Zhu,\\nJ. All are worth words: A vit backbone for diffusion\\nmodels. In IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , 2023.\\nBrock, A., Donahue, J., and Simonyan, K. Large scale gan\\ntraining for high fidelity natural image synthesis. arXiv\\npreprint arXiv:1809.11096 , 2018.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learn-\\ners.Advances in Neural Information Processing Systems ,\\n2020.\\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,\\nW. T. Maskgit: Masked generative image transformer. In\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , 2022.\\nChen, S., Wong, S., Chen, L., and Tian, Y . Extending\\ncontext window of large language models via positional\\ninterpolation. arXiv preprint arXiv:2306.15595 , 2023.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\\nGehrmann, S., et al. Palm: Scaling language modeling\\nwith pathways. Journal of Machine Learning Research ,\\n2023a.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., and et al, P. B. Palm: Scaling language\\nmodeling with pathways. Journal of Machine Learning\\nResearch , 2023b.\\nDehghani, M., Mustafa, B., Djolonga, J., Heek, J., Minderer,\\nM., Caron, M., Steiner, A., Puigcerver, J., Geirhos, R.,\\nAlabdulmohsin, I., et al. Patch n’pack: Navit, a vision\\ntransformer for any aspect ratio and resolution. arXiv\\npreprint arXiv:2307.06304 , 2023.\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\\nL. Imagenet: A large-scale hierarchical image database.\\nInIEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition , 2009.\\nDhariwal, P. and Nichol, A. Diffusion models beat gans\\non image synthesis. Advances in Neural Information\\nProcessing Systems , 2021.\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\\nHeigold, G., Gelly, S., et al. An image is worth 16x16\\nwords: Transformers for image recognition at scale. arXiv\\npreprint arXiv:2010.11929 , 2020.Esser, P., Rombach, R., and Ommer, B. Taming transform-\\ners for high-resolution image synthesis. In IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\n2021.\\nGao, S., Zhou, P., Cheng, M.-M., and Yan, S. Masked\\ndiffusion transformer is a strong image synthesizer. arXiv\\npreprint arXiv:2303.14389 , 2023.\\nHatamizadeh, A., Song, J., Liu, G., Kautz, J., and Vahdat, A.\\nDiffit: Diffusion vision transformers for image generation.\\narXiv preprint arXiv:2312.02139 , 2023.\\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\\nHochreiter, S. Gans trained by a two time-scale update\\nrule converge to a local nash equilibrium. Advances in\\nNeural Information Processing Systems , 2017.\\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\\nInNeurIPS 2021 Workshop on Deep Generative Models\\nand Downstream Applications , 2021.\\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\\nbilistic models. Advances in Neural Information Process-\\ning Systems , 2020.\\nHo, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M.,\\nand Salimans, T. Cascaded diffusion models for high\\nfidelity image generation. Journal of Machine Learning\\nResearch , 2022.\\nHyv¨arinen, A. and Dayan, P. Estimation of non-normalized\\nstatistical models by score matching. Journal of Machine\\nLearning Research , 2005.\\nKynk ¨a¨anniemi, T., Karras, T., Laine, S., and Lehtinen,\\nJ.and Aila, T. Improved precision and recall metric for\\nassessing generative models. Advances in Neural Infor-\\nmation Processing Systems , 2019.\\nLiu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D.\\nScaling laws of rope-based extrapolation. arXiv preprint\\narXiv:2310.05209 , 2023.\\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S.,\\nand Guo, B. Swin transformer: Hierarchical vision trans-\\nformer using shifted windows. In IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , 2021.\\nLiu, Z., Mao, H., Wu, C.-Y ., Feichtenhofer, C., Darrell,\\nT., and Xie, S. A convnet for the 2020s. In IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\n2022.\\nLocalLLaMA. Ntk-aware scaled rope allows llama\\nmodels to have extended (8k+) context size without\\nany fine-tuning and minimal perplexity degradation.\\nhttps://www.reddit.com/r/LocalLLaMA/\\n9', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 8}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\ncomments/14lz7j5/ntkaware_scaled_\\nrope_allows_llama_models_to_have/ .\\nAccessed: 2024-2-1.\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\\nlarization. arXiv preprint arXiv:1711.05101 , 2017.\\nNash, C., Menick, J., Dieleman, S., and Battaglia, P. W.\\nGenerating images with sparse representations. arXiv\\npreprint arXiv:2103.03841 , 2021.\\nPeebles, W. and Xie, S. Scalable diffusion models with\\ntransformers. In IEEE/CVF International Conference on\\nComputer Vision , 2023.\\nPeng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:\\nEfficient context window extension of large language\\nmodels. arXiv preprint arXiv:2309.00071 , 2023.\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\\net al. Learning transferable visual models from natural\\nlanguage supervision. In International Conference on\\nMachine Learning , 2021.\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\\nOmmer, B. High-resolution image synthesis with latent\\ndiffusion models. In IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , 2022.\\nRuoss, A., Del ´etang, G., Genewein, T., Grau-Moya, J.,\\nCsord ´as, R., Bennani, M., Legg, S., and Veness, J. Ran-\\ndomized positional encodings boost length generalization\\nof transformers. arXiv preprint arXiv:2305.16843 , 2023.\\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\\nB., Salimans, T., et al. Photorealistic text-to-image diffu-\\nsion models with deep language understanding. Advances\\nin Neural Information Processing Systems , 2022.\\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V .,\\nRadford, A., and Chen, X. Improved techniques for\\ntraining gans. Advances in Neural Information Processing\\nSystems , 2016.\\nSauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling\\nstylegan to large diverse datasets. In ACM SIGGRAPH\\n2022 conference proceedings , 2022.\\nShazeer, N. Glu variants improve transformer. arXiv\\npreprint arXiv:2002.05202 , 2020.\\nSong, J., Meng, C., and Ermon, S. Denoising diffusion im-\\nplicit models. arXiv preprint arXiv:2010.02502 , 2020a.\\nSong, Y ., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\\nmon, S., and Poole, B. Score-based generative modeling\\nthrough stochastic differential equations. arXiv preprint\\narXiv:2011.13456 , 2020b.Su, J., Ahmed, M., Lu, Y ., Pan, S., Bo, W., and Liu, Y .\\nRoformer: Enhanced transformer with rotary position\\nembedding. Neurocomputing , 2024.\\nSun, Y ., Dong, L., Patra, B., Ma, S., Huang, S., Ben-\\nhaim, A., Chaudhary, V ., Song, X., , and Wei, F.\\nA length-extrapolatable transformer. arXiv preprint\\narXiv:2212.10554 , 2022.\\nTeam, G., Anil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu,\\nJ., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al.\\nGemini: a family of highly capable multimodal models.\\narXiv preprint arXiv:2312.11805 , 2023.\\nTouvron, H., Vedaldi, A., Douze, M., and J ´egou, H. Fixing\\nthe train-test resolution discrepancy. Advances in Neural\\nInformation Processing Systems , 2019.\\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,\\nA., and J ´egou, H. Training data-efficient image transform-\\ners & distillation through attention. In International con-\\nference on machine learning , pp. 10347–10357. PMLR,\\n2021.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., and et al, B. R. Llama: Open and\\nefficient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,\\nBabaei, Y ., and et al, N. B. Llama 2: Open foundation and\\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 ,\\n2023b.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\\ntion is all you need. Advances in Neural Information\\nProcessing Systems , 2017.\\n10', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 9}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\nA. Experimentin Setups\\nWe provide detailed network configurations and performance of all models, which are listed in Tab. 5.\\nModels Layers Dim. Head Num. Patch Size Max Token Length Training Steps Batch Size Learning Rate FID-50K\\nDiT-B/2 12 768 12 2 256 400K 256 1×10−444.83\\nConfig A 12 768 12 2 256 400K 256 1×10−443.34\\nConfig B 12 768 12 2 256 400K 256 1×10−441.75\\nConfig C 12 768 12 2 256 400K 256 1×10−439.11\\nConfig D 12 768 12 2 256 400K 256 1×10−437.29\\nFiT-B/2 12 768 12 2 256 400K 256 1×10−436.36\\nFiT-XL/2-G 28 1152 16 2 256 1800K 256 1×10−44.27\\nTable 5: Network configurations and performance of all models.\\nWe use the same ft-EMA V AE2with DiT, which is provided by the Stable Diffusion to encode/decode the image/latent\\ntokens by default. The metrics are calculated using the ADM TensorFlow evaluation Suite3.\\nB. Detailed Attention Score with 2D RoPE and decoupled 2D-RoPE.\\n2D RoPE defines a vector-valued complex function f(x, hm, wm)in Equation (10) as follows:\\nf(x, hm, wm) =\\x02\\n(x0+ix1)eihmθ0,(x2+ix3)eihmθ1, . . . , (xd/2−2+ixd/2−1)eihmθd/4−1,\\n(xd/2+ixd/2+1)eiwmθ0,(xd/2+2+ixd/2+3)eiwmθ1, . . . , (xd−2+ixd−1)eiwmθd/4−1\\x03T.(17)\\nThe self-attention score Aninjected with 2D RoPE in Equation (11) is detailed defined as follows:\\nAn=Re⟨fq(qm, hm, wm), fk(kn, hn, wn)⟩\\n=Re\\uf8ee\\n\\uf8f0d/4−1X\\nj=0(q2j+iq2j+1)(k2j−ik2j+1)ei(hm−hn)θj+d/4−1X\\nj=0(q2j+iq2j+1)(k2j−ik2j+1)ei(wm−wn)θj\\uf8f9\\n\\uf8fb\\n=d/4−1X\\nj=0[(q2jk2j+q2j+1k2j+1) cos(( hm−hn)θj) + (q2jk2j+1−q2j+1k2j) sin(( hm−hn)θj)]+\\nd/4−1X\\nj=0[(q2jk2j+q2j+1k2j+1) cos(( wm−wn)θj) + (q2jk2j+1−q2j+1k2j) sin(( wm−wn)θj)],(18)\\nwhere 2-D coordinates of width and height as {(w, h)\\x0c\\x0c\\x0c1⩽w⩽W,1⩽h⩽H}, the subscripts of qandkdenote\\nthe dimensions of the attention head, θn= 10000−2n/d. There is no cross-term between handwin 2D-RoPE and\\nattention score An, so we can further decouple the rotary frequency as Θh={θh\\nd=b−2d/|D|\\nh,1⩽d⩽|D|\\n2}and\\nΘw={θw\\nd=b−2d/|D|\\nw ,1⩽d⩽|D|\\n2}, resulting in the decoupled 2D-RoPE, as follows:\\n2The model is downloaded in https://huggingface.co/stabilityai/sd-vae-ft-ema\\n3https://github.com/openai/guided-diffusion/tree/main/evaluations\\n11', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 10}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\nAn=d/4−1X\\nj=0[(q2jk2j+q2j+1k2j+1) cos(( hm−hn)θh\\nj) + (q2jk2j+1−q2j+1k2j) sin(( hm−hn)θh\\nj)]+\\nd/4−1X\\nj=0[(q2jk2j+q2j+1k2j+1) cos(( wm−wn)θw\\nj) + (q2jk2j+1−q2j+1k2j) sin(( wm−wn)θw\\nj)]\\n=Re\\uf8ee\\n\\uf8f0d/4−1X\\nj=0(q2j+iq2j+1)(k2j−ik2j+1)ei(hm−hn)θh\\nj+d/4−1X\\nj=0(q2j+iq2j+1)(k2j−ik2j+1)ei(wm−wn)θw\\nj\\uf8f9\\n\\uf8fb\\n=Re⟨ˆfq(qm, hm, wm),ˆfk(kn, hn, wn)⟩.(19)\\nSo we can reformulate the vector-valued complex function ˆf(x, hm, wm)in Equation (13) as follows:\\nˆf(x, hm, wm) =h\\n(x0+ix1)eihmθh\\n0,(x2+ix3)eihmθh\\n1, . . . , (xd/2−2+ixd/2−1)eihmθh\\nd/4−1,\\n(xd/2+ixd/2+1)eiwmθw\\n0,(xd/2+2+ixd/2+3)eiwmθw\\n1, . . . , (xd−2+ixd−1)eiwmθw\\nd/4−1iT\\n.(20)\\nC. Limitations and Future Work\\nConstrained by limited computational resources, we only train FiT-XL/2 for 1800K steps. At the resolution of 256x256,\\nthe performance of FiT-XL/2 is slightly inferior compared to the DiT-XL/2 model. On the other hand, we have not yet\\nthoroughly explored the generative capabilities of the FiT-XL/2 model when training with higher resolutions (larger token\\nlength limitation). Additionally, we only explore resolution extrapolation techniques that are training-free, without delving\\ninto other resolution extrapolation methods that require additional training. We believe that FiT will enable a range of\\ninteresting studies that have been infeasible before and encourage more attention towards generating images with arbitrary\\nresolutions and aspect ratios.\\nD. More Model Samples\\nWe show samples from our FiT-XL/2 models at resolutions of 256×256,224×448and448×224, trained for 1.8M\\n(generated with 250 DDPM sampling steps and the ft-EMA V AE decoder). Fig. 6 shows uncurated samples from FiT-XL/2\\nwith classifier-free guidance scale 4.0 and class label “loggerhead turtle” (33). Fig. 7 shows uncurated samples from\\nFiT-XL/2 with classifier-free guidance scale 4.0 and class label “Cacatua galerita” (89). Fig. 8 shows uncurated samples\\nfrom FiT-XL/2 with classifier-free guidance scale 4.0 and class label “golden retriever” (207). Fig. 9 shows uncurated\\nsamples from FiT-XL/2 with classifier-free guidance scale 4.0 and class label “white fox” (279). Fig. 10 shows uncurated\\nsamples from FiT-XL/2 with classifier-free guidance scale 4.0 and class label “otter” (360). Fig. 11 shows uncurated samples\\nfrom FiT-XL/2 with classifier-free guidance scale 4.0 and class label “volcano” (980).\\nWe also show some failure samples from DiT-XL/2, as shown in Fig. 5.\\n12', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 11}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\n0 200 400 600 800 1000 1200+\\nHeight020040060080010001200+Weight\\n0500010000150002000025000300003500040000+\\nFigure 4: Height/Width distribution of the original ImageNet (Deng et al., 2009) dataset.\\nFigure 5: Uncurated failure samples from DiT-XL/2.\\n13', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 12}),\n",
       " Document(page_content='FiT: Flexible Vision Transformer for Diffusion Model\\nFigure 6: Uncurated samples from FiT-XL/2 models at\\nresolutions of 256×256,224×448and448×224.\\nFigure 7: Uncurated samples from FiT-XL/2 models at\\nresolutions of 256×256,224×448and448×224.\\nFigure 8: Uncurated samples from FiT-XL/2 models at\\nresolutions of 256×256,224×448and448×224.\\nFigure 9: Uncurated samples from FiT-XL/2 models at\\nresolutions of 256×256,224×448and448×224.\\nFigure 10: Uncurated samples from FiT-XL/2 models at\\nresolutions of 256×256,224×448and448×224.\\nFigure 11: Uncurated samples from FiT-XL/2 models at\\nresolutions of 256×256,224×448and448×224.\\n14', metadata={'source': 'https://arxiv.org/pdf/2402.12376v1.pdf', 'page': 13}),\n",
       " Document(page_content='YOLO-World: Real-Time Open-Vocabulary Object Detection\\nTianheng Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang Wang3,✉, Ying Shan1,2\\n∗equal contribution†project lead✉corresponding author\\n1Tencent AI Lab2ARC Lab, Tencent PCG\\n3School of EIC, Huazhong University of Science & Technology\\nCode & Models: YOLO-World\\nAbstract\\nThe You Only Look Once (YOLO) series of detectors\\nhave established themselves as efficient and practical tools.\\nHowever, their reliance on predefined and trained ob-\\nject categories limits their applicability in open scenar-\\nios. Addressing this limitation, we introduce YOLO-World,\\nan innovative approach that enhances YOLO with open-\\nvocabulary detection capabilities through vision-language\\nmodeling and pre-training on large-scale datasets. Specif-\\nically, we propose a new Re-parameterizable Vision-\\nLanguage Path Aggregation Network (RepVL-PAN) and\\nregion-text contrastive loss to facilitate the interaction be-\\ntween visual and linguistic information. Our method excels\\nin detecting a wide range of objects in a zero-shot man-\\nner with high efficiency. On the challenging LVIS dataset,\\nYOLO-World achieves 35.4 AP with 52.0 FPS on V100,\\nwhich outperforms many state-of-the-art methods in terms\\nof both accuracy and speed. Furthermore, the fine-tuned\\nYOLO-World achieves remarkable performance on several\\ndownstream tasks, including object detection and open-\\nvocabulary instance segmentation.\\n1. Introduction\\nObject detection has been a long-standing and fundamental\\nchallenge in computer vision with numerous applications in\\nimage understanding, robotics, and autonomous vehicles.\\nTremendous works [16, 27, 43, 45] have achieved signif-\\nicant breakthroughs in object detection with the develop-\\nment of deep neural networks. Despite the success of these\\nmethods, they remain limited as they only handle object de-\\ntection with a fixed vocabulary, e.g., 80 categories in the\\nCOCO [26] dataset. Once object categories are defined and\\nlabeled, trained detectors can only detect those specific cat-\\negories, thus limiting the ability and applicability of open\\n20×SpeedupFigure 1. Speed-and-Accuracy Curve. We compare YOLO-\\nWorld with recent open-vocabulary methods in terms of speed and\\naccuracy. All models are evaluated on the LVIS minival and in-\\nference speeds are measured on one NVIDIA V100 w/o TensorRT.\\nThe size of the circle represents the model’s size.\\nscenarios.\\nRecent works [8, 13, 48, 53, 58] have explored the\\nprevalent vision-language models [19, 39] to address open-\\nvocabulary detection [58] through distilling vocabulary\\nknowledge from language encoders, e.g., BERT [5]. How-\\never, these distillation-based methods are much limited due\\nto the scarcity of training data with a limited diversity of\\nvocabulary, e.g., OV-COCO [58] containing 48 base cate-\\ngories. Several methods [24, 30, 56, 57, 59] reformulate ob-\\nject detection training as region-level vision-language pre-\\ntraining and train open-vocabulary object detectors at scale.\\nHowever, those methods still struggle for detection in real-\\nworld scenarios, which suffer from two aspects: (1) heavy\\ncomputation burden and (2) complicated deployment for\\nedge devices. Previous works [24, 30, 56, 57, 59] have\\n1arXiv:2401.17270v3  [cs.CV]  22 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 0}),\n",
       " Document(page_content='demonstrated the promising performance of pre-training\\nlarge detectors while pre-training small detectors to en-\\ndow them with open recognition capabilities remains un-\\nexplored.\\nIn this paper, we present YOLO-World, aiming for\\nhigh-efficiency open-vocabulary object detection, and ex-\\nplore large-scale pre-training schemes to boost the tradi-\\ntional YOLO detectors to a new open-vocabulary world.\\nCompared to previous methods, the proposed YOLO-\\nWorld is remarkably efficient with high inference speed\\nand easy to deploy for downstream applications. Specif-\\nically, YOLO-World follows the standard YOLO archi-\\ntecture [20] and leverages the pre-trained CLIP [39] text\\nencoder to encode the input texts. We further propose\\nthe Re-parameterizable Vision-Language Path Aggregation\\nNetwork (RepVL-PAN) to connect text features and im-\\nage features for better visual-semantic representation. Dur-\\ning inference, the text encoder can be removed and the\\ntext embeddings can be re-parameterized into weights of\\nRepVL-PAN for efficient deployment. We further inves-\\ntigate the open-vocabulary pre-training scheme for YOLO\\ndetectors through region-text contrastive learning on large-\\nscale datasets, which unifies detection data, grounding data,\\nand image-text data into region-text pairs. The pre-trained\\nYOLO-World with abundant region-text pairs demonstrates\\na strong capability for large vocabulary detection and train-\\ning more data leads to greater improvements in open-\\nvocabulary capability.\\nIn addition, we explore a prompt-then-detect paradigm\\nto further improve the efficiency of open-vocabulary object\\ndetection in real-world scenarios. As illustrated in Fig. 2,\\ntraditional object detectors [16, 20, 23, 41–43, 52] con-\\ncentrate on the fixed-vocabulary (close-set) detection with\\npredefined and trained categories. While previous open-\\nvocabulary detectors [24, 30, 56, 59] encode the prompts of\\na user for online vocabulary with text encoders and detect\\nobjects. Notably, those methods tend to employ large de-\\ntectors with heavy backbones, e.g., Swin-L [32], to increase\\nthe open-vocabulary capacity. In contrast, the prompt-then-\\ndetect paradigm (Fig. 2 (c)) first encodes the prompts of a\\nuser to build an offline vocabulary and the vocabulary varies\\nwith different needs. Then, the efficient detector can infer\\nthe offline vocabulary on the fly without re-encoding the\\nprompts. For practical applications, once we have trained\\nthe detector, i.e., YOLO-World, we can pre-encode the\\nprompts or categories to build an offline vocabulary and\\nthen seamlessly integrate it into the detector.\\nOur main contributions can be summarized into three\\nfolds:\\n• We introduce the YOLO-World, a cutting-edge open-\\nvocabulary object detector with high efficiency for real-\\nworld applications.\\n• We propose a Re-parameterizable Vision-Language PANto connect vision and language features and an open-\\nvocabulary region-text contrastive pre-training scheme\\nfor YOLO-World.\\n• The proposed YOLO-World pre-trained on large-scale\\ndatasets demonstrates strong zero-shot performance and\\nachieves 35.4 AP on LVIS with 52.0 FPS. The pre-trained\\nYOLO-World can be easily adapted to downstream tasks,\\ne.g., open-vocabulary instance segmentation and referring\\nobject detection. Moreover, the pre-trained weights and\\ncodes of YOLO-World will be open-sourced to facilitate\\nmore practical applications.\\n2. Related Works\\n2.1. Traditional Object Detection\\nPrevalent object detection research concentrates on fixed-\\nvocabulary (close-set) detection, in which object detectors\\nare trained on datasets with pre-defined categories, e.g.,\\nCOCO dataset [26] and Objects365 dataset [46], and then\\ndetect objects within the fixed set of categories. During\\nthe past decades, the methods for traditional object de-\\ntection can be simply categorized into three groups, i.e.,\\nregion-based methods, pixel-based methods, and query-\\nbased methods. The region-based methods [11, 12, 16, 27,\\n44], such as Faster R-CNN [44], adopt a two-stage frame-\\nwork for proposal generation [44] and RoI-wise (Region-\\nof-Interest) classification and regression. The pixel-based\\nmethods [28, 31, 42, 49, 61] tend to be one-stage detec-\\ntors, which perform classification and regression over pre-\\ndefined anchors or pixels. DETR [1] first explores object\\ndetection through transformers [50] and inspires extensive\\nquery-based methods [64]. In terms of inference speed,\\nRedmon et al. presents YOLOs [40–42] which exploit sim-\\nple convolutional architectures for real-time object detec-\\ntion. Several works [10, 23, 33, 52, 55] propose various\\narchitectures or designs for YOLO, including path aggrega-\\ntion networks [29], cross-stage partial networks [51], and\\nre-parameterization [6], which further improve both speed\\nand accuracy. In comparison to previous YOLOs, YOLO-\\nWorld in this paper aims to detect objects beyond the fixed\\nvocabulary with strong generalization ability.\\n2.2. Open-Vocabulary Object Detection\\nOpen-vocabulary object detection (OVD) [58] has emerged\\nas a new trend for modern object detection, which aims\\nto detect objects beyond the predefined categories. Early\\nworks [13] follow the standard OVD setting [58] by train-\\ning detectors on the base classes and evaluating the novel\\n(unknown) classes. Nevertheless, this open-vocabulary set-\\nting can evaluate the capability of detectors to detect and\\nrecognize novel objects, it is still limited for open scenar-\\nios and lacks generalization ability to other domains due\\nto training on the limited dataset and vocabulary. Inspired\\n2', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 1}),\n",
       " Document(page_content='DetectorText Encoder(a) Traditional Object Detector(b) PreivousOpen-V ocabulary Detector(c) YOLO-WorldObject DetectorFixedVocabulary\\nText EncoderLarge Detector\\nText EncoderLightweight DetectorOfflineVocabulary\\nUserUserOnlineVocabularyRe-parameterize\\nUserTrain-onlyFigure 2. Comparison with Detection Paradigms. (a) Traditional Object Detector : These object detectors can only detect objects\\nwithin the fixed vocabulary pre-defined by the training datasets, e.g., 80 categories of COCO dataset [26]. The fixed vocabulary limits the\\nextension for open scenes. (b) Previous Open-Vocabulary Detectors: Previous methods tend to develop large and heavy detectors for\\nopen-vocabulary detection which intuitively have strong capacity. In addition, these detectors simultaneously encode images and texts as\\ninput for prediction, which is time-consuming for practical applications. (c) YOLO-World: We demonstrate the strong open-vocabulary\\nperformance of lightweight detectors, e.g., YOLO detectors [20, 42], which is of great significance for real-world applications. Rather than\\nusing online vocabulary, we present a prompt-then-detect paradigm for efficient inference, in which the user generates a series of prompts\\naccording to the need and the prompts will be encoded into an offline vocabulary. Then it can be re-parameterized as the model weights\\nfor deployment and further acceleration.\\nYOLO BackboneTextEncoder\\nA manand a womanare skiing with a dogcaption, noun phrases, category…\\nUserText Embeddings\\nMulti-scale Image FeaturesTextContrastive HeadBox HeadmanwomandogV ocabulary Embeddingsmanwomandog\\nUser’s  V ocabularyDogImage-aware Embeddings\\nMulti-scaleImage FeaturesTraining: Online VocabularyDeployment: Offline VocabularyVision-Language PAN\\nObject EmbeddingsRegion-Text Matching\\nInputImageExtract Nouns\\nFigure 3. Overall Architecture of YOLO-World. Compared to traditional YOLO detectors, YOLO-World as an open-vocabulary detector\\nadopts text as input. The Text Encoder first encodes the input text input text embeddings. Then the Image Encoder encodes the input image\\ninto multi-scale image features and the proposed RepVL-PAN exploits the multi-level cross-modality fusion for both image and text features.\\nFinally, YOLO-World predicts the regressed bounding boxes and the object embeddings for matching the categories or nouns that appeared\\nin the input text.\\nby vision-language pre-training [19, 39], recent works [8,\\n22, 53, 62, 63] formulate open-vocabulary object detection\\nas image-text matching and exploit large-scale image-text\\ndata to increase the training vocabulary at scale. OWL-\\nViTs [35, 36] fine-tune the simple vision transformers [7]\\nwith detection and grounding datasets and build the sim-\\nple open-vocabulary detectors with promising performance.\\nGLIP [24] presents a pre-training framework for open-\\nvocabulary detection based on phrase grounding and eval-\\nuates in a zero-shot setting. Grounding DINO [30] incor-\\nporates the grounded pre-training [24] into detection trans-\\nformers [60] with cross-modality fusions. Several meth-\\nods [25, 56, 57, 59] unify detection datasets and image-textdatasets through region-text matching and pre-train detec-\\ntors with large-scale image-text pairs, achieving promising\\nperformance and generalization. However, these methods\\noften use heavy detectors like ATSS [61] or DINO [60]\\nwith Swin-L [32] as a backbone, leading to high com-\\nputational demands and deployment challenges. In con-\\ntrast, we present YOLO-World, aiming for efficient open-\\nvocabulary object detection with real-time inference and\\neasier downstream application deployment. Differing from\\nZSD-YOLO [54], which also explores open-vocabulary de-\\ntection [58] with YOLO through language model align-\\nment, YOLO-World introduces a novel YOLO framework\\nwith an effective pre-training strategy, enhancing open-\\n3', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 2}),\n",
       " Document(page_content='vocabulary performance and generalization.\\n3. Method\\n3.1. Pre-training Formulation: Region-Text Pairs\\nThe traditional object detection methods, including the\\nYOLO-series [20], are trained with instance annotations\\nΩ ={Bi, ci}N\\ni=1, which consist of bounding boxes {Bi}\\nand category labels {ci}. In this paper, we reformulate the\\ninstance annotations as region-text pairs Ω ={Bi, ti}N\\ni=1,\\nwhere tiis the corresponding text for the region Bi. Specif-\\nically, the text tican be the category name, noun phrases,\\nor object descriptions. Moreover, YOLO-World adopts both\\nthe image Iand texts T(a set of nouns) as input and outputs\\npredicted boxes {ˆBk}and the corresponding object embed-\\ndings{ek}(ek∈RD).\\n3.2. Model Architecture\\nThe overall architecture of the proposed YOLO-World is il-\\nlustrated in Fig. 3, which consists of a YOLO detector , a\\nText Encoder , and a Re-parameterizable Vision-Language\\nPath Aggregation Network (RepVL-PAN). Given the input\\ntext, the text encoder in YOLO-World encodes the text into\\ntext embeddings. The image encoder in the YOLO detector\\nextracts the multi-scale features from the input image. Then\\nwe leverage the RepVL-PAN to enhance both text and im-\\nage representation by exploiting the cross-modality fusion\\nbetween image features and text embeddings.\\nYOLO Detector. YOLO-World is mainly developed\\nbased on YOLOv8 [20], which contains a Darknet back-\\nbone [20, 43] as the image encoder, a path aggregation net-\\nwork (PAN) for multi-scale feature pyramids, and a head\\nfor bounding box regression and object embeddings.\\nText Encoder. Given the text T, we adopt the Trans-\\nformer text encoder pre-trained by CLIP [39] to extract the\\ncorresponding text embeddings W=TextEncoder (T)∈\\nRC×D, where Cis the number of nouns and Dis the em-\\nbedding dimension. The CLIP text encoder offers better\\nvisual-semantic capabilities for connecting visual objects\\nwith texts compared to text-only language encoders [5].\\nWhen the input text is a caption or referring expression,\\nwe adopt the simple n-gram algorithm to extract the noun\\nphrases and then feed them into the text encoder.\\nText Contrastive Head. Following previous works [20],\\nwe adopt the decoupled head with two 3×3convs to regress\\nbounding boxes {bk}K\\nk=1and object embeddings {ek}K\\nk=1,\\nwhere Kdenotes the number of objects. We present a text\\ncontrastive head to obtain the object-text similarity sk,jby:\\nsk,j=α·L2-Norm (ek)·L2-Norm (wj)⊤+β, (1)where L2-Norm (·)is the L2 normalization and wj∈W\\nis the j-th text embeddings. In addition, we add the affine\\ntransformation with the learnable scaling factor αand shift-\\ning factor β. Both the L2 norms and the affine transforma-\\ntions are important for stabilizing the region-text training.\\nTraining with Online Vocabulary. During training, we\\nconstruct an online vocabulary Tfor each mosaic sample\\ncontaining 4 images. Specifically, we sample all positive\\nnouns involved in the mosaic images and randomly sam-\\nple some negative nouns from the corresponding dataset.\\nThe vocabulary for each mosaic sample contains at most M\\nnouns, and Mis set to 80 as default.\\nInference with Offline Vocabulary. At the inference\\nstage, we present a prompt-then-detect strategy with an of-\\nfline vocabulary for further efficiency. As shown in Fig. 3,\\nthe user can define a series of custom prompts, which might\\ninclude captions or categories. We then utilize the text en-\\ncoder to encode these prompts and obtain offline vocabu-\\nlary embeddings. The offline vocabulary allows for avoid-\\ning computation for each input and provides the flexibility\\nto adjust the vocabulary as needed.\\n3.3. Re-parameterizable Vision-Language PAN\\nFig. 4 shows the structure of the proposed RepVL-PAN\\nwhich follows the top-down and bottom-up paths in [20, 29]\\nto establish the feature pyramids {P3, P4, P5}with the\\nmulti-scale image features {C3, C4, C5}. Furthermore,\\nwe propose the Text-guided CSPLayer (T-CSPLayer) and\\nImage-Pooling Attention (I-Pooling Attention) to further\\nenhance the interaction between image features and text\\nfeatures, which can improve the visual-semantic represen-\\ntation for open-vocabulary capability. During inference, the\\noffline vocabulary embeddings can be re-parameterized into\\nweights of convolutional or linear layers for deployment.\\nText-guided CSPLayer. As Fig. 4 illustrates, the cross-\\nstage partial layers (CSPLayer) are utilized after the top-\\ndown or bottom-up fusion. We extend the CSPLayer\\n(also called C2f) of [20] by incorporating text guidance\\ninto multi-scale image features to form the Text-guided\\nCSPLayer. Specifically, given the text embeddings Wand\\nimage features Xl∈RH×W×D(l∈ {3,4,5}), we adopt\\nthemax-sigmoid attention after the last dark bottleneck\\nblock to aggregate text features into image features by:\\nX′\\nl=Xl·δ( max\\nj∈{1..C}(XlW⊤\\nj))⊤, (2)\\nwhere the updated X′\\nlis concatenated with the cross-stage\\nfeatures as output. The δindicates the sigmoid function.\\n4', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 3}),\n",
       " Document(page_content='manwomandog=\\nC3C4C5T-CSPLayerT-CSPLayerT-CSPLayerT-CSPLayerI-Pooling AttentionP3P4P5I-Pooling AttentionText Embeddings\\n×12×12×2×2Image-aware Embeddings\\nText to ImageImage to TextDark BottleneckMax-SigmoidCSTextSplitSCConcatT-CSPLayer (C2f Block)I-PoolingAttentionCMHCAText3×3Figure 4. Illustration of the RepVL-PAN. The proposed RepVL-\\nPAN adopts the Text-guided CSPLayer (T-CSPLayer) for injecting\\nlanguage information into image features and the Image Pooling\\nAttention (I-Pooling Attention) for enhancing image-aware text\\nembeddings.\\nImage-Pooling Attention. To enhance the text embed-\\ndings with image-aware information, we aggregate image\\nfeatures to update the text embeddings by proposing the\\nImage-Pooling Attention. Rather than directly using cross-\\nattention on image features, we leverage max pooling on\\nmulti-scale features to obtain 3×3regions, resulting in a\\ntotal of 27 patch tokens ˜X∈R27×D. The text embeddings\\nare then updated by:\\nW′=W+MultiHead-Attention (W,˜X,˜X)(3)\\n3.4. Pre-training Schemes\\nIn this section, we present the training schemes for pre-\\ntraining YOLO-World on large-scale detection, grounding,\\nand image-text datasets.\\nLearning from Region-Text Contrastive Loss. Given\\nthe mosaic sample Iand texts T, YOLO-World outputs\\nKobject predictions {Bk, sk}K\\nk=1along with annotations\\nΩ ={Bi, ti}N\\ni=1. We follow [20] and leverage task-aligned\\nlabel assignment [9] to match the predictions with ground-\\ntruth annotations and assign each positive prediction with a\\ntext index as the classification label. Based on this vocabu-\\nlary, we construct the region-text contrastive loss Lconwith\\nregion-text pairs through cross entropy between object-text\\n(region-text) similarity and object-text assignments. In ad-\\ndition, we adopt IoU loss and distributed focal loss for\\nbounding box regression and the total training loss is de-\\nfined as: L(I) =Lcon+λI·(Liou+Ldfl),where λIis\\nan indicator factor and set to 1 when input image Iis from\\ndetection or grounding data and set to 0 when it is fromthe image-text data. Considering image-text datasets have\\nnoisy boxes, we only calculate the regression loss for sam-\\nples with accurate bounding boxes.\\nPseudo Labeling with Image-Text Data. Rather than di-\\nrectly using image-text pairs for pre-training, we propose an\\nautomatic labeling approach to generate region-text pairs.\\nSpecifically, the labeling approach contains three steps: (1)\\nextract noun phrases : we first utilize the n-gram algo-\\nrithm to extract noun phrases from the text; (2) pseudo la-\\nbeling : we adopt a pre-trained open-vocabulary detector,\\ne.g., GLIP [24], to generate pseudo boxes for the given\\nnoun phrases for each image, thus providing the coarse\\nregion-text pairs. (3) filtering : We employ the pre-trained\\nCLIP [39] to evaluate the relevance of image-text pairs and\\nregion-text pairs, and filter the low-relevance pseudo an-\\nnotations and images. We further filter redundant bound-\\ning boxes by incorporating methods such as Non-Maximum\\nSuppression (NMS). We suggest the readers refer to the ap-\\npendix for the detailed approach. With the above approach,\\nwe sample and label 246k images from CC3M [47] with\\n821k pseudo annotations.\\n4. Experiments\\nIn this section, we demonstrate the effectiveness of the\\nproposed YOLO-World by pre-training it on large-scale\\ndatasets and evaluating YOLO-World in a zero-shot manner\\non both LVIS benchmark and COCO benchmark (Sec. 4.2).\\nWe also evaluate the fine-tuning performance of YOLO-\\nWorld on COCO, LVIS for object detection.\\n4.1. Implementation Details\\nThe YOLO-World is developed based on the MMYOLO\\ntoolbox [3] and the MMDetection toolbox [2]. Following\\n[20], we provide three variants of YOLO-World for differ-\\nent latency requirements, e.g., small (S), medium (M), and\\nlarge (L). We adopt the open-source CLIP [39] text encoder\\nwith pre-trained weights to encode the input text. Unless\\nspecified, we measure the inference speeds of all models on\\none NVIDIA V100 GPU without extra acceleration mecha-\\nnisms, e.g., FP16 or TensorRT.\\n4.2. Pre-training\\nExperimental Setup. At the pre-training stage, we adopt\\nthe AdamW optimizer [34] with an initial learning rate\\nof 0.002 and weight decay of 0.05. YOLO-World is pre-\\ntrained for 100 epochs on on 32 NVIDIA V100 GPUs with\\na total batch size of 512. During pre-training, we follow\\nprevious works [20] and adopt color augmentation, random\\naffine, random flip, and mosaic with 4 images for data aug-\\nmentation. The text encoder is frozen during pre-training.\\n5', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 4}),\n",
       " Document(page_content='Dataset Type V ocab. Images Anno.\\nObjects365V1 [46] Detection 365 609k 9,621k\\nGQA [17] Grounding - 621k 3,681k\\nFlickr [38] Grounding - 149k 641k\\nCC3M †[47] Image-Text - 246k 821k\\nTable 1. Pre-training Data. The specifications of the datasets\\nused for pre-training YOLO-World.\\nPre-training Data. For pre-training YOLO-World, we\\nmainly adopt detection or grounding datasets including Ob-\\njects365 (V1) [46], GQA [17], Flickr30k [38], as specified\\nin Tab. 1. Following [24], we exclude the images from\\nthe COCO dataset in GoldG [21] (GQA and Flickr30k).\\nThe annotations of the detection datasets used for pre-\\ntraining contain both bounding boxes and categories or\\nnoun phrases. In addition, we also extend the pre-training\\ndata with image-text pairs, i.e., CC3M†[47], which we have\\nlabeled 246k images through the pseudo-labeling method\\ndiscussed in Sec. 3.4.\\nZero-shot Evaluation. After pre-training, we di-\\nrectly evaluate the proposed YOLO-World on the LVIS\\ndataset [14] in a zero-shot manner. The LVIS dataset\\ncontains 1203 object categories, which is much more\\nthan the categories of the pre-training detection datasets\\nand can measure the performance on large vocabulary\\ndetection. Following previous works [21, 24, 56, 57], we\\nmainly evaluate on LVIS minival [21] and report the\\nFixed AP [4] for comparison. The maximum number of\\npredictions is set to 1000.\\nMain Results on LVIS Object Detection. In Tab. 2, we\\ncompare the proposed YOLO-World with recent state-of-\\nthe-art methods [21, 30, 56, 57, 59] on LVIS benchmark in a\\nzero-shot manner. Considering the computation burden and\\nmodel parameters, we mainly compare with those methods\\nbased on lighter backbones, e.g., Swin-T [32]. Remarkably,\\nYOLO-World outperforms previous state-of-the-art meth-\\nods in terms of zero-shot performance and inference speed.\\nCompared to GLIP, GLIPv2, and Grounding DINO, which\\nincorporate more data, e.g., Cap4M (CC3M+SBU [37]),\\nYOLO-World pre-trained on O365 & GolG obtains bet-\\nter performance even with fewer model parameters. Com-\\npared to DetCLIP, YOLO-World achieves comparable per-\\nformance (35.4 v.s.34.4) while obtaining 20×increase in\\ninference speed. The experimental results also demonstrate\\nthat small models, e.g., YOLO-World-S with 13M parame-\\nters, can be used for vision-language pre-training and ob-\\ntain strong open-vocabulary capabilities.4.3. Ablation Experiments\\nWe provide extensive ablation studies to analyze YOLO-\\nWorld from two primary aspects, i.e., pre-training and ar-\\nchitecture. Unless specified, we mainly conduct ablation\\nexperiments based on YOLO-World-L and pre-train Ob-\\njects365 with zero-shot evaluation on LVIS minival .\\nPre-training Data. In Tab. 3, we evaluate the perfor-\\nmance of pre-training YOLO-World using different data.\\nCompared to the baseline trained on Objects365, adding\\nGQA can significantly improve performance with an 8.4\\nAP gain on LVIS. This improvement can be attributed to\\nthe richer textual information provided by the GQA dataset,\\nwhich can enhance the model’s ability to recognize large\\nvocabulary objects. Adding part of CC3M samples (8%\\nof the full datasets) can further bring 0.5 AP gain with 1.3\\nAP on rare objects. Tab. 3 demonstrates that adding more\\ndata can effectively improve the detection capabilities on\\nlarge-vocabulary scenarios. Furthermore, as the amount of\\ndata increases, the performance continues to improve, high-\\nlighting the benefits of leveraging larger and more diverse\\ndatasets for training.\\nAblations on RepVL-PAN. Tab. 4 demonstrates the ef-\\nfectiveness of the proposed RepVL-PAN of YOLO-World,\\nincluding Text-guided CSPLayers and Image Pooling At-\\ntention, for the zero-shot LVIS detection. Specifically, we\\nadopt two settings, i.e., (1) pre-training on O365 and (2)\\npre-training on O365 & GQA. Compared to O365 which\\nonly contains category annotations, GQA includes rich\\ntexts, particularly in the form of noun phrases. As shown\\nin Tab. 4, the proposed RepVL-PAN improves the base-\\nline (YOLOv8-PAN [20]) by 1.1 AP on LVIS, and the im-\\nprovements are remarkable in terms of the rare categories\\n(APr) of LVIS, which are hard to detect and recognize. In\\naddition, the improvements become more significant when\\nYOLO-World is pre-trained with the GQA dataset and ex-\\nperiments indicate that the proposed RepVL-PAN works\\nbetter with rich textual information.\\nText Encoders. In Tab. 5, we compare the performance\\nof using different text encoders, i.e., BERT-base [5] and\\nCLIP-base (ViT-base) [39]. We exploit two settings dur-\\ning pre-training, i.e., frozen and fine-tuned, and the learn-\\ning rate for fine-tuning text encoders is a 0.01×factor of\\nthe basic learning rate. As Tab. 5 shows, the CLIP text\\nencoder obtains superior results than BERT (+10.1 AP for\\nrare categories in LVIS), which is pre-trained with image-\\ntext pairs and has better capability for vision-centric embed-\\ndings. Fine-tuning BERT during pre-training brings signifi-\\ncant improvements (+3.7 AP) while fine-tuning CLIP leads\\nto a severe performance drop. We attribute the drop to that\\n6', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 5}),\n",
       " Document(page_content='Method Backbone Params Pre-trained Data FPS AP AP rAPcAPf\\nMDETR [21] R-101 [15] 169M GoldG - 24.2 20.9 24.3 24.2\\nGLIP-T [24] Swin-T [32] 232M O365,GoldG 0.12 24.9 17.7 19.5 31.0\\nGLIP-T [24] Swin-T [32] 232M O365,GoldG,Cap4M 0.12 26.0 20.8 21.4 31.0\\nGLIPv2-T [59] Swin-T [32] 232M O365,GoldG 0.12 26.9 - - -\\nGLIPv2-T [59] Swin-T [32] 232M O365,GoldG,Cap4M 0.12 29.0 - - -\\nGrounding DINO-T [30] Swin-T [32] 172M O365,GoldG 1.5 25.6 14.4 19.6 32.2\\nGrounding DINO-T [30] Swin-T [32] 172M O365,GoldG,Cap4M 1.5 27.4 18.1 23.3 32.7\\nDetCLIP-T [56] Swin-T [32] 155M O365,GoldG 2.3 34.4 26.9 33.9 36.3\\nYOLO-World-S YOLOv8-S 13M (77M) O365,GoldG 74.1 (19.9) 26.2 19.1 23.6 29.8\\nYOLO-World-M YOLOv8-M 29M (92M) O365,GoldG 58.1 (18.5) 31.0 23.8 29.2 33.9\\nYOLO-World-L YOLOv8-L 48M (110M) O365,GoldG 52.0 (17.6) 35.0 27.1 32.8 38.3\\nYOLO-World-L YOLOv8-L 48M (110M) O365,GoldG,CC3M†52.0 (17.6) 35.4 27.6 34.1 38.0\\nTable 2. Zero-shot Evaluation on LVIS. We evaluate YOLO-World on LVIS minival [21] in a zero-shot manner. We report the Fixed\\nAP[4] for a fair comparison with recent methods.†denotes the pseudo-labeled CC3M in our setting, which contains 246k samples.\\nThe FPS is evaluated on one NVIDIA V100 GPU w/o TensorRT. The parameters and FPS of YOLO-World are evaluated for both the\\nre-parameterized version (w/o bracket) and the original version (w/ bracket).\\nPre-trained Data AP AP rAPcAPf\\nO365 23.5 16.2 21.1 27.0\\nO365,GQA 31.9 22.5 29.9 35.4\\nO365,GoldG 32.5 22.3 30.6 36.0\\nO365,GoldG,CC3M†33.0 23.6 32.0 35.5\\nTable 3. Ablations on Pre-training Data. We evaluate the zero-\\nshot performance on LVIS of pre-training YOLO-World with dif-\\nferent amounts of data.\\nGQA T →I I→T AP AP rAPcAPf\\n✗ ✗ ✗ 22.4 14.5 20.1 26.0\\n✗✓ ✗ 23.2 15.2 20.6 27.0\\n✗✓ ✓ 23.5 16.2 21.1 27.0\\n✓ ✗ ✗ 29.7 21.0 27.1 33.6\\n✓ ✓ ✓ 31.9 22.5 29.9 35.4\\nTable 4. Ablations on Re-parameterizable Vision-Language\\nPath Aggregation Network. We evaluate the zero-shot perfor-\\nmance on LVIS of the proposed Vision-Language Path Aggrega-\\ntion Network. T →I and I→T denote the Text-guided CSPLayers\\nand Image-Pooling Attention, respectively.\\nfine-tuning on O365 may degrade the generalization ability\\nof the pre-trained CLIP, which contains only 365 categories\\nand lacks abundant textual information.\\n4.4. Fine-tuning YOLO-World\\nIn this section, we further fine-tune YOLO-World for close-\\nset object detection on the COCO dataset and LVIS datasetText Encoder Frozen? AP AP rAPcAPf\\nBERT-base Frozen 14.6 3.4 10.7 20.0\\nBERT-base Fine-tune 18.3 6.6 14.6 23.6\\nCLIP-base Frozen 22.4 14.5 20.1 26.0\\nCLIP-base Fine-tune 19.3 8.6 15.7 24.8\\nTable 5. Text Encoder in YOLO-World. We ablate different text\\nencoders in YOLO-World through the zero-shot LVIS evaluation.\\nto demonstrate the effectiveness of the pre-training.\\nExperimental Setup. We use the pre-trained weights to\\ninitialize YOLO-World for fine-tuning. All models are fine-\\ntuned for 80 epochs with the AdamW optimizer and the ini-\\ntial learning rate is set to 0.0002. In addition, we fine-tune\\nthe CLIP text encoder with a learning factor of 0.01. For the\\nLVIS dataset, we follow previous works [8, 13, 63] and fine-\\ntune YOLO-World on the LVIS-base (common & frequent)\\nand evaluate it on the LVIS-novel (rare).\\nCOCO Object Detection. We compare the pre-trained\\nYOLO-World with previous YOLO detectors [20, 23, 52]\\nin Tab. 6. For fine-tuning YOLO-World on the COCO\\ndataset, we remove the proposed RepVL-PAN for fur-\\nther acceleration considering that the vocabulary size of\\nthe COCO dataset is small. In Tab. 6, it’s evident that\\nour approach can achieve decent zero-shot performance on\\nthe COCO dataset, which indicates that YOLO-World has\\nstrong generalization ability. Moreover, YOLO-World af-\\nter fine-tuning on the COCO train2017 demonstrates\\n7', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 6}),\n",
       " Document(page_content='Method Pre-train AP AP 50AP75FPS\\nTraining from scratch.\\nYOLOv6-S [23] ✗ 43.7 60.8 47.0 442\\nYOLOv6-M [23] ✗ 48.4 65.7 52.7 277\\nYOLOv6-L [23] ✗ 50.7 68.1 54.8 166\\nYOLOv7-T [52] ✗ 37.5 55.8 40.2 404\\nYOLOv7-L [52] ✗ 50.9 69.3 55.3 182\\nYOLOv7-X [52] ✗ 52.6 70.6 57.3 131\\nYOLOv8-S [20] ✗ 44.4 61.2 48.1 386\\nYOLOv8-M [20] ✗ 50.5 67.3 55.0 238\\nYOLOv8-L [20] ✗ 52.9 69.9 57.7 159\\nZero-shot transfer.\\nYOLO-World-S O+G 37.6 52.3 40.7 -\\nYOLO-World-M O+G 42.8 58.3 46.4 -\\nYOLO-World-L O+G 44.4 59.8 48.3 -\\nYOLO-World-L O+G+C 45.1 60.7 48.9 -\\nFine-tuned w/ RepVL-PAN.\\nYOLO-World-S O+G 45.9 62.3 50.1 -\\nYOLO-World-M O+G 51.2 68.1 55.9 -\\nYOLO-World-L O+G+C 53.3 70.1 58.2 -\\nFine-tuned w/o RepVL-PAN.\\nYOLO-World-S O+G 45.7 62.3 49.9 373\\nYOLO-World-M O+G 50.7 67.2 55.1 231\\nYOLO-World-L O+G+C 53.3 70.3 58.1 156\\nTable 6. Comparison with YOLOs on COCO Object Detec-\\ntion. We fine-tune the YOLO-World on COCO train2017 and\\nevaluate on COCO val2017 . The results of YOLOv7 [52] and\\nYOLOv8 [20] are obtained from MMYOLO [3]. ‘O’, ‘G’, and ‘C’\\ndenote pertaining using Objects365, GoldG, and CC3M†, respec-\\ntively. The FPS is measured on one NVIDIA V100 w/ TensorRT.\\nhigher performance compared to previous methods trained\\nfrom scratch.\\nLVIS Object Detection. In Tab. 7, we evaluate the fine-\\ntuning performance of YOLO-World on the standard LVIS\\ndataset. Firstly, compared to the oracle YOLOv8s [20]\\ntrained on the full LVIS datasets, YOLO-World achieves\\nsignificant improvements, especially for larger models, e.g.,\\nYOLO-World-L outperforms YOLOv8-L by 7.2 AP and\\n10.2 AP r. The improvements can demonstrate the effec-\\ntiveness of the proposed pre-training strategy for large-\\nvocabulary detection. Moreover, YOLO-World, as an effi-\\ncient one-stage detector, outperforms previous state-of-the-\\nart two-stage methods [8, 13, 22, 53, 63] on the overall per-\\nformance without extra designs, e.g., learnable prompts [8]\\nor region-based alginments [13].Method AP AP r APcAPf\\nViLD [13] 27.8 16.7 26.5 34.2\\nRegionCLIP [62] 28.2 17.1 - -\\nDetic [63] 26.8 17.8 - -\\nFVLM [22] 24.2 18.6 - -\\nDetPro [8] 28.4 20.8 27.8 32.4\\nBARON [53] 29.5 23.2 29.3 32.5\\nYOLOv8-S 19.4 7.4 17.4 27.0\\nYOLOv8-M 23.1 8.4 21.3 31.5\\nYOLOv8-L 26.9 10.2 25.4 35.8\\nYOLO-World-S 23.9 12.8 20.4 32.7\\nYOLO-World-M 28.8 15.9 24.6 39.0\\nYOLO-World-L 34.1 20.4 31.1 43.5\\nTable 7. Comparison with Open-Vocabulary Detectors on\\nLVIS. We train YOLO-World on the LVIS-base (including com-\\nmon and frequent) report the bbox AP . The YOLO-v8 are trained\\non the full LVIS datasets (including base and novel) along with the\\nclass balanced sampling.\\n4.5. Open-Vocabulary Instance Segmentation\\nIn this section, we further fine-tune YOLO-World for\\nsegmenting objects under the open-vocabulary setting,\\nwhich can be termed open-vocabulary instance segmenta-\\ntion (OVIS). Previous methods [18] have explored OVIS\\nwith pseudo-labelling on novel objects. Differently, con-\\nsidering that YOLO-World has strong transfer and gener-\\nalization capabilities, we directly fine-tune YOLO-World\\non a subset of data with mask annotations and evaluate the\\nsegmentation performance under large-vocabulary settings.\\nSpecifically, we benchmark open-vocabulary instance seg-\\nmentation under two settings:\\n• (1) COCO to LVIS setting, we fine-tune YOLO-World on\\nthe COCO dataset (including 80 categories) with mask\\nannotations, under which the models need to transfer\\nfrom 80 categories to 1203 categories ( 80→1203 );\\n• (2) LVIS-base to LVIS setting, we fine-tune YOLO-World\\non the LVIS-base (including 866 categories, common &\\nfrequent) with mask annotations, under which the models\\nneed to transfer from 866 categories to 1203 categories\\n(866→1203 ).\\nWe evaluate the fine-tuned models on the standard LVIS\\nval2017 with 1203 categories, in which 337 rare cate-\\ngories are unseen and can be used to measure the open-\\nvocabulary performance.\\nResults. Tab. 8 shows the experimental results of extend-\\ning YOLO-World for open-vocabulary instance segmenta-\\ntion. Specifically, we adopt two fine-tuning strategies: (1)\\nonly fine-tuning the segmentation head and (2) fine-tuning\\n8', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 7}),\n",
       " Document(page_content='all modules. Under strategy (1), the fine-tuned YOLO-\\nWorld still retains the zero-shot capabilities acquired from\\nthe pre-training stage, allowing it to generalize to unseen\\ncategories without additional fine-tuning. Strategy (2) en-\\nables YOLO-World fit the LVIS dataset better, but it may\\nresult in the degradation of the zero-shot capabilities.\\nTab. 8 shows the comparisons of fine-tuning YOLO-\\nWorld with different settings (COCO or LVIS-base) and\\ndifferent strategies (fine-tuning seg. head or fine-tuning\\nall). Firstly, fine-tuning on LVIS-base obtains better perfor-\\nmance compared to that based on COCO. However, the ra-\\ntios between AP and AP r(APr/AP) are nearly unchanged,\\ne.g., the ratios of YOLO-World on COCO and LVIS-base\\nare 76.5% and 74.3%, respectively. Considering that the\\ndetector is frozen, we attribute the performance gap to the\\nfact that the LVIS dataset provides more detailed and denser\\nsegmentation annotations, which are beneficial for learn-\\ning the segmentation head. When fine-tuning all mod-\\nules, YOLO-World obtains remarkable improvements on\\nLVIS, e.g., YOLO-World-L achieves 9.6 AP gain. However,\\nthe fine-tuning might degrade the open-vocabulary perfor-\\nmance and lead to a 0.6 box AP rdrop for YOLO-World-L.\\n4.6. Visualizations\\nWe provide the visualization results of pre-trained YOLO-\\nWorld-L under three settings: (a) we perform zero-shot\\ninference with LVIS categories; (b) we input the custom\\nprompts with fine-grained categories with attributes; (c) re-\\nferring detection. The visualizations also demonstrate that\\nYOLO-World has a strong generalization ability for open-\\nvocabulary scenarios along with referring ability.\\nZero-shot Inference on LVIS. Fig. 5 shows the visual-\\nization results based on the LVIS categories which are gen-\\nerated by the pre-trained YOLO-World-L in a zero-shot\\nmanner. The pre-trained YOLO-World exhibits strong zero-\\nshot transfer capabilities and is able to detect as many ob-\\njects as possible within the image.\\nInference with User’s Vocabulary. In Fig. 6, we explore\\nthe detection capabilities of YOLO-World with our defined\\ncategories. The visualization results demonstrate that the\\npre-trained YOLO-World-L also exhibits the capability for\\n(1) fine-grained detection ( i.e., detect the parts of one ob-\\nject) and (2) fine-grained classification ( i.e., distinguish dif-\\nferent sub-categories of objects.).\\nReferring Object Detection. In Fig. 7, we leverage some\\ndescriptive (discriminative) noun phrases as input, e.g., the\\nstanding person, to explore whether the model can locate\\nregions or objects in the image that match our given in-\\nput. The visualization results display the phrases and theircorresponding bounding boxes, demonstrating that the pre-\\ntrained YOLO-World has the referring or grounding capa-\\nbility. This ability can be attributed to the proposed pre-\\ntraining strategy with large-scale training data.\\n5. Conclusion\\nWe present YOLO-World, a cutting-edge real-time open-\\nvocabulary detector aiming to improve efficiency and open-\\nvocabulary capability in real-world applications. In this pa-\\nper, we have reshaped the prevalent YOLOs as a vision-\\nlanguage YOLO architecture for open-vocabulary pre-\\ntraining and detection and proposed RepVL-PAN, which\\nconnects vision and language information with the network\\nand can be re-parameterized for efficient deployment. We\\nfurther present the effective pre-training schemes with de-\\ntection, grounding and image-text data to endow YOLO-\\nWorld with a strong capability for open-vocabulary de-\\ntection. Experiments can demonstrate the superiority of\\nYOLO-World in terms of speed and open-vocabulary per-\\nformance and indicate the effectiveness of vision-language\\npre-training on small models, which is insightful for future\\nresearch. We hope YOLO-World can serve as a new bench-\\nmark for addressing real-world open-vocabulary detection.\\nReferences\\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\\nto-end object detection with transformers. In ECCV , pages\\n213–229, 2020. 2\\n[2] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,\\nJifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\\nChen Change Loy, and Dahua Lin. MMDetection: Open\\nmmlab detection toolbox and benchmark. arXiv preprint\\narXiv:1906.07155 , 2019. 5\\n[3] MMYOLO Contributors. MMYOLO: OpenMMLab YOLO\\nseries toolbox and benchmark. https://github.com/\\nopen-mmlab/mmyolo , 2022. 5, 8\\n[4] Achal Dave, Piotr Doll ´ar, Deva Ramanan, Alexander Kir-\\nillov, and Ross B. Girshick. Evaluating large-vocabulary\\nobject detectors: The devil is in the details. CoRR ,\\nabs/2102.01066, 2021. 6, 7\\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\\nToutanova. BERT: pre-training of deep bidirectional trans-\\nformers for language understanding. In NAACL-HLT , pages\\n4171–4186, 2019. 1, 4, 6\\n[6] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,\\nGuiguang Ding, and Jian Sun. Repvgg: Making vgg-style\\nconvnets great again. In CVPR , pages 13733–13742, 2021.\\n2\\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\n9', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 8}),\n",
       " Document(page_content='Model Fine-tune Data Fine-tune Modules AP AP rAPcAPfAPbAPb\\nr\\nYOLO-World-M COCO Seg Head 12.3 9.1 10.9 14.6 22.3 16.2\\nYOLO-World-L COCO Seg Head 16.2 12.4 15.0 19.2 25.3 18.0\\nYOLO-World-M LVIS-base Seg Head 16.7 12.6 14.6 20.8 22.3 16.2\\nYOLO-World-L LVIS-base Seg Head 19.1 14.2 17.2 23.5 25.3 18.0\\nYOLO-World-M LVIS-base All 25.9 13.4 24.9 32.6 32.6 15.8\\nYOLO-World-L LVIS-base All 28.7 15.0 28.3 35.2 36.2 17.4\\nTable 8. Open-Vocabulary Instance Segmentation. We evaluate YOLO-World for open-vocabulary instance segmentation under the two\\nsettings. We fine-tune the segmentation head or all modules of YOLO-World and report Mask AP for comparison. APbdenotes the box\\nAP.\\nFigure 5. Visualization Results on Zero-shot Inference on LVIS. We adopt the pre-trained YOLO-World-L and infer with the LVIS\\nvocabulary (containing 1203 categories) on the COCO val2017 .\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\nworth 16x16 words: Transformers for image recognition at\\nscale. In ICLR , 2021. 3\\n[8] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao,\\nand Guoqi Li. Learning to prompt for open-vocabulary ob-\\nject detection with vision-language model. In CVPR , pages\\n14064–14073, 2022. 1, 3, 7, 8\\n[9] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R. Scott,\\nand Weilin Huang. TOOD: task-aligned one-stage object de-\\ntection. In ICCV , pages 3490–3499. IEEE, 2021. 5\\n[10] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: exceeding YOLO series in 2021. CoRR ,\\nabs/2107.08430, 2021. 2\\n[11] Ross B. Girshick. Fast R-CNN. In ICCV , pages 1440–1448,\\n2015. 2\\n[12] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\\nMalik. Rich feature hierarchies for accurate object detection\\nand semantic segmentation. In CVPR , pages 580–587, 2014.\\n2\\n[13] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.Open-vocabulary object detection via vision and language\\nknowledge distillation. In ICLR , 2022. 1, 2, 7, 8\\n[14] Agrim Gupta, Piotr Doll ´ar, and Ross B. Girshick. LVIS: A\\ndataset for large vocabulary instance segmentation. In CVPR ,\\npages 5356–5364, 2019. 6\\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In CVPR ,\\npages 770–778, 2016. 7\\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B.\\nGirshick. Mask R-CNN. In ICCV , pages 2980–2988, 2017.\\n1, 2\\n[17] Drew A. Hudson and Christopher D. Manning. GQA: A new\\ndataset for real-world visual reasoning and compositional\\nquestion answering. In CVPR , pages 6700–6709, 2019. 6\\n[18] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan\\nElhamifar. Open-vocabulary instance segmentation via ro-\\nbust cross-modal pseudo-labeling. In CVPR , pages 7010–\\n7021, 2022. 8\\n[19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\\nHieu Pham, Quoc V . Le, Yun-Hsuan Sung, Zhen Li, and Tom\\nDuerig. Scaling up visual and vision-language representation\\n10', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 9}),\n",
       " Document(page_content='{men, women, boy, girl}{golden dog, black dog, spotted dog}{elephant, ear, leg, trunk, ivory}{grass, sky, zebra, trunk, tree}Figure 6. Visualization Results on User’s Vocabulary. We define the custom vocabulary for each input image and YOLO-World can\\ndetect the accurate regions according to the vocabulary. Images are obtained from COCO val2017 .\\nthe person in redthe brown animalthe tallest personperson with a white shirt\\nmoonthe standing personperson holding a toyperson holding a baseball batthe jumping person\\nFigure 7. Visualization Results on Referring Object Detection. We explore the capability of the pre-trained YOLO-World to detect\\nobjects with descriptive noun phrases. Images are obtained from COCO val2017 .\\nlearning with noisy text supervision. In ICML , pages 4904–\\n4916, 2021. 1, 3\\n[20] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralyt-\\nics yolov8. https://github.com/ultralytics/\\nultralytics , 2023. 2, 3, 4, 5, 6, 7, 8\\n[21] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\\nSynnaeve, Ishan Misra, and Nicolas Carion. MDETR - mod-\\nulated detection for end-to-end multi-modal understanding.\\nInICCV , pages 1760–1770, 2021. 6, 7\\n[22] Weicheng Kuo, Yin Cui, Xiuye Gu, A. J. Piergiovanni,\\nand Anelia Angelova. F-VLM: open-vocabulary object de-\\ntection upon frozen vision and language models. CoRR ,\\nabs/2209.15639, 2022. 3, 8\\n[23] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei\\nGeng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, Yiduo Li, Bo Zhang, Yufei Liang, Linyuan\\nZhou, Xiaoming Xu, Xiangxiang Chu, Xiaoming Wei, and\\nXiaolin Wei. Yolov6: A single-stage object detection frame-work for industrial applications. CoRR , abs/2209.02976,\\n2022. 2, 7, 8\\n[24] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\\nYuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and\\nJianfeng Gao. Grounded language-image pre-training. In\\nCVPR , pages 10955–10965, 2022. 1, 2, 3, 5, 6, 7, 13\\n[25] Chuang Lin, Peize Sun, Yi Jiang, Ping Luo, Lizhen Qu, Gho-\\nlamreza Haffari, Zehuan Yuan, and Jianfei Cai. Learning\\nobject-language alignments for open-vocabulary object de-\\ntection. In ICLR , 2023. 3\\n[26] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\\nC. Lawrence Zitnick. Microsoft COCO: common objects\\nin context. In Proceedings of the European Conference on\\nComputer Vision (ECCV) , pages 740–755, 2014. 1, 2, 3, 13\\n[27] Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He,\\nBharath Hariharan, and Serge J. Belongie. Feature pyramid\\n11', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 10}),\n",
       " Document(page_content='networks for object detection. In CVPR , pages 936–944,\\n2017. 1, 2\\n[28] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,\\nand Piotr Doll ´ar. Focal loss for dense object detection. In\\nICCV , pages 2999–3007, 2017. 2\\n[29] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\\nPath aggregation network for instance segmentation. In\\nCVPR , pages 8759–8768, 2018. 2, 4\\n[30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\\nZhu, and Lei Zhang. Grounding DINO: marrying DINO with\\ngrounded pre-training for open-set object detection. CoRR ,\\nabs/2303.05499, 2023. 1, 2, 3, 6, 7\\n[31] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\\nSzegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C.\\nBerg. SSD: single shot multibox detector. In ECCV , pages\\n21–37, 2016. 2\\n[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nICCV , pages 9992–10002, 2021. 2, 3, 6, 7\\n[33] Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang,\\nQingqing Dang, Yuan Gao, Hui Shen, Jianguo Ren, Shumin\\nHan, Errui Ding, and Shilei Wen. PP-YOLO: an effec-\\ntive and efficient implementation of object detector. CoRR ,\\nabs/2007.12099, 2020. 2\\n[34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. In ICLR , 2019. 5\\n[35] Matthias Minderer, Alexey A. Gritsenko, Austin Stone,\\nMaxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy,\\nAravindh Mahendran, Anurag Arnab, Mostafa Dehghani,\\nZhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and\\nNeil Houlsby. Simple open-vocabulary object detection with\\nvision transformers. In ECCV , 2022. 3\\n[36] Matthias Minderer, Alexey A. Gritsenko, and Neil Houlsby.\\nScaling open-vocabulary object detection. In NeurIPS , 2023.\\n3\\n[37] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\\nIm2text: Describing images using 1 million captioned pho-\\ntographs. In NeurIPS , pages 1143–1151, 2011. 6\\n[38] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes,\\nJuan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.\\nFlickr30k entities: Collecting region-to-phrase correspon-\\ndences for richer image-to-sentence models. Int. J. Comput.\\nVis., pages 74–93, 2017. 6\\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\\nKrueger, and Ilya Sutskever. Learning transferable visual\\nmodels from natural language supervision. In ICML , pages\\n8748–8763, 2021. 1, 2, 3, 4, 5, 6, 13\\n[40] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In CVPR , pages 6517–6525, 2017. 2\\n[41] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\\nimprovement. CoRR , abs/1804.02767, 2018. 2\\n[42] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick,\\nand Ali Farhadi. You only look once: Unified, real-time ob-\\nject detection. In CVPR , pages 779–788, 2016. 2, 3[43] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick,\\nand Ali Farhadi. You only look once: Unified, real-time ob-\\nject detection. In CVPR , pages 779–788, 2016. 1, 2, 4\\n[44] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\\nFaster R-CNN: towards real-time object detection with re-\\ngion proposal networks. IEEE Transactions on Pattern Anal-\\nysis and Machine Intelligence , pages 1137–1149, 2017. 2\\n[45] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\\nFaster R-CNN: towards real-time object detection with re-\\ngion proposal networks. IEEE Transactions on Pattern Anal-\\nysis and Machine Intelligence , pages 1137–1149, 2017. 1\\n[46] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:\\nA large-scale, high-quality dataset for object detection. In\\nICCV , pages 8429–8438, 2019. 2, 6\\n[47] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\\nage alt-text dataset for automatic image captioning. In ACL,\\npages 2556–2565, 2018. 5, 6, 13\\n[48] Cheng Shi and Sibei Yang. Edadet: Open-vocabulary ob-\\nject detection using early dense alignment. In ICCV , pages\\n15678–15688, 2023. 1\\n[49] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nfully convolutional one-stage object detection. In ICCV ,\\npages 9626–9635, 2019. 2\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS , pages\\n5998–6008, 2017. 2\\n[51] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A\\nnew backbone that can enhance learning capability of CNN.\\nInCVPRW , pages 1571–1580, 2020. 2\\n[52] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Yolov7: Trainable bag-of-freebies sets\\nnew state-of-the-art for real-time object detectors. In CVPR ,\\npages 7464–7475, 2023. 2, 7, 8\\n[53] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and\\nChen Change Loy. Aligning bag of regions for open-\\nvocabulary object detection. In CVPR , pages 15254–15264,\\n2023. 1, 3, 8\\n[54] Johnathan Xie and Shuai Zheng. ZSD-YOLO: zero-shot\\nYOLO detection using vision-language knowledgedistilla-\\ntion. CoRR , 2021. 3\\n[55] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,\\nCheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing\\nDang, Shengyu Wei, Yuning Du, and Baohua Lai.\\nPP-YOLOE: an evolved version of YOLO. CoRR ,\\nabs/2203.16250, 2022. 2\\n[56] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan\\nXu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu.\\nDetclip: Dictionary-enriched visual-concept paralleled pre-\\ntraining for open-world detection. In NeurIPS , 2022. 1, 2, 3,\\n6, 7\\n[57] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei\\nZhang, Zhenguo Li, and Hang Xu. Detclipv2: Scal-\\nable open-vocabulary object detection pre-training via word-\\n12', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 11}),\n",
       " Document(page_content='region alignment. In CVPR , pages 23497–23506, 2023. 1, 3,\\n6\\n[58] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-\\nFu Chang. Open-vocabulary object detection using captions.\\nInCVPR , pages 14393–14402, 2021. 1, 2, 3\\n[59] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\\nChen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu\\nYuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Uni-\\nfying localization and vision-language understanding. In\\nNeurIPS , 2022. 1, 2, 3, 6, 7\\n[60] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\\nZhu, Lionel M. Ni, and Heung-Yeung Shum. DINO: DETR\\nwith improved denoising anchor boxes for end-to-end object\\ndetection. In ICLR , 2023. 3\\n[61] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\\nStan Z. Li. Bridging the gap between anchor-based and\\nanchor-free detection via adaptive training sample selection.\\nInCVPR , pages 9756–9765, 2020. 2, 3\\n[62] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan\\nLi, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang\\nDai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip:\\nRegion-based language-image pretraining. In CVPR , pages\\n16772–16782, 2022. 3, 8\\n[63] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp\\nKr¨ahenb ¨uhl, and Ishan Misra. Detecting twenty-thousand\\nclasses using image-level supervision. In ECCV , pages 350–\\n368, 2022. 3, 7, 8\\n[64] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\\nand Jifeng Dai. Deformable DETR: deformable transformers\\nfor end-to-end object detection. In ICLR , 2021. 2\\nA. Additional Details\\nA.1. Re-parameterization for RepVL-PAN\\nDuring inference on an offline vocabulary, we adopt re-\\nparameterization for RepVL-PAN for faster inference speed\\nand deployment. Firstly, we pre-compute the text embed-\\ndings W∈RC×Dthrough the text encoder.\\nRe-parameterize T-CSPLayer. For each T-CSPLayer in\\nRepVL-PAN, we can re-parameterize and simplify the pro-\\ncess of adding text guidance by reshaping the text embed-\\ndings W∈RC×D×1×1into the weights of a 1×1convo-\\nlution layer (or a linear layer), as follows:\\nX′=X⊙Sigmoid (max(Conv (X, W ),dim=1 )),(4)\\nwhere X× ∈RB×D×H×WandX′∈RB×D×H×Ware\\nthe input and output image features. ⊙is the matrix multi-\\nplication with reshape ortranspose .\\nRe-parameterize I-Pooling Attention. The I-Pooling\\nAttention can be re-parameterize or simplified by:\\n˜X=cat(MP(X3,3),MP(X4,3),MP(X5,3)),(5)where cat is the concentration and MP(·, 3) denotes the\\nmax pooling for 3×3output features. {X3, X4, X5}are\\nthe multi-scale features in RepVL-PAN. ˜Xis flattened and\\nhas the shape of B×D×27. Then we can update the text\\nembeddings by:\\nW′=W+Softmax (W⊙˜X),dim=-1 )⊙W, (6)\\nA.2. Fine-tuning Details.\\nWe remove all T-CSPLayers and Image-Pooling Atten-\\ntion in RepVL-PAN when transferring YOLO-World to\\nCOCO [26] object detection, which only contains 80 cat-\\negories and has a relatively low dependency on visual-\\nlanguage interaction. During fine-tuning, we initialize\\nYOLO-World using pre-trained weights. The learning rate\\nof fine-tuning is set to 0.0002 with the weight decay set to\\n0.05. After fine-tuning, we pre-compute the class text em-\\nbeddings with given COCO categories and store the embed-\\ndings into the weights of the classification layers.\\nB. Automatic Labeling on Large-scale Image-\\nText Data\\nIn this section, we add details procedures for labeling\\nregion-text pairs with large-scale image-text data, e.g.,\\nCC3M [47]. The overall labeling pipeline is illustrated in\\nFig. 8, which mainly consists of three procedures, i.e., (1)\\nextract object nouns, (2) pseudo labeling, and (3) filtering.\\nAs discussed in Sec. 3.4, we adopt the simple n-gram algo-\\nrithm to extract nouns from captions.\\nRegion-Text Proposals. After obtaining the set of object\\nnouns T={tk}Kfrom the first step, we leverage a pre-\\ntrained open-vocabulary detector, i.e., GLIP-L [24], to gen-\\nerate pseudo boxes {Bi}along with confidence scores {ci}:\\n{Bi, ti, ci}N\\ni=1=GLIP-Labeler (I, T), (7)\\nwhere {Bi, ti, ci}N\\ni=1are the coarse region-text proposals.\\nCLIP-based Re-scoring & Filtering. Considering the\\nregion-text proposals containing much noise, we present\\na restoring and filtering pipeline with the pre-trained\\nCLIP [39]. Given the input image I, caption T, and\\nthe coarse region-text proposals {Bi, ti, ci}N\\ni=1, the specific\\npipeline is listed as follows:\\n• (1) Compute Image-Text Score: we forward the image I\\nwith its caption Tinto CLIP and obtain the image-text\\nsimilarity score simg.\\n• (2) Compute Region-Text Score: we crop the region im-\\nages from the input image according to the region boxes\\n{Bi}. Then we forward the cropped images along with\\ntheir texts {ti}into CLIP and obtain the region-text simi-\\nlarity Sr={sr\\ni}N\\ni=1.\\n13', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 12}),\n",
       " Document(page_content='Open-V ocabularyLabelerCLIP LabelerAutomatic Labeling Pipeline\\nimagePseudo LabelingCLIP-based Filteringimage, object nounsboxes\\nn-gramcaptionExtracting Object Nouns“A photography of a manand a woman”nouns\\nobjectsFigure 8. Labeling Pipeline for Image-Text Data We first leverage the simple n-gram to extract object nouns from the captions. We adopt\\na pre-trained open-vocabulary detector to generate pseudo boxes given the object nouns, which forms the coarse region-text proposals.\\nThen we use a pre-trained CLIP to rescore or relabel the boxes along with filtering.\\n• (3) [Optional] Re-Labeling: we can forward each\\ncropped image with all nouns and assign the noun with\\nmaximum similarity, which can help correct the texts\\nwrongly labeled by GLIP.\\n• (4) Rescoring: we adopt the region-text similarity Srto\\nrescore the confidence scores ˜ci=pci∗sr\\ni.\\n• (5) Region-level Filtering: we first divide the region-text\\nproposals into different groups according to the texts and\\nthen perform non-maximum suppression (NMS) to fil-\\nter the duplicate predictions (the NMS threshold is set to\\n0.5). Then we filter out the proposals with low confidence\\nscores (the threshold is set to 0.3).\\n• (6) Image-level Filtering: we compute the image-level\\nregion-text scores sregionby averaging the kept region-\\ntext scores. Then we obtain the image-level confidence\\nscore by s=√\\nsimg∗sregion and we keep the images\\nwith scores larger than 0.3.\\nThe thresholds mentioned above are empirically set accord-\\ning to the part of labeled results and the whole pipeline is\\nautomatic without human verification. Finally, the labeled\\nsamples are used for pre-training YOLO-World. We will\\nprovide the pseudo annotations of CC3M for further re-\\nsearch.\\nC. Pre-training YOLO-World at Scale\\nWhen pre-training small models, e.g., YOLO-World-S, a\\nnatural question we have is: how much capacity does a\\nsmall model have, and how much training data or what kind\\nof data does a small model need? To answer this question,\\nwe leverage different amounts of pseudo-labeled region-text\\npairs to pre-train YOLO-World. As shown in Tab. 9, adding\\nmore image-text samples can increase the zero-shot per-\\nformance of YOLO-World-S. Tab. 9 indicates: (1) adding\\nimage-text data can improve the overall zero-shot perfor-\\nmance of YOLO-World-S; (2) using an excessive amount\\nof pseudo-labeled data may have some negative effects for\\nsmall models (YOLO-World-S), though it can improve theon rare categories (AP r). However, using fine-grained an-\\nnotations (GoldG) for small models can provide significant\\nimprovements, which indicates that large-scale high-quality\\nannotated data can significantly enhance the capabilities of\\nsmall models. And Tab. 3 in the main text has shown that\\npre-training with the combination of fine-annotated data\\nand pseudo-annotated data can perform better. We will ex-\\nplore more about the data for pre-training small models or\\nYOLO detectors in future work.\\n14', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 13}),\n",
       " Document(page_content='Method Pre-trained Data Samples AP AP rAPcAPf\\nYOLO-World-S O365 0.61M 16.3 9.2 14.1 20.1\\nYOLO-World-S O365+GoldG 1.38M 24.2 16.4 21.7 27.8\\nYOLO-World-S O365+CC3M-245k 0.85M 16.5 10.8 14.8 19.1\\nYOLO-World-S O365+CC3M-520k 1.13M 19.2 10.7 17.4 22.4\\nYOLO-World-S O365+CC3M-750k 1.36M 18.2 11.2 16.0 21.1\\nTable 9. Zero-shot Evaluation on LVIS. We evaluate the performance of pre-training YOLO-World-S with different amounts of data, the\\nimage-text data.\\n15', metadata={'source': 'https://arxiv.org/pdf/2401.17270v3.pdf', 'page': 14})]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_documents))\n",
    "all_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval - Parent Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 0}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception occurred invoking consumer for subscription eb1adf5c5a0a434c95e7080e2ad4785ato topic persistent://default/default/56f0957e-f130-479b-bde4-fdf5c3a3883c 'utf-8' codec can't encode character '\\ud83d' in position 240: surrogates not allowed\n"
     ]
    }
   ],
   "source": [
    "retriever.add_documents(all_documents, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Neural Network Diffusion', metadata={'doc_id': '91775f58-7b31-4564-bcef-40b541433fae', 'page': 6, 'source': 'https://arxiv.org/pdf/2402.13144v1.pdf'}),\n",
       " Document(page_content='Neural Network Diffusion\\nFig. 1). i) Both neural network training and the reverse\\nprocess of diffusion models can be regarded as transitions\\nfrom random noise/initialization to specific distributions. ii)\\nHigh-quality images and high-performing parameters can\\nalso be degraded into simple distributions, such as Gaussian\\ndistribution, through multiple noise additions.', metadata={'doc_id': '3cdde3f8-bda4-4bc7-b5d9-2b3d4588b014', 'page': 1, 'source': 'https://arxiv.org/pdf/2402.13144v1.pdf'}),\n",
       " Document(page_content='ploration on the versatile use of diffusion models.\\n1. Introduction\\nThe origin of diffusion models can be traced back to\\nnon-equilibrium thermodynamics (Jarzynski, 1997; Sohl-\\nDickstein et al., 2015). Diffusion processes were first uti-\\nlized to progressively remove noise from inputs and generate\\nclear images in (Sohl-Dickstein et al., 2015). Later works,', metadata={'doc_id': '58dc29f6-6652-469b-b9d9-fc77cbee10b1', 'page': 0, 'source': 'https://arxiv.org/pdf/2402.13144v1.pdf'}),\n",
       " Document(page_content='et al., 2022), and Stable Diffusion (Rombach et al., 2022)\\nachieve photorealistic images adopted by artists.\\nDespite the great success of diffusion models in visual gener-\\nation, their potential in other domains remains relatively un-\\nderexplored. In this work, we demonstrate the surprising ca-\\npability of diffusion models in generating high-performing', metadata={'doc_id': '58dc29f6-6652-469b-b9d9-fc77cbee10b1', 'page': 0, 'source': 'https://arxiv.org/pdf/2402.13144v1.pdf'})]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"Diffusion\")\n",
    "sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_id': '91775f58-7b31-4564-bcef-40b541433fae', 'page': 6, 'source': 'https://arxiv.org/pdf/2402.13144v1.pdf'}\n",
      "{'doc_id': '3cdde3f8-bda4-4bc7-b5d9-2b3d4588b014', 'page': 1, 'source': 'https://arxiv.org/pdf/2402.13144v1.pdf'}\n",
      "{'doc_id': '58dc29f6-6652-469b-b9d9-fc77cbee10b1', 'page': 0, 'source': 'https://arxiv.org/pdf/2402.13144v1.pdf'}\n",
      "{'doc_id': '58dc29f6-6652-469b-b9d9-fc77cbee10b1', 'page': 0, 'source': 'https://arxiv.org/pdf/2402.13144v1.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for doc in sub_docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Neural Network Diffusion\\n/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001c/uni00000016/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000019/uni00000013/uni00000011/uni0000001c/uni0000001a/uni00000036/uni0000004c/uni00000050/uni0000004c/uni0000004f/uni00000044/uni00000055/uni0000004c/uni00000057/uni0000005c/uni00000024/uni00000050/uni00000052/uni00000051/uni0000004a/uni00000003/uni00000052/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f\\n/uni00000024/uni00000050/uni00000052/uni00000051/uni0000004a/uni00000003/uni00000053/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049\\n/uni00000025/uni00000048/uni00000057/uni0000005a/uni00000048/uni00000048/uni00000051/uni00000003/uni00000052/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003/uni00000053/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049\\n/uni00000025/uni00000048/uni00000057/uni0000005a/uni00000048/uni00000048/uni00000051/uni00000003/uni00000052/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003/uni00000053/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049/uni00000003/uni0000000b/uni00000050/uni00000044/uni0000005b/uni0000000c\\n(a) Similarity comparisons of original and p-\\ndiff models.\\n/uni00000013/uni00000011/uni0000001b/uni00000017 /uni00000013/uni00000011/uni0000001b/uni0000001c /uni00000013/uni00000011/uni0000001c/uni00000017 /uni00000013/uni00000011/uni0000001c/uni0000001c\\n/uni00000036/uni0000004c/uni00000050/uni0000004c/uni0000004f/uni00000044/uni00000055/uni0000004c/uni00000057/uni0000005c/uni0000001a/uni00000019/uni00000011/uni00000017/uni0000001a/uni00000019/uni00000011/uni00000018/uni0000001a/uni00000019/uni00000011/uni00000019/uni0000001a/uni00000019/uni00000011/uni0000001a/uni0000001a/uni00000019/uni00000011/uni0000001b/uni00000024/uni00000046/uni00000046/uni00000011\\n/uni00000032/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f/uni00000003/uni00000050/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000055/uni00000044/uni00000051/uni0000004a/uni00000048\\n/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni00000048/uni00000047\\n/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000010/uni00000044/uni00000047/uni00000047/uni00000048/uni00000047\\n/uni00000033/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049(b) Similarity comparisons of fine-tuned, noise-\\nadded, and p-diff models.\\n/uni00000032/uni00000055/uni0000004c/uni0000004a/uni0000004c/uni00000051/uni00000044/uni0000004f\\n/uni00000024/uni00000047/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048\\n/uni00000033/uni00000010/uni00000047/uni0000004c/uni00000049/uni00000049(c) t-SNE of the latent representations\\nof original, p-diff, and adding noise.\\nFigure 4. The similarity represents the Intersection of Union (IoU) over wrong predictions between/among two models (a) shows the\\ncomparisons in four cases: similarity among original models and p-diff models, similarity between original and p-diff models, and the\\nmaximum similarity (nearest neighbor) between original and p-diff models. (b) displays the accuracy and max similarity of fine-tuned,\\nnoise-added, and p-diff models. All the maximum similarities are calculated with the original models. (c) presents the t-SNE (Van der\\nMaaten et al., 2008) of latent representations of the original models, p-diff models, and adding noise operation.\\noutperform the original models. Besides, the similarities\\nbetween fine-tuned or noise-added and original models are\\nvery high, which indicates these two operations can not\\nobtain novel but high-performing models. However, our\\ngenerated models achieve diverse similarities and superior\\nperformances compared to the original models.\\nComparison of latent representations. In addition to pre-\\ndictions, we assess the distributions of latent representations\\nfor the original and generated models using t-SNE (Van der\\nMaaten et al., 2008). To identify the differences between\\nour approach and the operation of adding noise to the la-\\ntent representations of original models, we also include the\\nadding noise operation as a comparison in Fig. 4(c). The\\nadded noise is random Gaussian noise with an amplitude\\nof 0.1. One can find that p-diff can generate novel latent\\nrepresentations while adding noise just makes interpolation\\naround the latent representations of original models.\\nThe trajectories of p-diff process. We plot the generated\\nparameters of different time steps in the inference stage to\\nform trajectories to explore its generation process. Five\\ntrajectories (initialized by 5 random noise) are shown in\\nFig. 5(a). We also plot the average parameters of the origi-\\nnal models and their standard deviation (std). As the time\\nstep increases, the generated parameters are overall close\\nto the original models. Although we keep a narrow per-\\nformance range constraint for visualization, there is still a\\ncertain distance between the end points (orange triangles)\\nof trajectories and average parameters (five-pointed star).\\nAnother finding is that the five trajectories are diverse.\\nFrom memorizing to generate new parameters. To in-vestigate the impact of the number of original models ( K)\\non the diversity of generated models, we visualize the max\\nsimilarities between original and generated models with\\ndifferent Kin Fig. 5(b). Specifically, we continually gener-\\nate parameters until 50 models perform better than 76.5%\\nin all cases. The generated models almost memorize the\\noriginal model when K= 1, as indicated by the narrow sim-\\nilarity range and high value. The similarity range of these\\ngenerated models becomes larger as Kincreases, demon-\\nstrating our approach can generate parameters that perform\\ndifferently from the original models.\\n5. Related Work\\nDiffusion models. Diffusion models have achieved re-\\nmarkable results in visual generation. These methods (Ho\\net al., 2020; Dhariwal & Nichol, 2021; Ho et al., 2022;\\nPeebles & Xie, 2022; Hertz et al., 2023; Li et al., 2023)\\nare based on non-equilibrium thermodynamics (Jarzynski,\\n1997; Sohl-Dickstein et al., 2015), and the its pathway is\\nsimilar to GAN (Zhu et al., 2017; Isola et al., 2017; Brock\\net al., 2018a), V AE (Kingma & Welling, 2013; Razavi et al.,\\n2019), and flow-based model (Dinh et al., 2014; Rezende\\n& Mohamed, 2015). Diffusion models can be categorized\\ninto three main branches. The first branch focuses on en-\\nhancing the synthesis quality of diffusion models, exem-\\nplified by models like DALL ·E 2 (Ramesh et al., 2022),\\nImagen (Saharia et al., 2022), and Stable Diffusion (Rom-\\nbach et al., 2022). The second branch aims to improve\\nthe sampling speed, including DDIM (Song et al., 2021),\\nAnalytic-DPM (Bao et al., 2022), and DPM-Solver (Lu\\net al., 2022). The final branch involves reevaluating diffu-\\n7', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 6}),\n",
       " Document(page_content='Neural Network Diffusion\\nFig. 1). i) Both neural network training and the reverse\\nprocess of diffusion models can be regarded as transitions\\nfrom random noise/initialization to specific distributions. ii)\\nHigh-quality images and high-performing parameters can\\nalso be degraded into simple distributions, such as Gaussian\\ndistribution, through multiple noise additions.\\nBased on the observations above, we introduce a novel ap-\\nproach for parameter generation, named neural network\\ndiffusion ( p-diff , p stands for parameter), which employs a\\nstandard latent diffusion model to synthesize a new set of\\nparameters. That is motivated by the fact that the diffusion\\nmodel has the capability to transform a given random distri-\\nbution to a specific one. Our method is simple, comprising\\nan autoencoder and a standard latent diffusion model to\\nlearn the distribution of high-performing parameters. First,\\nfor a subset of parameters of models trained by the SGD\\noptimizer, the autoencoder is trained to extract the latent\\nrepresentations for these parameters. Then, we leverage a\\nstandard latent diffusion model to synthesize latent represen-\\ntations from random noise. Finally, the synthesized latent\\nrepresentations are passed through the trained autoencoder’s\\ndecoder to yield new high-performing model parameters.\\nOur approach has the following characteristics: i) It consis-\\ntently achieves similar, even enhanced performance than its\\ntraining data, i.e.,models trained by SGD optimizer, across\\nmultiple datasets and architectures within seconds. ii) Our\\ngenerated models have great differences from the trained\\nmodels, which illustrates our approach can synthesize new\\nparameters instead of memorizing the training samples. We\\nhope our research can provide fresh insights into expanding\\nthe applications of diffusion models to other domains.\\n2. Nerual Network Diffusion\\n2.1. Preliminaries of diffusion models\\nDiffusion models typically consist of forward and reverse\\nprocesses in a multi-step chain indexed by timesteps. We\\nintroduce these two processes in the following.\\nForward process. Given a sample x0∼q(x), the forward\\nprocess progressively adds Gaussian noise for Tsteps and\\nobtain x1, x2,···, xT. The formulation of this process can\\nbe written as follows,\\nq(xt|xt−1) =N(xt;p\\n1−βtxt−1, βtI),\\nq(x1:T|x0) =TY\\nt=1q(xt|xt−1),(1)\\nwhere qandNrepresent forward process and adding Gaus-\\nsian noise parameterized by βt, and Iis the identity matrix.\\nReverse process. Different from the forward process, thereverse process aims to train a denoising network to recur-\\nsively remove the noise from xt. It moves backward on the\\nmulti-step chain as t decreases from Tto 0. Mathematically,\\nthe reverse process can be formulated as follows,\\npθ(xt−1|xt) =N(xt−1;µθ(xt, t),Σθ(xt, t)),\\npθ(x0:T) =p(xT)TY\\nt=1pθ(xt−1|xt),(2)\\nwhere prepresents the reverse process, µθ(xt, t)and\\nΣθ(xt, t))are the Gaussian mean and variance that esti-\\nmated by the denoising network parameter θ. The denoising\\nnetwork in the reverse process is optimized by the standard\\nnegative log-likelihood:\\nLdm=DKL(q(xt−1|xt, x0)||pθ(xt−1|xt)), (3)\\nwhere the DKL(·||·)denotes the Kullback–Leibler (KL)\\ndivergence that is normally used to compute the difference\\nbetween two distributions.\\nTraining and inference procedures. The goal of the train-\\ning diffusion model is to find the reverse transitions that\\nmaximize the likelihood of the forward transitions in each\\ntime step t. In practice, training equivalently consists of\\nminimizing the variational upper bound. The inference pro-\\ncedure aims to generate novel samples from random noise\\nvia the optimized denoising parameters θ∗and the multi-\\nstep chains in the reverse process.\\n2.2. Overview\\nWe propose neural network diffusion (p-diff), which aims\\nto generate high-performing parameters from random noise.\\nAs illustrated in Fig. 2, our method consists of two processes,\\nnamed parameter autoencoder and generation. Given a set\\nof trained high-performing models, we first select a subset\\nof these parameters and flatten them into 1-dimensional\\nvectors. Subsequently, we introduce an encoder to extract\\nlatent representations from these vectors, accompanied by a\\ndecoder responsible for reconstructing the parameters from\\nlatent representations. Then, a standard latent diffusion\\nmodel is trained to synthesize latent representations from\\nrandom noise. After training, we utilize p-diff to generate\\nnew parameters via the following chain: random noise →\\nreverse process →trained decoder →generated parameters.\\n2.3. Parameter autoencoder\\nPreparing the data for training the autoencoder. In our\\npaper, we default to synthesizing a subset of model pa-\\nrameters. Therefore, to collect the training data for the\\nautoencoder, we train a model from scratch and densely\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 1}),\n",
       " Document(page_content='Neural Network Diffusion\\nKai Wang1Zhaopan Xu1Yukun Zhou1Zelin Zang1Trevor Darrell2Zhuang Liu* 3Yang You* 1\\nCode: https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion\\nAbstract\\nDiffusion models have achieved remarkable suc-\\ncess in image and video generation. In this work,\\nwe demonstrate that diffusion models can also\\ngenerate high-performing neural network param-\\neters . Our approach is simple, utilizing an au-\\ntoencoder and a standard latent diffusion model.\\nThe autoencoder extracts latent representations\\nof a subset of the trained network parameters.\\nA diffusion model is then trained to synthesize\\nthese latent parameter representations from ran-\\ndom noise. It then generates new representations\\nthat are passed through the autoencoder’s decoder,\\nwhose outputs are ready to use as new subsets of\\nnetwork parameters. Across various architectures\\nand datasets, our diffusion process consistently\\ngenerates models of comparable or improved per-\\nformance over trained networks, with minimal\\nadditional cost. Notably, we empirically find that\\nthe generated models perform differently with the\\ntrained networks. Our results encourage more ex-\\nploration on the versatile use of diffusion models.\\n1. Introduction\\nThe origin of diffusion models can be traced back to\\nnon-equilibrium thermodynamics (Jarzynski, 1997; Sohl-\\nDickstein et al., 2015). Diffusion processes were first uti-\\nlized to progressively remove noise from inputs and generate\\nclear images in (Sohl-Dickstein et al., 2015). Later works,\\nsuch as DDPM (Ho et al., 2020) and DDIM (Song et al.,\\n2021), refine diffusion models, with a training paradigm\\ncharacterized by forward and reverse processes.\\nAt that time, the quality of images generated by diffu-\\nsion models had not yet reached a desired level. Guided-\\nDiffusion (Dhariwal & Nichol, 2021) conducts sufficient\\nablations and finds a better architecture, which represents\\nthe pioneering effort to elevate diffusion models beyond\\nGAN-based methods (Zhu et al., 2017; Isola et al., 2017) in\\n*Equal advising,1National University of Singapore2University\\nof California, Berkeley3Meta AI Research.\\narXiv preprint\\nImage\\nNoiseForward ProcessReverse ProcessModel\\nInitial.Adding NoiseSGD Optimization\\nAcc:76.6Acc:64.0Acc:42.1Acc:1.4\\nmin.maxFigure 1. The top: illustrates the standard diffusion process in\\nimage generation. The bottom: denotes the parameter distribution\\nof batch normalization (BN) during the training CIFAR-100 with\\nResNet-18. The upper half of the bracket: BN weights. The lower\\nhalf of the bracket: BN biases.\\nterms of image quality. Subsequently, GLIDE (Nichol et al.,\\n2021), Imagen (Saharia et al., 2022), DALL ·E 2 (Ramesh\\net al., 2022), and Stable Diffusion (Rombach et al., 2022)\\nachieve photorealistic images adopted by artists.\\nDespite the great success of diffusion models in visual gener-\\nation, their potential in other domains remains relatively un-\\nderexplored. In this work, we demonstrate the surprising ca-\\npability of diffusion models in generating high-performing\\nmodel parameters , a task fundamentally distinct from tradi-\\ntional visual generation. Parameter generation focuses on\\ncreating neural network parameters that can perform well on\\ngiven tasks. It has been explored from prior and probability\\nmodeling aspects, i.e.stochastic neural network (Sompolin-\\nsky et al., 1988; Bottou et al., 1991; Wong, 1991; Schmidt\\net al., 1992; Murata et al., 1994) and Bayesian neural net-\\nwork (Neal, 2012; Kingma & Welling, 2013; Rezende et al.,\\n2014; Kingma et al., 2015; Gal & Ghahramani, 2016). How-\\never, using a diffusion model in parameter generation has\\nnot been well-explored yet.\\nTaking a closer look at the neural network training and dif-\\nfusion models, the diffusion-based image generation shares\\ncommonalities with the stochastic gradient descent (SGD)\\nlearning process in the following aspects (illustrated in\\n1arXiv:2402.13144v1  [cs.LG]  20 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2402.13144v1.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"Diffusion\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SME_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
