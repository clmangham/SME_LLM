{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL you want to scrape\n",
    "url = \"https://paperswithcode.com/\"\n",
    "\n",
    "# Make an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Now you can use Beautiful Soup methods to find data in the soup object\n",
    "    # For example, to find all 'a' tags (hyperlinks) in the document:\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    pattern = r\"\\/paper\\/(?!.*(#code|#tasks)$).*$\"\n",
    "    unique_papers = []\n",
    "\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "        # Check if the href attribute is a string and if it matches the pattern\n",
    "        if isinstance(href, str) and re.search(pattern, href):\n",
    "            # If the link is not already in the list, add it\n",
    "            if href not in unique_papers:\n",
    "                unique_papers.append(href)\n",
    "\n",
    "    # Now, unique_papers contains all unique links that include \"/paper/\"\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage: HTTP {response.status_code}\")\n",
    "\n",
    "\n",
    "paper_urls = []\n",
    "for paper in unique_papers:\n",
    "    paper_urls.append(\"https://paperswithcode.com\" + paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using',\n",
       " 'https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers',\n",
       " 'https://paperswithcode.com/paper/neural-network-diffusion',\n",
       " 'https://paperswithcode.com/paper/intent-based-prompt-calibration-enhancing',\n",
       " 'https://paperswithcode.com/paper/gaussianobject-just-taking-four-images-to-get',\n",
       " 'https://paperswithcode.com/paper/ufo-a-ui-focused-agent-for-windows-os',\n",
       " 'https://paperswithcode.com/paper/cleaner-pretraining-corpus-curation-with',\n",
       " 'https://paperswithcode.com/paper/fit-flexible-vision-transformer-for-diffusion',\n",
       " 'https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object',\n",
       " 'https://paperswithcode.com/paper/revisiting-feature-prediction-for-learning']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get each paper from arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_link(url):\n",
    "    # URL you want to scrape\n",
    "\n",
    "    # Make an HTTP GET request to the URL\n",
    "\n",
    "    pdf_link = None\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using Beautiful Soup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Define a function that checks if an 'a' tag's 'href' attribute contains the arXiv PDF URL\n",
    "        def is_arxiv_pdf(tag):\n",
    "            return tag.name == 'a' and tag.get('href') and 'https://arxiv.org/pdf' in tag.get('href')\n",
    "\n",
    "        # Use the function to filter 'a' tags\n",
    "        links = soup.find_all(is_arxiv_pdf)\n",
    "\n",
    "        # Print the 'href' attribute of each link\n",
    "        for link in links:\n",
    "            pdf_link = link.get('href')\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage: HTTP {response.status_code}, {url}\")\n",
    "        \n",
    "\n",
    "\n",
    "    return pdf_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/pdf/2402.13616v1.pdf'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using\"\n",
    "get_paper_link(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = WebBaseLoader(link.get('href'))\n",
    "# data = loader.load()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "# loader = UnstructuredPDFLoader(link.get('href'))\n",
    "# data = loader.load()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL you want to scrape\n",
    "url = \"https://paperswithcode.com/\"\n",
    "\n",
    "# Make an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Now you can use Beautiful Soup methods to find data in the soup object\n",
    "    # For example, to find all 'a' tags (hyperlinks) in the document:\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    pattern = r\"\\/paper\\/(?!.*(#code|#tasks)$).*$\"\n",
    "    unique_papers = []\n",
    "\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "        # Check if the href attribute is a string and if it matches the pattern\n",
    "        if isinstance(href, str) and re.search(pattern, href):\n",
    "            # If the link is not already in the list, add it\n",
    "            if href not in unique_papers:\n",
    "                unique_papers.append(href)\n",
    "\n",
    "    # Now, unique_papers contains all unique links that include \"/paper/\"\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage: HTTP {response.status_code}\")\n",
    "\n",
    "\n",
    "paper_urls = []\n",
    "for paper in unique_papers:\n",
    "    paper_urls.append(\"https://paperswithcode.com\" + paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using',\n",
       " 'https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers',\n",
       " 'https://paperswithcode.com/paper/neural-network-diffusion',\n",
       " 'https://paperswithcode.com/paper/intent-based-prompt-calibration-enhancing',\n",
       " 'https://paperswithcode.com/paper/gaussianobject-just-taking-four-images-to-get',\n",
       " 'https://paperswithcode.com/paper/ufo-a-ui-focused-agent-for-windows-os',\n",
       " 'https://paperswithcode.com/paper/cleaner-pretraining-corpus-curation-with',\n",
       " 'https://paperswithcode.com/paper/fit-flexible-vision-transformer-for-diffusion',\n",
       " 'https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object',\n",
       " 'https://paperswithcode.com/paper/revisiting-feature-prediction-for-learning']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/pdf/2402.13616v1.pdf',\n",
       " 'https://arxiv.org/pdf/2212.09748v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.13144v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.03099v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.10259v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.07939v3.pdf',\n",
       " 'https://arxiv.org/pdf/2402.14652v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.12376v1.pdf',\n",
       " 'https://arxiv.org/pdf/2401.17270v3.pdf']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/2402.13616v1.pdf\n",
      "https://arxiv.org/pdf/2212.09748v2.pdf\n",
      "https://arxiv.org/pdf/2402.13144v1.pdf\n",
      "https://arxiv.org/pdf/2402.03099v1.pdf\n",
      "https://arxiv.org/pdf/2402.10259v2.pdf\n",
      "https://arxiv.org/pdf/2402.07939v3.pdf\n",
      "https://arxiv.org/pdf/2402.14652v1.pdf\n",
      "https://arxiv.org/pdf/2402.12376v1.pdf\n",
      "https://arxiv.org/pdf/2401.17270v3.pdf\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/pdf/2402.13616v1.pdf',\n",
       " 'https://arxiv.org/pdf/2212.09748v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.13144v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.03099v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.10259v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.07939v3.pdf',\n",
       " 'https://arxiv.org/pdf/2402.14652v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.12376v1.pdf',\n",
       " 'https://arxiv.org/pdf/2401.17270v3.pdf',\n",
       " None]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = []\n",
    "for url in paper_urls:\n",
    "    # print(get_paper_link(url))\n",
    "    links.append(get_paper_link(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/pdf/2402.13616v1.pdf',\n",
       " 'https://arxiv.org/pdf/2212.09748v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.13144v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.03099v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.10259v2.pdf',\n",
       " 'https://arxiv.org/pdf/2402.07939v3.pdf',\n",
       " 'https://arxiv.org/pdf/2402.14652v1.pdf',\n",
       " 'https://arxiv.org/pdf/2402.12376v1.pdf',\n",
       " 'https://arxiv.org/pdf/2401.17270v3.pdf',\n",
       " None]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/pdf/2402.13616v1.pdf'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(links[0])\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='YOLOv9: Learning What You Want to Learn\\nUsing Programmable Gradient Information\\nChien-Yao Wang1,2, I-Hau Yeh2, and Hong-Yuan Mark Liao1,2,3\\n1Institute of Information Science, Academia Sinica, Taiwan\\n2National Taipei University of Technology, Taiwan\\n3Department of Information and Computer Engineering, Chung Yuan Christian University, Taiwan\\nkinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, and liao@iis.sinica.edu.tw\\nAbstract\\nToday’s deep learning methods focus on how to design\\nthe most appropriate objective functions so that the pre-\\ndiction results of the model can be closest to the ground\\ntruth. Meanwhile, an appropriate architecture that can\\nfacilitate acquisition of enough information for prediction\\nhas to be designed. Existing methods ignore a fact that\\nwhen input data undergoes layer-by-layer feature extrac-\\ntion and spatial transformation, large amount of informa-\\ntion will be lost. This paper will delve into the important is-\\nsues of data loss when data is transmitted through deep net-\\nworks, namely information bottleneck and reversible func-\\ntions. We proposed the concept of programmable gradi-\\nent information (PGI) to cope with the various changes\\nrequired by deep networks to achieve multiple objectives.\\nPGI can provide complete input information for the tar-\\nget task to calculate objective function, so that reliable\\ngradient information can be obtained to update network\\nweights. In addition, a new lightweight network architec-\\nture – Generalized Efficient Layer Aggregation Network\\n(GELAN), based on gradient path planning is designed.\\nGELAN’s architecture confirms that PGI has gained su-\\nperior results on lightweight models. We verified the pro-\\nposed GELAN and PGI on MS COCO dataset based ob-\\nject detection. The results show that GELAN only uses\\nconventional convolution operators to achieve better pa-\\nrameter utilization than the state-of-the-art methods devel-\\noped based on depth-wise convolution. PGI can be used\\nfor variety of models from lightweight to large. It can be\\nused to obtain complete information, so that train-from-\\nscratch models can achieve better results than state-of-the-\\nart models pre-trained using large datasets, the compari-\\nson results are shown in Figure 1. The source codes are at:\\nhttps://github.com/WongKinYiu/yolov9 .\\n1. Introduction\\nDeep learning-based models have demonstrated far bet-\\nter performance than past artificial intelligence systems in\\nvarious fields, such as computer vision, language process-\\ning, and speech recognition. In recent years, researchers\\nFigure 1. Comparisons of the real-time object detecors on MS\\nCOCO dataset. The GELAN and PGI-based object detection\\nmethod surpassed all previous train-from-scratch methods in terms\\nof object detection performance. In terms of accuracy, the new\\nmethod outperforms RT DETR [43] pre-trained with a large\\ndataset, and it also outperforms depth-wise convolution-based de-\\nsign YOLO MS [7] in terms of parameters utilization.\\nin the field of deep learning have mainly focused on how\\nto develop more powerful system architectures and learn-\\ning methods, such as CNNs [21–23, 42, 55, 71, 72], Trans-\\nformers [8, 9, 40, 41, 60, 69, 70], Perceivers [26, 26, 32, 52,\\n56, 81, 81], and Mambas [17, 38, 80]. In addition, some\\nresearchers have tried to develop more general objective\\nfunctions, such as loss function [5, 45, 46, 50, 77, 78], la-\\nbel assignment [10, 12, 33, 67, 79] and auxiliary supervi-\\nsion [18, 20, 24, 28, 29, 51, 54, 68, 76]. The above studies\\nall try to precisely find the mapping between input and tar-\\nget tasks. However, most past approaches have ignored that\\ninput data may have a non-negligible amount of informa-\\ntion loss during the feedforward process. This loss of in-\\nformation can lead to biased gradient flows, which are sub-\\nsequently used to update the model. The above problems\\ncan result in deep networks to establish incorrect associa-\\ntions between targets and inputs, causing the trained model\\nto produce incorrect predictions.', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 0}),\n",
       " Document(page_content='can result in deep networks to establish incorrect associa-\\ntions between targets and inputs, causing the trained model\\nto produce incorrect predictions.\\n1arXiv:2402.13616v1  [cs.CV]  21 Feb 2024', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 0}),\n",
       " Document(page_content='Figure 2. Visualization results of random initial weight output feature maps for different network architectures: (a) input image, (b)\\nPlainNet, (c) ResNet, (d) CSPNet, and (e) proposed GELAN. From the figure, we can see that in different architectures, the information\\nprovided to the objective function to calculate the loss is lost to varying degrees, and our architecture can retain the most complete\\ninformation and provide the most reliable gradient information for calculating the objective function.\\nIn deep networks, the phenomenon of input data losing\\ninformation during the feedforward process is commonly\\nknown as information bottleneck [59], and its schematic di-\\nagram is as shown in Figure 2. At present, the main meth-\\nods that can alleviate this phenomenon are as follows: (1)\\nThe use of reversible architectures [3, 16, 19]: this method\\nmainly uses repeated input data and maintains the informa-\\ntion of the input data in an explicit way; (2) The use of\\nmasked modeling [1, 6, 9, 27, 71, 73]: it mainly uses recon-\\nstruction loss and adopts an implicit way to maximize the\\nextracted features and retain the input information; and (3)\\nIntroduction of the deep supervision concept [28,51,54,68]:\\nit uses shallow features that have not lost too much impor-\\ntant information to pre-establish a mapping from features\\nto targets to ensure that important information can be trans-\\nferred to deeper layers. However, the above methods have\\ndifferent drawbacks in the training process and inference\\nprocess. For example, a reversible architecture requires ad-\\nditional layers to combine repeatedly fed input data, which\\nwill significantly increase the inference cost. In addition,\\nsince the input data layer to the output layer cannot have a\\ntoo deep path, this limitation will make it difficult to model\\nhigh-order semantic information during the training pro-\\ncess. As for masked modeling, its reconstruction loss some-\\ntimes conflicts with the target loss. In addition, most mask\\nmechanisms also produce incorrect associations with data.\\nFor the deep supervision mechanism, it will produce error\\naccumulation, and if the shallow supervision loses informa-\\ntion during the training process, the subsequent layers will\\nnot be able to retrieve the required information. The above\\nphenomenon will be more significant on difficult tasks and\\nsmall models.\\nTo address the above-mentioned issues, we propose a\\nnew concept, which is programmable gradient information\\n(PGI). The concept is to generate reliable gradients through\\nauxiliary reversible branch, so that the deep features can\\nstill maintain key characteristics for executing target task.\\nThe design of auxiliary reversible branch can avoid the se-\\nmantic loss that may be caused by a traditional deep super-\\nvision process that integrates multi-path features. In other\\nwords, we are programming gradient information propaga-\\ntion at different semantic levels, and thereby achieving the\\nbest training results. The reversible architecture of PGI isbuilt on auxiliary branch, so there is no additional cost.\\nSince PGI can freely select loss function suitable for the\\ntarget task, it also overcomes the problems encountered by\\nmask modeling. The proposed PGI mechanism can be ap-\\nplied to deep neural networks of various sizes and is more\\ngeneral than the deep supervision mechanism, which is only\\nsuitable for very deep neural networks.\\nIn this paper, we also designed generalized ELAN\\n(GELAN) based on ELAN [65], the design of GELAN si-\\nmultaneously takes into account the number of parameters,\\ncomputational complexity, accuracy and inference speed.\\nThis design allows users to arbitrarily choose appropriate\\ncomputational blocks for different inference devices. We\\ncombined the proposed PGI and GELAN, and then de-\\nsigned a new generation of YOLO series object detection\\nsystem, which we call YOLOv9. We used the MS COCO\\ndataset to conduct experiments, and the experimental results', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 1}),\n",
       " Document(page_content='signed a new generation of YOLO series object detection\\nsystem, which we call YOLOv9. We used the MS COCO\\ndataset to conduct experiments, and the experimental results\\nverified that our proposed YOLOv9 achieved the top perfor-\\nmance in all comparisons.\\nWe summarize the contributions of this paper as follows:\\n1. We theoretically analyzed the existing deep neural net-\\nwork architecture from the perspective of reversible\\nfunction, and through this process we successfully ex-\\nplained many phenomena that were difficult to explain\\nin the past. We also designed PGI and auxiliary re-\\nversible branch based on this analysis and achieved ex-\\ncellent results.\\n2. The PGI we designed solves the problem that deep su-\\npervision can only be used for extremely deep neu-\\nral network architectures, and therefore allows new\\nlightweight architectures to be truly applied in daily\\nlife.\\n3. The GELAN we designed only uses conventional con-\\nvolution to achieve a higher parameter usage than the\\ndepth-wise convolution design that based on the most\\nadvanced technology, while showing great advantages\\nof being light, fast, and accurate.\\n4. Combining the proposed PGI and GELAN, the object\\ndetection performance of the YOLOv9 on MS COCO\\ndataset greatly surpasses the existing real-time object\\ndetectors in all aspects.\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 1}),\n",
       " Document(page_content='2. Related work\\n2.1. Real-time Object Detectors\\nThe current mainstream real-time object detectors are the\\nYOLO series [2, 7, 13–15, 25, 30, 31, 47–49, 61–63, 74, 75],\\nand most of these models use CSPNet [64] or ELAN [65]\\nand their variants as the main computing units. In terms of\\nfeature integration, improved PAN [37] or FPN [35] is of-\\nten used as a tool, and then improved YOLOv3 head [49] or\\nFCOS head [57, 58] is used as prediction head. Recently\\nsome real-time object detectors, such as RT DETR [43],\\nwhich puts its fundation on DETR [4], have also been pro-\\nposed. However, since it is extremely difficult for DETR\\nseries object detector to be applied to new domains without\\na corresponding domain pre-trained model, the most widely\\nused real-time object detector at present is still YOLO se-\\nries. This paper chooses YOLOv7 [63], which has been\\nproven effective in a variety of computer vision tasks and\\nvarious scenarios, as a base to develop the proposed method.\\nWe use GELAN to improve the architecture and the training\\nprocess with the proposed PGI. The above novel approach\\nmakes the proposed YOLOv9 the top real-time object de-\\ntector of the new generation.\\n2.2. Reversible Architectures\\nThe operation unit of reversible architectures [3, 16, 19]\\nmust maintain the characteristics of reversible conversion,\\nso it can be ensured that the output feature map of each\\nlayer of operation unit can retain complete original informa-\\ntion. Before, RevCol [3] generalizes traditional reversible\\nunit to multiple levels, and in doing so can expand the se-\\nmantic levels expressed by different layer units. Through\\na literature review of various neural network architectures,\\nwe found that there are many high-performing architectures\\nwith varying degree of reversible properties. For exam-\\nple, Res2Net module [11] combines different input parti-\\ntions with the next partition in a hierarchical manner, and\\nconcatenates all converted partitions before passing them\\nbackwards. CBNet [34, 39] re-introduces the original in-\\nput data through composite backbone to obtain complete\\noriginal information, and obtains different levels of multi-\\nlevel reversible information through various composition\\nmethods. These network architectures generally have ex-\\ncellent parameter utilization, but the extra composite layers\\ncause slow inference speeds. DynamicDet [36] combines\\nCBNet [34] and the high-efficiency real-time object detec-\\ntor YOLOv7 [63] to achieve a very good trade-off among\\nspeed, number of parameters, and accuracy. This paper in-\\ntroduces the DynamicDet architecture as the basis for de-\\nsigning reversible branches. In addition, reversible infor-\\nmation is further introduced into the proposed PGI. The\\nproposed new architecture does not require additional con-\\nnections during the inference process, so it can fully retain\\nthe advantages of speed, parameter amount, and accuracy.2.3. Auxiliary Supervision\\nDeep supervision [28,54,68] is the most common auxil-\\niary supervision method, which performs training by insert-\\ning additional prediction layers in the middle layers. Es-\\npecially the application of multi-layer decoders introduced\\nin the transformer-based methods is the most common one.\\nAnother common auxiliary supervision method is to utilize\\nthe relevant meta information to guide the feature maps pro-\\nduced by the intermediate layers and make them have the\\nproperties required by the target tasks [18, 20, 24, 29, 76].\\nExamples of this type include using segmentation loss or\\ndepth loss to enhance the accuracy of object detectors. Re-\\ncently, there are many reports in the literature [53, 67, 82]\\nthat use different label assignment methods to generate dif-\\nferent auxiliary supervision mechanisms to speed up the\\nconvergence speed of the model and improve the robustness\\nat the same time. However, the auxiliary supervision mech-\\nanism is usually only applicable to large models, so when\\nit is applied to lightweight models, it is easy to cause an', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 2}),\n",
       " Document(page_content='at the same time. However, the auxiliary supervision mech-\\nanism is usually only applicable to large models, so when\\nit is applied to lightweight models, it is easy to cause an\\nunder parameterization phenomenon, which makes the per-\\nformance worse. The PGI we proposed designed a way to\\nreprogram multi-level semantic information, and this design\\nallows lightweight models to also benefit from the auxiliary\\nsupervision mechanism.\\n3. Problem Statement\\nUsually, people attribute the difficulty of deep neural net-\\nwork convergence problem due to factors such as gradient\\nvanish or gradient saturation, and these phenomena do ex-\\nist in traditional deep neural networks. However, modern\\ndeep neural networks have already fundamentally solved\\nthe above problem by designing various normalization and\\nactivation functions. Nevertheless, deep neural networks\\nstill have the problem of slow convergence or poor conver-\\ngence results.\\nIn this paper, we explore the nature of the above issue\\nfurther. Through in-depth analysis of information bottle-\\nneck, we deduced that the root cause of this problem is that\\nthe initial gradient originally coming from a very deep net-\\nwork has lost a lot of information needed to achieve the\\ngoal soon after it is transmitted. In order to confirm this\\ninference, we feedforward deep networks of different archi-\\ntectures with initial weights, and then visualize and illus-\\ntrate them in Figure 2. Obviously, PlainNet has lost a lot of\\nimportant information required for object detection in deep\\nlayers. As for the proportion of important information that\\nResNet, CSPNet, and GELAN can retain, it is indeed posi-\\ntively related to the accuracy that can be obtained after train-\\ning. We further design reversible network-based methods to\\nsolve the causes of the above problems. In this section we\\nshall elaborate our analysis of information bottleneck prin-\\nciple and reversible functions.\\n3', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 2}),\n",
       " Document(page_content='3.1. Information Bottleneck Principle\\nAccording to information bottleneck principle, we know\\nthat data Xmay cause information loss when going through\\ntransformation, as shown in Eq. 1 below:\\nI(X, X )≥I(X, f θ(X))≥I(X, gϕ(fθ(X))),(1)\\nwhere Iindicates mutual information, fandgare transfor-\\nmation functions, and θandϕare parameters of fandg,\\nrespectively.\\nIn deep neural networks, fθ(·)andgϕ(·)respectively\\nrepresent the operations of two consecutive layers in deep\\nneural network. From Eq. 1, we can predict that as the num-\\nber of network layer becomes deeper, the original data will\\nbe more likely to be lost. However, the parameters of the\\ndeep neural network are based on the output of the network\\nas well as the given target, and then update the network after\\ngenerating new gradients by calculating the loss function.\\nAs one can imagine, the output of a deeper neural network\\nis less able to retain complete information about the pre-\\ndiction target. This will make it possible to use incomplete\\ninformation during network training, resulting in unreliable\\ngradients and poor convergence.\\nOne way to solve the above problem is to directly in-\\ncrease the size of the model. When we use a large number\\nof parameters to construct a model, it is more capable of\\nperforming a more complete transformation of the data. The\\nabove approach allows even if information is lost during the\\ndata feedforward process, there is still a chance to retain\\nenough information to perform the mapping to the target.\\nThe above phenomenon explains why the width is more im-\\nportant than the depth in most modern models. However,\\nthe above conclusion cannot fundamentally solve the prob-\\nlem of unreliable gradients in very deep neural network.\\nBelow, we will introduce how to use reversible functions\\nto solve problems and conduct relative analysis.\\n3.2. Reversible Functions\\nWhen a function rhas an inverse transformation func-\\ntionv, we call this function reversible function, as shown in\\nEq. 2.\\nX=vζ(rψ(X)), (2)\\nwhere ψandζare parameters of randv, respectively. Data\\nXis converted by reversible function without losing infor-\\nmation, as shown in Eq. 3.\\nI(X, X ) =I(X, rψ(X)) =I(X, v ζ(rψ(X))).(3)\\nWhen the network’s transformation function is composed\\nof reversible functions, more reliable gradients can be ob-\\ntained to update the model. Almost all of today’s populardeep learning methods are architectures that conform to the\\nreversible property, such as Eq. 4.\\nXl+1=Xl+fl+1\\nθ(Xl), (4)\\nwhere lindicates the l-th layer of a PreAct ResNet and\\nfis the transformation function of the l-th layer. PreAct\\nResNet [22] repeatedly passes the original data Xto sub-\\nsequent layers in an explicit way. Although such a design\\ncan make a deep neural network with more than a thousand\\nlayers converge very well, it destroys an important reason\\nwhy we need deep neural networks. That is, for difficult\\nproblems, it is difficult for us to directly find simple map-\\nping functions to map data to targets. This also explains\\nwhy PreAct ResNet performs worse than ResNet [21] when\\nthe number of layers is small.\\nIn addition, we tried to use masked modeling that al-\\nlowed the transformer model to achieve significant break-\\nthroughs. We use approximation methods, such as Eq. 5,\\nto try to find the inverse transformation vofr, so that the\\ntransformed features can retain enough information using\\nsparse features. The form of Eq. 5 is as follows:\\nX=vζ(rψ(X)·M), (5)\\nwhere Mis a dynamic binary mask. Other methods that\\nare commonly used to perform the above tasks are diffusion\\nmodel and variational autoencoder, and they both have the\\nfunction of finding the inverse function. However, when\\nwe apply the above approach to a lightweight model, there\\nwill be defects because the lightweight model will be under\\nparameterized to a large amount of raw data. Because of\\nthe above reason, important information I(Y, X)that maps\\ndataXto target Ywill also face the same problem. For this', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 3}),\n",
       " Document(page_content='parameterized to a large amount of raw data. Because of\\nthe above reason, important information I(Y, X)that maps\\ndataXto target Ywill also face the same problem. For this\\nissue, we will explore it using the concept of information\\nbottleneck [59]. The formula for information bottleneck is\\nas follows:\\nI(X, X )≥I(Y, X)≥I(Y, fθ(X))≥...≥I(Y,ˆY).(6)\\nGenerally speaking, I(Y, X)will only occupy a very small\\npart of I(X, X ). However, it is critical to the target mis-\\nsion. Therefore, even if the amount of information lost in\\nthe feedforward stage is not significant, as long as I(Y, X)\\nis covered, the training effect will be greatly affected. The\\nlightweight model itself is in an under parameterized state,\\nso it is easy to lose a lot of important information in the\\nfeedforward stage. Therefore, our goal for the lightweight\\nmodel is how to accurately filter I(Y, X)fromI(X, X ). As\\nfor fully preserving the information of X, that is difficult to\\nachieve. Based on the above analysis, we hope to propose a\\nnew deep neural network training method that can not only\\ngenerate reliable gradients to update the model, but also be\\nsuitable for shallow and lightweight neural networks.\\n4', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 3. PGI and related network architectures and methods. (a) Path Aggregation Network (PAN)) [37], (b) Reversible Columns\\n(RevCol) [3], (c) conventional deep supervision, and (d) our proposed Programmable Gradient Information (PGI). PGI is mainly composed\\nof three components: (1) main branch: architecture used for inference, (2) auxiliary reversible branch: generate reliable gradients to supply\\nmain branch for backward transmission, and (3) multi-level auxiliary information: control main branch learning plannable multi-level of\\nsemantic information.\\n4. Methodology\\n4.1. Programmable Gradient Information\\nIn order to solve the aforementioned problems, we pro-\\npose a new auxiliary supervision framework called Pro-\\ngrammable Gradient Information (PGI), as shown in Fig-\\nure 3 (d). PGI mainly includes three components, namely\\n(1) main branch, (2) auxiliary reversible branch, and (3)\\nmulti-level auxiliary information. From Figure 3 (d) we\\nsee that the inference process of PGI only uses main branch\\nand therefore does not require any additional inference cost.\\nAs for the other two components, they are used to solve or\\nslow down several important issues in deep learning meth-\\nods. Among them, auxiliary reversible branch is designed\\nto deal with the problems caused by the deepening of neural\\nnetworks. Network deepening will cause information bot-\\ntleneck, which will make the loss function unable to gener-\\nate reliable gradients. As for multi-level auxiliary informa-\\ntion, it is designed to handle the error accumulation problem\\ncaused by deep supervision, especially for the architecture\\nand lightweight model of multiple prediction branch. Next,\\nwe will introduce these two components step by step.\\n4.1.1 Auxiliary Reversible Branch\\nIn PGI, we propose auxiliary reversible branch to gener-\\nate reliable gradients and update network parameters. By\\nproviding information that maps from data to targets, the\\nloss function can provide guidance and avoid the possibil-\\nity of finding false correlations from incomplete feedfor-\\nward features that are less relevant to the target. We pro-pose the maintenance of complete information by introduc-\\ning reversible architecture, but adding main branch to re-\\nversible architecture will consume a lot of inference costs.\\nWe analyzed the architecture of Figure 3 (b) and found that\\nwhen additional connections from deep to shallow layers\\nare added, the inference time will increase by 20%. When\\nwe repeatedly add the input data to the high-resolution com-\\nputing layer of the network (yellow box), the inference time\\neven exceeds twice the time.\\nSince our goal is to use reversible architecture to ob-\\ntain reliable gradients, “reversible” is not the only neces-\\nsary condition in the inference stage. In view of this, we\\nregard reversible branch as an expansion of deep supervi-\\nsion branch, and then design auxiliary reversible branch, as\\nshown in Figure 3 (d). As for the main branch deep fea-\\ntures that would have lost important information due to in-\\nformation bottleneck, they will be able to receive reliable\\ngradient information from the auxiliary reversible branch.\\nThese gradient information will drive parameter learning to\\nassist in extracting correct and important information, and\\nthe above actions can enable the main branch to obtain fea-\\ntures that are more effective for the target task. Moreover,\\nthe reversible architecture performs worse on shallow net-\\nworks than on general networks because complex tasks re-\\nquire conversion in deeper networks. Our proposed method\\ndoes not force the main branch to retain complete origi-\\nnal information but updates it by generating useful gradient\\nthrough the auxiliary supervision mechanism. The advan-\\ntage of this design is that the proposed method can also be\\napplied to shallower networks.\\n5', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 4}),\n",
       " Document(page_content='Figure 4. The architecture of GELAN: (a) CSPNet [64], (b) ELAN [65], and (c) proposed GELAN. We imitate CSPNet and extend ELAN\\ninto GELAN that can support any computational blocks.\\nFinally, since auxiliary reversible branch can be removed\\nduring the inference phase, the inference capabilities of the\\noriginal network can be retained. We can also choose any\\nreversible architectures in PGI to play the role of auxiliary\\nreversible branch.\\n4.1.2 Multi-level Auxiliary Information\\nIn this section we will discuss how multi-level auxiliary in-\\nformation works. The deep supervision architecture includ-\\ning multiple prediction branch is shown in Figure 3 (c). For\\nobject detection, different feature pyramids can be used to\\nperform different tasks, for example together they can de-\\ntect objects of different sizes. Therefore, after connecting\\nto the deep supervision branch, the shallow features will be\\nguided to learn the features required for small object detec-\\ntion, and at this time the system will regard the positions\\nof objects of other sizes as the background. However, the\\nabove deed will cause the deep feature pyramids to lose a lot\\nof information needed to predict the target object. Regard-\\ning this issue, we believe that each feature pyramid needs\\nto receive information about all target objects so that subse-\\nquent main branch can retain complete information to learn\\npredictions for various targets.\\nThe concept of multi-level auxiliary information is to in-\\nsert an integration network between the feature pyramid hi-\\nerarchy layers of auxiliary supervision and the main branch,\\nand then uses it to combine returned gradients from differ-\\nent prediction heads, as shown in Figure 3 (d). Multi-level\\nauxiliary information is then to aggregate the gradient infor-\\nmation containing all target objects, and pass it to the main\\nbranch and then update parameters. At this time, the charac-\\nteristics of the main branch’s feature pyramid hierarchy will\\nnot be dominated by some specific object’s information. As\\na result, our method can alleviate the broken information\\nproblem in deep supervision. In addition, any integrated\\nnetwork can be used in multi-level auxiliary information.\\nTherefore, we can plan the required semantic levels to guide\\nthe learning of network architectures of different sizes.4.2. Generalized ELAN\\nIn this Section we describe the proposed new network\\narchitecture – GELAN. By combining two neural network\\narchitectures, CSPNet [64] and ELAN [65], which are de-\\nsigned with gradient path planning, we designed gener-\\nalized efficient layer aggregation network (GELAN) that\\ntakes into account lighweight, inference speed, and accu-\\nracy. Its overall architecture is shown in Figure 4. We gen-\\neralized the capability of ELAN [65], which originally only\\nused stacking of convolutional layers, to a new architecture\\nthat can use any computational blocks.\\n5. Experiments\\n5.1. Experimental Setup\\nWe verify the proposed method with MS COCO dataset.\\nAll experimental setups follow YOLOv7 AF [63], while the\\ndataset is MS COCO 2017 splitting. All models we men-\\ntioned are trained using the train-from-scratch strategy, and\\nthe total number of training times is 500 epochs. In setting\\nthe learning rate, we use linear warm-up in the first three\\nepochs, and the subsequent epochs set the corresponding\\ndecay manner according to the model scale. As for the last\\n15 epochs, we turn mosaic data augmentation off. For more\\nsettings, please refer to Appendix.\\n5.2. Implimentation Details\\nWe built general and extended version of YOLOv9 based\\non YOLOv7 [63] and Dynamic YOLOv7 [36] respectively.\\nIn the design of the network architecture, we replaced\\nELAN [65] with GELAN using CSPNet blocks [64] with\\nplanned RepConv [63] as computational blocks. We also\\nsimplified downsampling module and optimized anchor-\\nfree prediction head. As for the auxiliary loss part of PGI,\\nwe completely follow YOLOv7’s auxiliary head setting.\\nPlease see Appendix for more details.\\n6', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 5}),\n",
       " Document(page_content='Table 1. Comparison of state-of-the-art real-time object detectors.\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv5-N r7.0 [14] 1.9 4.5 28.0 45.7 – – – –\\nYOLOv5-S r7.0 [14] 7.2 16.5 37.4 56.8 – – – –\\nYOLOv5-M r7.0 [14] 21.2 49.0 45.4 64.1 – – – –\\nYOLOv5-L r7.0 [14] 46.5 109.1 49.0 67.3 – – – –\\nYOLOv5-X r7.0 [14] 86.7 205.7 50.7 68.9 – – – –\\nYOLOv6-N v3.0 [30] 4.7 11.4 37.0 52.7 – – – –\\nYOLOv6-S v3.0 [30] 18.5 45.3 44.3 61.2 – – – –\\nYOLOv6-M v3.0 [30] 34.9 85.8 49.1 66.1 – – – –\\nYOLOv6-L v3.0 [30] 59.6 150.7 51.8 69.2 – – – –\\nYOLOv7 [63] 36.9 104.7 51.2 69.7 55.9 31.8 55.5 65.0\\nYOLOv7-X [63] 71.3 189.9 52.9 71.1 51.4 36.9 57.7 68.6\\nYOLOv7-N AF [63] 3.1 8.7 37.6 53.3 40.6 18.7 41.7 52.8\\nYOLOv7-S AF [63] 11.0 28.1 45.1 61.8 48.9 25.7 50.2 61.2\\nYOLOv7 AF [63] 43.6 130.5 53.0 70.2 57.5 35.8 58.7 68.9\\nYOLOv8-N [15] 3.2 8.7 37.3 52.6 – – – –\\nYOLOv8-S [15] 11.2 28.6 44.9 61.8 – – – –\\nYOLOv8-M [15] 25.9 78.9 50.2 67.2 – – – –\\nYOLOv8-L [15] 43.7 165.2 52.9 69.8 57.5 35.3 58.3 69.8\\nYOLOv8-X [15] 68.2 257.8 53.9 71.0 58.7 35.7 59.3 70.7\\nDAMO YOLO-T [75] 8.5 18.1 42.0 58.0 45.2 23.0 46.1 58.5\\nDAMO YOLO-S [75] 12.3 37.8 46.0 61.9 49.5 25.9 50.6 62.5\\nDAMO YOLO-M [75] 28.2 61.8 49.2 65.5 53.0 29.7 53.1 66.1\\nDAMO YOLO-L [75] 42.1 97.3 50.8 67.5 55.5 33.2 55.7 66.6\\nGold YOLO-N [61] 5.6 12.1 39.6 55.7 – 19.7 44.1 57.0\\nGold YOLO-S [61] 21.5 46.0 45.4 62.5 – 25.3 50.2 62.6\\nGold YOLO-M [61] 41.3 87.5 49.8 67.0 – 32.3 55.3 66.3\\nGold YOLO-L [61] 75.1 151.7 51.8 68.9 – 34.1 57.4 68.2\\nYOLO MS-N [7] 4.5 17.4 43.4 60.4 47.6 23.7 48.3 60.3\\nYOLO MS-S [7] 8.1 31.2 46.2 63.7 50.5 26.9 50.5 63.0\\nYOLO MS [7] 22.2 80.2 51.0 68.6 55.7 33.1 56.1 66.5\\nGELAN-S (Ours) 7.2 26.7 46.7 63.0 50.7 25.9 51.5 64.0\\nGELAN-M (Ours) 20.1 76.8 51.1 67.9 55.7 33.6 56.4 67.3\\nGELAN-C (Ours) 25.5 102.8 52.5 69.5 57.3 35.8 57.6 69.4\\nGELAN-E (Ours) 58.1 192.5 55.0 71.9 60.0 38.0 60.6 70.9\\nYOLOv9-S (Ours) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nYOLOv9-M (Ours) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nYOLOv9-C (Ours) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nYOLOv9-E (Ours) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4\\n5.3. Comparison with state-of-the-arts\\nTable 1 lists comparison of our proposed YOLOv9 with\\nother train-from-scratch real-time object detectors. Over-\\nall, the best performing methods among existing methods\\nare YOLO MS-S [7] for lightweight models, YOLO MS [7]\\nfor medium models, YOLOv7 AF [63] for general mod-\\nels, and YOLOv8-X [15] for large models. Compared with\\nlightweight and medium model YOLO MS [7], YOLOv9\\nhas about 10% less parameters and 5 ∼15% less calcula-\\ntions, but still has a 0.4 ∼0.6% improvement in AP. Com-\\npared with YOLOv7 AF, YOLOv9-C has 42% less pa-\\nrameters and 21% less calculations, but achieves the same\\nAP (53%). Compared with YOLOv8-X, YOLOv9-X has\\n15% less parameters, 25% less calculations, and has sig-\\nnificant improvement of 1.7% AP. The above comparison\\nresults show that our proposed YOLOv9 has significantlyimproved in all aspects compared with existing methods.\\nOn the other hand, we also include ImageNet pretrained\\nmodel in the comparison, and the results are shown in Fig-\\nure 5. We compare them based on the parameters and the\\namount of computation respectively. In terms of the num-\\nber of parameters, the best performing large model is RT\\nDETR [43]. From Figure 5, we can see that YOLOv9 using\\nconventional convolution is even better than YOLO MS us-\\ning depth-wise convolution in parameter utilization. As for\\nthe parameter utilization of large models, it also greatly sur-\\npasses RT DETR using ImageNet pretrained model. Even\\nbetter is that in the deep model, YOLOv9 shows the huge\\nadvantages of using PGI. By accurately retaining and ex-\\ntracting the information needed to map the data to the tar-\\nget, our method requires only 64% of the parameters while\\nmaintaining the accuracy as RT DETR-X.\\n7', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 6}),\n",
       " Document(page_content='Figure 5. Comparison of state-of-the-art real-time object detectors. The methods participating in the comparison all use ImageNet as\\npre-trained weights, including RT DETR [43], RTMDet [44], and PP-YOLOE [74], etc. The YOLOv9 that uses train-from-scratch method\\nclearly surpasses the performance of other methods.\\nAs for the amount of computation, the best existing mod-\\nels from the smallest to the largest are YOLO MS [7], PP\\nYOLOE [74], and RT DETR [43]. From Figure 5, we can\\nsee that YOLOv9 is far superior to the train-from-scratch\\nmethods in terms of computational complexity. In addi-\\ntion, if compared with those based on depth-wise convo-\\nlution and ImageNet-based pretrained models, YOLOv9 is\\nalso very competitive.\\n5.4. Ablation Studies\\n5.4.1 Generalized ELAN\\nFor GELAN, we first do ablation studies for computational\\nblocks. We used Res blocks [21], Dark blocks [49], and\\nCSP blocks [64] to conduct experiments, respectively. Ta-\\nble 2 shows that after replacing convolutional layers in\\nELAN with different computational blocks, the system can\\nmaintain good performance. Users are indeed free to re-\\nplace computational blocks and use them on their respective\\ninference devices. Among different computational block re-\\nplacements, CSP blocks perform particularly well. They\\nnot only reduce the amount of parameters and computation,\\nbut also improve AP by 0.7%. Therefore, we choose CSP-\\nELAN as the component unit of GELAN in YOLOv9.\\nTable 2. Ablation study on various computational blocks.\\nModel CB type #Param. FLOPs APval\\n50:95\\nGELAN-S Conv 6.3M 24.0G 44.8%\\nGELAN-S Res [21] 5.5M 21.1G 44.3%\\nGELAN-S Dark [49] 5.7M 21.8G 44.5%\\nGELAN-S CSP [64] 5.9M 22.5G 45.5%\\n1CB type nedotes as computational block type.\\n2-S nedotes small size model.Next, we conduct ELAN block-depth and CSP block-\\ndepth experiments on GELAN of different sizes, and dis-\\nplay the results in Table 3. We can see that when the depth\\nof ELAN is increased from 1 to 2, the accuracy is signif-\\nicantly improved. But when the depth is greater than or\\nequal to 2, no matter it is improving the ELAN depth or the\\nCSP depth, the number of parameters, the amount of com-\\nputation, and the accuracy will always show a linear rela-\\ntionship. This means GELAN is not sensitive to the depth.\\nIn other words, users can arbitrarily combine the compo-\\nnents in GELAN to design the network architecture, and\\nhave a model with stable performance without special de-\\nsign. In Table 3, for YOLOv9- {S,M,C}, we set the pairing\\nof the ELAN depth and the CSP depth to {{2, 3},{2, 1},\\n{2, 1}}.\\nTable 3. Ablation study on ELAN and CSP depth.\\nModel D ELAN DCSP #Param. FLOPs APval\\n50:95\\nGELAN-S 2 1 5.9M 22.5G 45.5%\\nGELAN-S 2 2 6.6M 24.6G 46.0%\\nGELAN-S 3 1 7.1M 26.4G 46.5%\\nGELAN-S 2 3 7.2M 26.7G 46.7%\\nGELAN-M 2 1 20.1M 76.8G 51.1%\\nGELAN-M 2 2 22.4M 86.1G 51.7%\\nGELAN-M 3 1 24.5M 94.2G 51.8%\\nGELAN-M 2 3 24.8M 95.5G 52.3%\\nGELAN-C 1 1 19.0M 77.8G 50.7%\\nGELAN-C 2 1 25.5M 102.8G 52.5%\\nGELAN-C 2 2 28.9M 115.8G 53.0%\\nGELAN-C 3 1 32.0M 127.9G 53.2%\\nGELAN-C 2 3 32.4M 128.7G 53.3%\\n1DELAN andDCSP respectively nedotes depth of ELAN and CSP.\\n2-{S, M, C }indicate small, medium, and compact models.\\n8', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 7}),\n",
       " Document(page_content='5.4.2 Programmable Gradient Information\\nIn terms of PGI, we performed ablation studies on auxiliary\\nreversible branch and multi-level auxiliary information on\\nthe backbone and neck, respectively. We designed auxiliary\\nreversible branch ICN to use DHLC [34] linkage to obtain\\nmulti-level reversible information. As for multi-level aux-\\niliary information, we use FPN and PAN for ablation stud-\\nies and the role of PFH is equivalent to the traditional deep\\nsupervision. The results of all experiments are listed in Ta-\\nble 4. From Table 4, we can see that PFH is only effective in\\ndeep models, while our proposed PGI can improve accuracy\\nunder different combinations. Especially when using ICN,\\nwe get stable and better results. We also tried to apply the\\nlead-head guided assignment proposed in YOLOv7 [63] to\\nthe PGI’s auxiliary supervision, and achieved much better\\nperformance.\\nTable 4. Ablation study on PGI of backbone and neck.\\nModel G backbone Gneck APval\\n50:95APval\\nSAPval\\nMAPval\\nL\\nGELAN-C – – 52.5% 35.8% 57.6% 69.4%\\nGELAN-C PFH – 52.5% 35.3% 58.1% 68.9%\\nGELAN-C FPN – 52.6% 35.3% 58.1% 68.9%\\nGELAN-C – ICN 52.7% 35.3% 58.4% 68.9%\\nGELAN-C FPN ICN 52.8% 35.8% 58.2% 69.1%\\nGELAN-C ICN – 52.9% 35.2% 58.7% 68.6%\\nGELAN-C LHG-ICN – 53.0% 36.3% 58.5% 69.1%\\nGELAN-E – – 55.0% 38.0% 60.6% 70.9%\\nGELAN-E PFH – 55.3% 38.3% 60.3% 71.6%\\nGELAN-E FPN – 55.6% 40.2% 61.0% 71.4%\\nGELAN-E PAN – 55.5% 39.0% 61.1% 71.5%\\nGELAN-E FPN ICN 55.6% 39.8% 60.9% 71.9%\\n1DELAN andDCSP respectively nedotes depth of ELAN and CSP.\\n2LHG indicates lead head guided training proposed by YOLOv7 [63].\\nWe further implemented the concepts of PGI and deep\\nsupervision on models of various sizes and compared the\\nresults, these results are shown in Table 5. As analyzed at\\nthe beginning, introduction of deep supervision will cause\\na loss of accuracy for shallow models. As for general mod-\\nels, introducing deep supervision will cause unstable perfor-\\nmance, and the design concept of deep supervision can only\\nbring gains in extremely deep models. The proposed PGI\\ncan effectively handle problems such as information bottle-\\nneck and information broken, and can comprehensively im-\\nprove the accuracy of models of different sizes. The concept\\nof PGI brings two valuable contributions. The first one is to\\nmake the auxiliary supervision method applicable to shal-\\nlow models, while the second one is to make the deep model\\ntraining process obtain more reliable gradients. These gra-\\ndients enable deep models to use more accurate information\\nto establish correct correlations between data and targets.Table 5. Ablation study on PGI.\\nModel APval\\n50:95APval\\n50APval\\n75\\nGELAN-S 46.7% 63.0% 50.7%\\n+ DS 46.5% -0.2 62.9% -0.1 50.5% -0.2\\n+ PGI 46.8% +0.1 63.4% +0.4 50.7% =\\nGELAN-M 51.1% 67.9% 55.7%\\n+ DS 51.2% +0.1 68.2% +0.3 55.7% =\\n+ PGI 51.4% +0.3 68.1% +0.2 56.1% +0.4\\nGELAN-C 52.5% 69.5% 57.3%\\n+ DS 52.5% = 69.9% +0.4 57.1% -0.2\\n+ PGI 53.0% +0.5 70.3% +0.8 57.8% +0.5\\nGELAN-E 55.0% 71.9% 60.0%\\n+ DS 55.3% +0.3 72.3% +0.4 60.2% +0.2\\n+ PGI 55.6% +0.6 72.8% +0.9 60.6% +0.6\\n1DS indicates deep supervision.\\nFinally, we show in the table the results of gradually in-\\ncreasing components from baseline YOLOv7 to YOLOv9-\\nE. The GELAN and PGI we proposed have brought all-\\nround improvement to the model.\\nTable 6. Ablation study on GELAN and PGI.\\nModel #Param. FLOPs APval\\n50:95APval\\nSAPval\\nMAPval\\nL\\nYOLOv7 [63] 36.9 104.7 51.2% 31.8% 55.5% 65.0%\\n+ AF [63] 43.6 130.5 53.0% 35.8% 58.7% 68.9%\\n+ GELAN 41.7 127.9 53.2% 36.2% 58.5% 69.9%\\n+ DHLC [34] 58.1 192.5 55.0% 38.0% 60.6% 70.9%\\n+ PGI 58.1 192.5 55.6% 40.2% 61.0% 71.4%\\n5.5. Visualization\\nThis section will explore the information bottleneck is-\\nsues and visualize them. In addition, we will also visualize\\nhow the proposed PGI uses reliable gradients to find the\\ncorrect correlations between data and targets. In Figure 6\\nwe show the visualization results of feature maps obtained\\nby using random initial weights as feedforward under dif-', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 8}),\n",
       " Document(page_content='correct correlations between data and targets. In Figure 6\\nwe show the visualization results of feature maps obtained\\nby using random initial weights as feedforward under dif-\\nferent architectures. We can see that as the number of lay-\\ners increases, the original information of all architectures\\ngradually decreases. For example, at the 50thlayer of the\\nPlainNet, it is difficult to see the location of objects, and all\\ndistinguishable features will be lost at the 100thlayer. As\\nfor ResNet, although the position of object can still be seen\\nat the 50thlayer, the boundary information has been lost.\\nWhen the depth reached to the 100thlayer, the whole image\\nbecomes blurry. Both CSPNet and the proposed GELAN\\nperform very well, and they both can maintain features that\\nsupport clear identification of objects until the 200thlayer.\\nAmong the comparisons, GELAN has more stable results\\nand clearer boundary information.\\n9', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 8}),\n",
       " Document(page_content='Figure 6. Feature maps (visualization results) output by random initial weights of PlainNet, ResNet, CSPNet, and GELAN at different\\ndepths. After 100 layers, ResNet begins to produce feedforward output that is enough to obfuscate object information. Our proposed\\nGELAN can still retain quite complete information up to the 150thlayer, and is still sufficiently discriminative up to the 200thlayer.\\nFigure 7. PAN feature maps (visualization results) of GELAN\\nand YOLOv9 (GELAN + PGI) after one epoch of bias warm-up.\\nGELAN originally had some divergence, but after adding PGI’s\\nreversible branch, it is more capable of focusing on the target ob-\\nject.\\nFigure 7 is used to show whether PGI can provide more\\nreliable gradients during the training process, so that the\\nparameters used for updating can effectively capture the\\nrelationship between the input data and the target. Fig-\\nure 7 shows the visualization results of the feature map of\\nGELAN and YOLOv9 (GELAN + PGI) in PAN bias warm-\\nup. From the comparison of Figure 7(b) and (c), we can\\nclearly see that PGI accurately and concisely captures the\\narea containing objects. As for GELAN that does not use\\nPGI, we found that it had divergence when detecting ob-ject boundaries, and it also produced unexpected responses\\nin some background areas. This experiment confirms that\\nPGI can indeed provide better gradients to update parame-\\nters and enable the feedforward stage of the main branch to\\nretain more important features.\\n6. Conclusions\\nIn this paper, we propose to use PGI to solve the infor-\\nmation bottleneck problem and the problem that the deep\\nsupervision mechanism is not suitable for lightweight neu-\\nral networks. We designed GELAN, a highly efficient\\nand lightweight neural network. In terms of object detec-\\ntion, GELAN has strong and stable performance at different\\ncomputational blocks and depth settings. It can indeed be\\nwidely expanded into a model suitable for various inference\\ndevices. For the above two issues, the introduction of PGI\\nallows both lightweight models and deep models to achieve\\nsignificant improvements in accuracy. The YOLOv9, de-\\nsigned by combining PGI and GELAN, has shown strong\\ncompetitiveness. Its excellent design allows the deep model\\nto reduce the number of parameters by 49% and the amount\\nof calculations by 43% compared with YOLOv8, but it still\\nhas a 0.6% AP improvement on MS COCO dataset.\\n7. Acknowledgements\\nThe authors wish to thank National Center for High-\\nperformance Computing (NCHC) for providing computa-\\ntional and storage resources.\\n10', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 9}),\n",
       " Document(page_content='References\\n[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT:\\nBERT pre-training of image transformers. In International\\nConference on Learning Representations (ICLR) , 2022. 2\\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\\nYuan Mark Liao. YOLOv4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934 , 2020. 3\\n[3] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiang-\\nwen Kong, Jun Li, and Xiangyu Zhang. Reversible column\\nnetworks. In International Conference on Learning Repre-\\nsentations (ICLR) , 2023. 2, 3, 5\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\\nto-end object detection with transformers. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\npages 213–229, 2020. 3\\n[5] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\\nJunni Zou. AP-loss for accurate one-stage object detection.\\nIEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence (TPAMI) , 43(11):3782–3798, 2020. 1\\n[6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang,\\nWenrui Dai, Hongkai Xiong, and Qi Tian. SdAE: Self-\\ndistillated masked autoencoder. In Proceedings of the Euro-\\npean Conference on Computer Vision (ECCV) , pages 108–\\n124, 2022. 2\\n[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin\\nHou, and Ming-Ming Cheng. YOLO-MS: rethinking multi-\\nscale representation learning for real-time object detection.\\narXiv preprint arXiv:2308.05480 , 2023. 1, 3, 7, 8\\n[8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong\\nWang, and Lu Yuan. DaVIT: Dual attention vision trans-\\nformers. In Proceedings of the European Conference on\\nComputer Vision (ECCV) , pages 74–92, 2022. 1\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. In International Con-\\nference on Learning Representations (ICLR) , 2021. 1, 2\\n[10] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,\\nand Weilin Huang. TOOD: Task-aligned one-stage object\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV) , pages 3490–3499,\\n2021. 1\\n[11] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu\\nZhang, Ming-Hsuan Yang, and Philip Torr. Res2Net: A\\nnew multi-scale backbone architecture. IEEE Transac-\\ntions on Pattern Analysis and Machine Intelligence (TPAMI) ,\\n43(2):652–662, 2019. 3\\n[12] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian\\nSun. OTA: Optimal transport assignment for object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 303–\\n312, 2021. 1\\n[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: Exceeding YOLO series in 2021. arXiv\\npreprint arXiv:2107.08430 , 2021. 3[14] Jocher Glenn. YOLOv5 release v7.0. https://github.\\ncom/ultralytics/yolov5/releases/tag/v7.\\n0, 2022. 3, 7\\n[15] Jocher Glenn. YOLOv8 release v8.1.0. https :\\n/ / github . com / ultralytics / ultralytics /\\nreleases/tag/v8.1.0 , 2024. 3, 7\\n[16] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B\\nGrosse. The reversible residual network: Backpropagation\\nwithout storing activations. Advances in Neural Information\\nProcessing Systems (NeurIPS) , 2017. 2, 3\\n[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence\\nmodeling with selective state spaces. arXiv preprint\\narXiv:2312.00752 , 2023. 1\\n[18] Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, and\\nChunhong Pan. AugFPN: Improving multi-scale fea-\\nture learning for object detection. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 12595–12604, 2020. 1, 3\\n[19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Ex-\\nploring disentangled representations in masked image mod-\\neling. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 2, 3', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 10}),\n",
       " Document(page_content='[19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Ex-\\nploring disentangled representations in masked image mod-\\neling. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 2, 3\\n[20] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.\\nBoundary-aware instance segmentation. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 5696–5704, 2017. 1, 3\\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 770–778, 2016. 1, 4, 8\\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nIdentity mappings in deep residual networks. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\npages 630–645. Springer, 2016. 1, 4\\n[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4700–4708, 2017. 1\\n[24] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Win-\\nston H Hsu. MonoDTR: Monocular 3D object detection with\\ndepth-aware transformer. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 4012–4021, 2022. 1, 3\\n[25] Lin Huang, Weisheng Li, Linlin Shen, Haojie Fu, Xue Xiao,\\nand Suihan Xiao. YOLOCS: Object detection based on dense\\nchannel compression for feature spatial solidification. arXiv\\npreprint arXiv:2305.04170 , 2023. 3\\n[26] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,\\nAndrew Zisserman, and Joao Carreira. Perceiver: General\\nperception with iterative attention. In International Confer-\\nence on Machine Learning (ICML) , pages 4651–4664, 2021.\\n1\\n[27] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\\nToutanova. BERT: Pre-training of deep bidirectional trans-\\nformers for language understanding. In Proceedings of\\nNAACL-HLT , volume 1, page 2, 2019. 2\\n11', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 10}),\n",
       " Document(page_content='[28] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\\nZhang, and Zhuowen Tu. Deeply-supervised nets. In Ar-\\ntificial Intelligence and Statistics , pages 562–570, 2015. 1,\\n2, 3\\n[29] Alex Levinshtein, Alborz Rezazadeh Sereshkeh, and Kon-\\nstantinos Derpanis. DATNet: Dense auxiliary tasks for ob-\\nject detection. In Proceedings of the IEEE/CVF Winter Con-\\nference on Applications of Computer Vision (WACV) , pages\\n1419–1427, 2020. 1, 3\\n[30] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng\\nCheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-\\niang Chu. YOLOv6 v3.0: A full-scale reloading. arXiv\\npreprint arXiv:2301.05586 , 2023. 3, 7, 2, 4\\n[31] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei\\nGeng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, et al. YOLOv6: A single-stage object de-\\ntection framework for industrial applications. arXiv preprint\\narXiv:2209.02976 , 2022. 3\\n[32] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng\\nLi, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang,\\nWenhai Wang, et al. Uni-perceiver v2: A generalist model\\nfor large-scale vision and vision-language tasks. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 2691–2700, 2023. 1\\n[33] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A\\ndual weighting label assignment scheme for object detection.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 9387–9396,\\n2022. 1\\n[34] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\\nZhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-\\nNet: A composite backbone network architecture for object\\ndetection. IEEE Transactions on Image Processing (TIP) ,\\n2022. 3, 9\\n[35] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyra-\\nmid networks for object detection. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 2117–2125, 2017. 3\\n[36] Zhihao Lin, Yongtao Wang, Jinhe Zhang, and Xiaojie Chu.\\nDynamicDet: A unified dynamic architecture for object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n6282–6291, 2023. 3, 6, 2, 4\\n[37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\\nPath aggregation network for instance segmentation. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR) , pages 8759–8768, 2018.\\n3, 5\\n[38] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi\\nXie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba:\\nVisual state space model. arXiv preprint arXiv:2401.10166 ,\\n2024. 1\\n[39] Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang,\\nQijie Zhao, Zhi Tang, and Haibin Ling. CBNet: A novel\\ncomposite backbone network architecture for object detec-\\ntion. In Proceedings of the AAAI Conference on Artificial\\nIntelligence (AAAI) , pages 11653–11660, 2020. 3[40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\\nSwin transformer v2: Scaling up capacity and resolution. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR) , 2022. 1\\n[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV) , pages 10012–10022, 2021. 1\\n[42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A ConvNet for the\\n2020s. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 11976–\\n11986, 2022. 1\\n[43] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang,\\nJinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and\\nYi Liu. DETRs beat YOLOs on real-time object detection.\\narXiv preprint arXiv:2304.08069 , 2023. 1, 3, 7, 8, 2, 4', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 11}),\n",
       " Document(page_content='Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and\\nYi Liu. DETRs beat YOLOs on real-time object detection.\\narXiv preprint arXiv:2304.08069 , 2023. 1, 3, 7, 8, 2, 4\\n[44] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou,\\nYudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.\\nRTMDet: An empirical study of designing real-time object\\ndetectors. arXiv preprint arXiv:2212.07784 , 2022. 8, 2, 3, 4\\n[45] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. A ranking-based, balanced loss function unify-\\ning classification and localisation in object detection. Ad-\\nvances in Neural Information Processing Systems (NeurIPS) ,\\n33:15534–15545, 2020. 1\\n[46] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. Rank & sort loss for object detection and instance\\nsegmentation. In Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision (ICCV) , pages 3009–\\n3018, 2021. 1\\n[47] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Unified, real-time object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 779–\\n788, 2016. 3\\n[48] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7263–7271, 2017. 3\\n[49] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental\\nimprovement. arXiv preprint arXiv:1804.02767 , 2018. 3, 8\\n[50] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\\ntersection over union: A metric and a loss for bounding box\\nregression. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n658–666, 2019. 1\\n[51] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,\\nYurong Chen, and Xiangyang Xue. Object detection from\\nscratch with deep supervision. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence (TPAMI) , 42(2):398–412,\\n2019. 1, 2\\n[52] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-\\nactor: A multi-task transformer for robotic manipulation.\\n12', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 11}),\n",
       " Document(page_content='InConference on Robot Learning (CoRL) , pages 785–799,\\n2023. 1\\n[53] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan,\\nChanghu Wang, and Ping Luo. What makes for end-to-end\\nobject detection? In International Conference on Machine\\nLearning (ICML) , pages 9934–9944, 2021. 3\\n[54] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n1–9, 2015. 1, 2, 3\\n[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\\ntecture for computer vision. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 2818–2826, 2016. 1\\n[56] Zineng Tang, Jaemin Cho, Jie Lei, and Mohit Bansal.\\nPerceiver-VL: Efficient vision-and-language modeling with\\niterative latent attention. In Proceedings of the IEEE/CVF\\nWinter Conference on Applications of Computer Vision\\n(WACV) , pages 4410–4420, 2023. 1\\n[57] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nFully convolutional one-stage object detection. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV) , pages 9627–9636, 2019. 3\\n[58] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nA simple and strong anchor-free object detector. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(TPAMI) , 44(4):1922–1933, 2022. 3\\n[59] Naftali Tishby and Noga Zaslavsky. Deep learning and the\\ninformation bottleneck principle. In IEEE Information The-\\nory Workshop (ITW) , pages 1–5, 2015. 2, 4\\n[60] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\\nPeyman Milanfar, Alan Bovik, and Yinxiao Li. MaxVIT:\\nMulti-axis vision transformer. In Proceedings of the Euro-\\npean Conference on Computer Vision (ECCV) , pages 459–\\n479, 2022. 1\\n[61] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo,\\nChuanjian Liu, Kai Han, and Yunhe Wang. Gold-YOLO:\\nEfficient object detector via gather-and-distribute mecha-\\nnism. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 3, 7, 2, 4\\n[62] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Scaled-YOLOv4: Scaling cross stage\\npartial network. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 13029–13038, 2021. 3\\n[63] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. YOLOv7: Trainable bag-of-freebies\\nsets new state-of-the-art for real-time object detectors. In\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 7464–7475,\\n2023. 3, 6, 7, 9, 1\\n[64] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet: A\\nnew backbone that can enhance learning capability of CNN.InProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition Workshops (CVPRW) , pages\\n390–391, 2020. 3, 6, 8\\n[65] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh.\\nDesigning network design strategies through gradient path\\nanalysis. Journal of Information Science and Engineering\\n(JISE) , 39(4):975–995, 2023. 2, 3, 6\\n[66] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\\nYou only learn one representation: Unified network for mul-\\ntiple tasks. Journal of Information Science & Engineering\\n(JISE) , 39(3):691–709, 2023. 2, 3, 4\\n[67] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\\nSun, and Nanning Zheng. End-to-end object detection\\nwith fully convolutional network. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 15849–15858, 2021. 1, 3\\n[68] Liwei Wang, Chen-Yu Lee, Zhuowen Tu, and Svetlana\\nLazebnik. Training deeper convolutional networks with deep\\nsupervision. arXiv preprint arXiv:1505.02496 , 2015. 1, 2, 3\\n[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 12}),\n",
       " Document(page_content='supervision. arXiv preprint arXiv:1505.02496 , 2015. 1, 2, 3\\n[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\\nPyramid vision transformer: A versatile backbone for dense\\nprediction without convolutions. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision\\n(ICCV) , pages 568–578, 2021. 1\\n[70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. PVT\\nv2: Improved baselines with pyramid vision transformer.\\nComputational Visual Media , 8(3):415–424, 2022. 1\\n[71] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\\nChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\\nvNeXt v2: Co-designing and scaling convnets with masked\\nautoencoders. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n16133–16142, 2023. 1, 2\\n[72] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\\nKaiming He. Aggregated residual transformations for deep\\nneural networks. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 1492–1500, 2017. 1\\n[73] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\\nBao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: A simple\\nframework for masked image modeling. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 9653–9663, 2022. 2\\n[74] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,\\nCheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing\\nDang, Shengyu Wei, Yuning Du, et al. PP-YOLOE: An\\nevolved version of YOLO. arXiv preprint arXiv:2203.16250 ,\\n2022. 3, 8, 2, 4\\n[75] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang,\\nYuan Zhang, and Xiuyu Sun. DAMO-YOLO: A re-\\nport on real-time object detection design. arXiv preprint\\narXiv:2211.15444 , 2022. 3, 7, 2, 4\\n[76] Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui, Yu\\nQiao, Hongsheng Li, and Peng Gao. MonoDETR: Depth-\\nguided transformer for monocular 3D object detection. In\\n13', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 12}),\n",
       " Document(page_content='Proceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV) , pages 9155–9166, 2023. 1, 3\\n[77] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\\nYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-\\nter learning for bounding box regression. In Proceedings of\\nthe AAAI Conference on Artificial Intelligence (AAAI) , vol-\\nume 34, pages 12993–13000, 2020. 1\\n[78] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\\nYin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D\\nobject detection. In International Conference on 3D Vision\\n(3DV) , pages 85–94, 2019. 1\\n[79] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,\\nSongtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-\\nentiable label assignment for dense object detection. arXiv\\npreprint arXiv:2007.03496 , 2020. 1\\n[80] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,\\nWenyu Liu, and Xinggang Wang. Vision mamba: Efficient\\nvisual representation learning with bidirectional state space\\nmodel. arXiv preprint arXiv:2401.09417 , 2024. 1\\n[81] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng\\nLi, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-\\ntraining unified architecture for generic perception for zero-\\nshot and few-shot tasks. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 16804–16815, 2022. 1\\n[82] Zhuofan Zong, Guanglu Song, and Yu Liu. DETRs with\\ncollaborative hybrid assignments training. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 6748–6758, 2023. 3\\n14', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 13}),\n",
       " Document(page_content='Appendix\\nA. Implementation Details\\nTable 1. Hyper parameter settings of YOLOv9.\\nhyper parameter value\\nepochs 500\\noptimizer SGD\\ninitial learning rate 0.01\\nfinish learning rate 0.0001\\nlearning rate decay linear\\nmomentum 0.937\\nweight decay 0.0005\\nwarm-up epochs 3\\nwarm-up momentum 0.8\\nwarm-up bias learning rate 0.1\\nbox loss gain 7.5\\nclass loss gain 0.5\\nDFL loss gain 1.5\\nHSV saturation augmentation 0.7\\nHSV value augmentation 0.4\\ntranslation augmentation 0.1\\nscale augmentation 0.9\\nmosaic augmentation 1.0\\nMixUp augmentation 0.15\\ncopy & paste augmentation 0.3\\nclose mosaic epochs 15\\nThe training parameters of YOLOv9 are shown in Ta-\\nble 1. We fully follow the settings of YOLOv7 AF [63],\\nwhich is to use SGD optimizer to train 500 epochs. We first\\nwarm-up for 3 epochs and only update the bias during the\\nwarm-up stage. Next we step down from the initial learning\\nrate 0.01 to 0.0001 in linear decay manner, and the data aug-\\nmentation settings are listed in the bottom part of Table 1.\\nWe shut down mosaic data augmentation operations on the\\nlast 15 epochs.Table 2. Network configurations of YOLOv9.\\nIndex Module Route Filters Depth Size Stride\\n0 Conv – 64 – 3 2\\n1 Conv 0 128 – 3 2\\n2 CSP-ELAN 1 256, 128, 64 2, 1 – 1\\n3 DOWN 2 256 – 3 2\\n4 CSP-ELAN 3 512, 256, 128 2, 1 – 1\\n5 DOWN 4 512 – 3 2\\n6 CSP-ELAN 5 512, 512, 256 2, 1 – 1\\n7 DOWN 6 512 – 3 2\\n8 CSP-ELAN 7 512, 512, 256 2, 1 – 1\\n9 SPP-ELAN 8 512, 256, 256 3, 1 – 1\\n10 Up 9 512 – – 2\\n11 Concat 10, 6 1024 – – 1\\n12 CSP-ELAN 11 512, 512, 256 2, 1 – 1\\n13 Up 12 512 – – 2\\n14 Concat 13, 4 1024 – – 1\\n15 CSP-ELAN 14 256, 256, 128 2, 1 – 1\\n16 DOWN 15 256 – 3 2\\n17 Concat 16, 12 768 – – 1\\n18 CSP-ELAN 17 512, 512, 256 2, 1 – 1\\n19 DOWN 18 512 – 3 2\\n20 Concat 19, 9 1024 – – 1\\n21 CSP-ELAN 20 512, 512, 256 2, 1 – 1\\n22 Predict 15, 18, 21 – – – –\\nThe network topology of YOLOv9 completely follows\\nYOLOv7 AF [63], that is, we replace ELAN with the pro-\\nposed CSP-ELAN block. As listed in Table 2, the depth\\nparameters of CSP-ELAN are represented as ELAN depth\\nand CSP depth, respectively. As for the parameters of CSP-\\nELAN filters, they are represented as ELAN output fil-\\nter, CSP output filter, and CSP inside filter. In the down-\\nsampling module part, we simplify CSP-DOWN module to\\nDOWN module. DOWN module is composed of a pooling\\nlayer with size 2 and stride 1, and a Conv layer with size 3\\nand stride 2. Finally, we optimized the prediction layer and\\nreplaced top, left, bottom, and right in the regression branch\\nwith decoupled branch.\\n1', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 14}),\n",
       " Document(page_content='Table 3. Comparison of state-of-the-art object detectors with different training settings.\\nModel #Param. (M) FLOPs (G) AP 50:95 (%) AP 50(%) AP 75(%) AP S(%) AP M(%) AP L(%)Train-from-scratchDy-YOLOv7 [36] – 181.7 53.9 72.2 58.7 35.3 57.6 66.4\\nDy-YOLOv7-X [36] – 307.9 55.0 73.2 60.0 36.6 58.7 68.5\\nYOLOv9-S (Ours) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nYOLOv9-M (Ours) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nYOLOv9-C (Ours) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nYOLOv9-E (Ours) 35.0 148.1 54.5 71.7 59.2 38.1 59.9 70.3\\nYOLOv9-E (Ours) 44.8 187.0 55.1 72.3 60.7 38.7 60.6 71.4\\nYOLOv9-E (Ours) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4ImageNet PretrainedRTMDet-T [44] 4.8 12.6 41.1 57.9 – – – –\\nRTMDet-S [44] 9.0 25.6 44.6 61.9 – – – –\\nRTMDet-M [44] 24.7 78.6 49.4 66.8 – – – –\\nRTMDet-L [44] 52.3 160.4 51.5 68.8 – – – –\\nRTMDet-X [44] 94.9 283.4 52.8 70.4 – – – –\\nPPYOLOE-S [74] 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nPPYOLOE-M [74] 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nPPYOLOE-L [74] 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nPPYOLOE-X [74] 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\nRT DETR-L [43] 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nRT DETR-X [43] 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nRT DETR-R18 [43] 20 60 46.5 63.8 – – – –\\nRT DETR-R34 [43] 31 92 48.9 66.8 – – – –\\nRT DETR-R50M [43] 36 100 51.3 69.6 – – – –\\nRT DETR-R50 [43] 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nRT DETR-R101 [43] 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nGold YOLO-S [61] 21.5 46.0 45.5 62.2 – – – –\\nGold YOLO-M [61] 41.3 57.5 50.2 67.5 – – – –\\nGold YOLO-L [61] 75.1 151.7 52.3 69.6 – – – –Knowledge DistillationYOLOv6-N v3.0 [30] 4.7 11.4 37.5 53.1 – – – –\\nYOLOv6-S v3.0 [30] 18.5 45.3 45.0 61.8 – – – –\\nYOLOv6-M v3.0 [30] 34.9 85.8 50.0 66.9 – – – –\\nYOLOv6-L v3.0 [30] 59.6 150.7 52.8 70.3 – – – –\\nDAMO YOLO-T [75] 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nDAMO YOLO-S [75] 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nDAMO YOLO-M [75] 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nDAMO YOLO-L [75] 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nGold YOLO-N [61] 5.6 12.1 39.9 55.9 – – – –\\nGold YOLO-S [61] 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-M [61] 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-L [61] 75.1 151.7 53.2 70.5 – – – –Complex SettingGold YOLO-S [61] 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-M [61] 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-L [61] 75.1 151.7 53.3 70.9 – – – –\\nYOLOR-CSP [66] 52.9 120.4 52.8 71.2 57.6 – – –\\nYOLOR-CSP-X [66] 96.9 226.8 54.8 73.1 59.7 – – –\\nPPYOLOE+-S [74] 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE+-M [74] 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE+-L [74] 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE+-X [74] 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nB. More Comparison\\nWe compare YOLOv9 to state-of-the-art real-time object\\ndetectors trained with different methods. It mainly includes\\nfour different training methods: (1) train-from-scratch: we\\nhave completed most of the comparisons in the text. Here\\nare only list of additional data of DynamicDet [36] for com-\\nparisons; (2) Pretrained by ImageNet: this includes two\\nmethods of using ImageNet for supervised pretrain and self-\\nsupervised pretrain; (3) knowledge distillation: a method\\nto perform additional self-distillation after training is com-pleted; and (4) a more complex training process: a combi-\\nnation of steps including pretrained by ImageNet, knowl-\\nedge distillation, DAMO-YOLO and even additional pre-\\ntrained large object detection dataset. We show the results\\nin Table 3. From this table, we can see that our proposed\\nYOLOv9 performed better than all other methods. Com-\\npared with PPYOLOE+-X trained using ImageNet and Ob-\\njects365, our method still reduces the number of parameters\\nby 54% and the amount of computation by 9%, and improv-\\ning 0.4% AP.\\n2', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 15}),\n",
       " Document(page_content='Table 4. Comparison of state-of-the-art object detectors with different training settings (sorted by number of parameters).\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv6-N v3.0 [30] (D) 4.7 11.4 37.5 53.1 – – – –\\nRTMDet-T [44] (I) 4.8 12.6 41.1 57.9 – – – –\\nGold YOLO-N [61] (D) 5.6 12.1 39.9 55.9 – – – –\\nYOLOv9-S (S) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nPPYOLOE+-S [74] (C) 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE-S [74] (I) 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nDAMO YOLO-T [75] (D) 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nRTMDet-S [44] (I) 9.0 25.6 44.6 61.9 – – – –\\nDAMO YOLO-S [75] (D) 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nYOLOv6-S v3.0 [30] (D) 18.5 45.3 45.0 61.8 – – – –\\nRT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\\nYOLOv9-M (S) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nGold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\\nPPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nRTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\\nYOLOv9-C (S) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nDAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nRT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\\nRT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nYOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\\nYOLOv9-E (S) 35.0 148.1 54.5 71.7 59.2 38.1 59.9 70.3\\nRT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\\nGold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\\nRT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nDAMO YOLO-L [75] (D) 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nYOLOv9-E (S) 44.8 187.0 55.1 72.3 60.7 38.7 60.6 71.4\\nPPYOLOE+-L [74] (C) 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE-L [74] (I) 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nRTMDet-L [44] (I) 52.3 160.4 51.5 68.8 – – – –\\nYOLOR-CSP [66] (C) 52.9 120.4 52.8 71.2 57.6 – – –\\nYOLOv9-E (S) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4\\nYOLOv6-L v3.0 [30] (D) 59.6 150.7 52.8 70.3 – – – –\\nRT DETR-X [43] (I) 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nGold YOLO-L [61] (C) 75.1 151.7 53.3 70.9 – – – –\\nGold YOLO-L [61] (D) 75.1 151.7 53.2 70.5 – – – –\\nGold YOLO-L [61] (I) 75.1 151.7 52.3 69.6 – – – –\\nRT DETR-R101 [43] (I) 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nRTMDet-X [44] (I) 94.9 283.4 52.8 70.4 – – – –\\nYOLOR-CSP-X [66] (C) 96.9 226.8 54.8 73.1 59.7 – – –\\nPPYOLOE+-X [74] (C) 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nPPYOLOE-X [74] (I) 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\n1(S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.\\nTable 4 shows the performance of all models sorted by\\nparameter size. Our proposed YOLOv9 is Pareto optimal\\nin all models of different sizes. Among them, we found no\\nother method for Pareto optimal in models with more than\\n20M parameters. The above experimental data shows that\\nour YOLOv9 has excellent parameter usage efficiency.Shown in Table 5 is the performance of all participat-\\ning models sorted by the amount of computation. Our pro-\\nposed YOLOv9 is Pareto optimal in all models with differ-\\nent scales. Among models with more than 60 GFLOPs, only\\nELAN-based DAMO-YOLO and DETR-based RT DETR\\ncan rival the proposed YOLOv9. The above comparison\\nresults show that YOLOv9 has the most outstanding per-\\nformance in the trade-off between computation complexity\\nand accuracy.\\n3', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 16}),\n",
       " Document(page_content='Table 5. Comparison of state-of-the-art object detectors with different training settings (sorted by amount of computation).\\nModel #Param. (M) FLOPs (G) APval\\n50:95(%) APval\\n50(%) APval\\n75(%) APval\\nS(%) APval\\nM(%) APval\\nL(%)\\nYOLOv6-N v3.0 [30] (D) 4.7 11.4 37.5 53.1 – – – –\\nGold YOLO-N [61] (D) 5.6 12.1 39.9 55.9 – – – –\\nRTMDet-T [44] (I) 4.8 12.6 41.1 57.9 – – – –\\nPPYOLOE+-S [74] (C) 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\\nPPYOLOE-S [74] (I) 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\\nDAMO YOLO-T [75] (D) 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\\nRTMDet-S [44] (I) 9.0 25.6 44.6 61.9 – – – –\\nYOLOv9-S (S) 7.2 26.7 46.8 63.4 50.7 26.6 56.0 64.5\\nDAMO YOLO-S [75] (D) 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\\nYOLOv6-S v3.0 [30] (D) 18.5 45.3 45.0 61.8 – – – –\\nGold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\\nGold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\\nGold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\\nPPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\\nPPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\\nGold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\\nGold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\\nGold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\\nRT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\\nDAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\\nYOLOv9-M (S) 20.1 76.8 51.4 68.1 56.1 33.6 57.0 68.0\\nRTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\\nYOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\\nRT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\\nDAMO YOLO-L [75] (D) 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\\nRT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\\nYOLOv9-C (S) 25.5 102.8 53.0 70.2 57.8 36.2 58.5 69.3\\nRT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\\nPPYOLOE+-L [74] (C) 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\\nPPYOLOE-L [74] (I) 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\\nYOLOR-CSP [66] (C) 52.9 120.4 52.8 71.2 57.6 – – –\\nRT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\\nYOLOv9-E (S) 35.0 148.1 54.5 71.7 59.2 38.1 59.9 70.3\\nYOLOv6-L v3.0 [30] (D) 59.6 150.7 52.8 70.3 – – – –\\nGold YOLO-L [61] (C) 75.1 151.7 53.3 70.9 – – – –\\nGold YOLO-L [61] (D) 75.1 151.7 53.2 70.5 – – – –\\nGold YOLO-L [61] (I) 75.1 151.7 52.3 69.6 – – – –\\nRTMDet-L [44] (I) 52.3 160.4 51.5 68.8 – – – –\\nDy-YOLOv7 [36] (S) – 181.7 53.9 72.2 58.7 35.3 57.6 66.4\\nYOLOv9-E (S) 44.8 187.0 55.1 72.3 60.7 38.7 60.6 71.4\\nYOLOv9-E (S) 58.1 192.5 55.6 72.8 60.6 40.2 61.0 71.4\\nPPYOLOE+-X [74] (C) 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\\nPPYOLOE-X [74] (I) 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\\nYOLOR-CSP-X [66] (C) 96.9 226.8 54.8 73.1 59.7 – – –\\nRT DETR-X [43] (I) 67 234 54.8 73.1 59.4 35.7 59.6 72.9\\nRT DETR-R101 [43] (I) 76 259 54.3 72.7 58.6 36.0 58.8 72.1\\nRTMDet-X [44] (I) 94.9 283.4 52.8 70.4 – – – –\\nDy-YOLOv7-X [36] (S) – 307.9 55.0 73.2 60.0 36.6 58.7 68.5\\n1(S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.\\n4', metadata={'source': 'https://arxiv.org/pdf/2402.13616v1.pdf', 'page': 17})]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "\n",
    "for link in links:\n",
    "    if link != None:\n",
    "\n",
    "        loader = PyPDFLoader(link)\n",
    "        all_documents.append(loader.load_and_split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Scalable Diffusion Models with Transformers\\nWilliam Peebles*\\nUC BerkeleySaining Xie\\nNew York University\\nFigure 1. Diffusion models with transformer backbones achieve state-of-the-art image quality. We show selected samples from two\\nof our class-conditional DiT-XL/2 models trained on ImageNet at 512 ×512 and 256×256 resolution, respectively.\\nAbstract\\nWe explore a new class of diffusion models based on the\\ntransformer architecture. We train latent diffusion models\\nof images, replacing the commonly-used U-Net backbone\\nwith a transformer that operates on latent patches. We an-\\nalyze the scalability of our Diffusion Transformers (DiTs)\\nthrough the lens of forward pass complexity as measured by\\nGﬂops. We ﬁnd that DiTs with higher Gﬂops—through in-\\ncreased transformer depth/width or increased number of in-\\nput tokens—consistently have lower FID. In addition to pos-\\nsessing good scalability properties, our largest DiT-XL/2\\nmodels outperform all prior diffusion models on the class-\\nconditional ImageNet 512 ×512 and 256×256 benchmarks,\\nachieving a state-of-the-art FID of 2.27 on the latter.1. Introduction\\nMachine learning is experiencing a renaissance powered\\nby transformers. Over the past ﬁve years, neural architec-\\ntures for natural language processing [8, 42], vision [10]\\nand several other domains have largely been subsumed by\\ntransformers [60]. Many classes of image-level genera-\\ntive models remain holdouts to the trend, though—while\\ntransformers see widespread use in autoregressive mod-\\nels [3,6,43,47], they have seen less adoption in other gener-\\native modeling frameworks. For example, diffusion models\\nhave been at the forefront of recent advances in image-level\\ngenerative models [9,46]; yet, they all adopt a convolutional\\nU-Net architecture as the de-facto choice of backbone.\\n*Work done during an internship at Meta AI, FAIR Team.\\nCode and project page available here.\\n1arXiv:2212.09748v2  [cs.CV]  2 Mar 2023', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 0}),\n",
       " Document(page_content='520 80 320GflopsDiameterFigure 2. ImageNet generation with Diffusion Transformers (DiTs). Bubble area indicates the ﬂops of the diffusion model. Left:\\nFID-50K (lower is better) of our DiT models at 400K training iterations. Performance steadily improves in FID as model ﬂops increase.\\nRight: Our best model, DiT-XL/2, is compute-efﬁcient and outperforms all prior U-Net-based diffusion models, like ADM and LDM.\\nThe seminal work of Ho et al. [19] ﬁrst introduced the\\nU-Net backbone for diffusion models. Having initially seen\\nsuccess within pixel-level autoregressive models and con-\\nditional GANs [23], the U-Net was inherited from Pixel-\\nCNN++ [52, 58] with a few changes. The model is con-\\nvolutional, comprised primarily of ResNet [15] blocks. In\\ncontrast to the standard U-Net [49], additional spatial self-\\nattention blocks, which are essential components in trans-\\nformers, are interspersed at lower resolutions. Dhariwal and\\nNichol [9] ablated several architecture choices for the U-\\nNet, such as the use of adaptive normalization layers [40] to\\ninject conditional information and channel counts for con-\\nvolutional layers. However, the high-level design of the U-\\nNet from Ho et al. has largely remained intact.\\nWith this work, we aim to demystify the signiﬁcance of\\narchitectural choices in diffusion models and offer empiri-\\ncal baselines for future generative modeling research. We\\nshow that the U-Net inductive bias is notcrucial to the per-\\nformance of diffusion models, and they can be readily re-\\nplaced with standard designs such as transformers. As a\\nresult, diffusion models are well-poised to beneﬁt from the\\nrecent trend of architecture uniﬁcation—e.g., by inheriting\\nbest practices and training recipes from other domains, as\\nwell as retaining favorable properties like scalability, ro-\\nbustness and efﬁciency. A standardized architecture would\\nalso open up new possibilities for cross-domain research.\\nIn this paper, we focus on a new class of diffusion models\\nbased on transformers. We call them Diffusion Transform-\\ners, or DiTs for short. DiTs adhere to the best practices of\\nVision Transformers (ViTs) [10], which have been shown to\\nscale more effectively for visual recognition than traditional\\nconvolutional networks (e.g., ResNet [15]).More speciﬁcally, we study the scaling behavior of trans-\\nformers with respect to network complexity vs. sample\\nquality . We show that by constructing and benchmark-\\ning the DiT design space under the Latent Diffusion Mod-\\nels(LDMs) [48] framework, where diffusion models are\\ntrained within a V AE’s latent space, we can successfully\\nreplace the U-Net backbone with a transformer. We further\\nshow that DiTs are scalable architectures for diffusion mod-\\nels: there is a strong correlation between the network com-\\nplexity (measured by Gﬂops) vs. sample quality (measured\\nby FID). By simply scaling-up DiT and training an LDM\\nwith a high-capacity backbone (118.6 Gﬂops), we are able\\nto achieve a state-of-the-art result of 2.27 FID on the class-\\nconditional 256×256ImageNet generation benchmark.\\n2. Related Work\\nTransformers. Transformers [60] have replaced domain-\\nspeciﬁc architectures across language, vision [10], rein-\\nforcement learning [5, 25] and meta-learning [39]. They\\nhave shown remarkable scaling properties under increas-\\ning model size, training compute and data in the language\\ndomain [26], as generic autoregressive models [17] and\\nas ViTs [63]. Beyond language, transformers have been\\ntrained to autoregressively predict pixels [6, 7, 38]. They\\nhave also been trained on discrete codebooks [59] as both\\nautoregressive models [11,47] and masked generative mod-\\nels [4, 14]; the former has shown excellent scaling behavior\\nup to 20B parameters [62]. Finally, transformers have been\\nexplored in DDPMs to synthesize non-spatial data; e.g., to\\ngenerate CLIP image embeddings in DALL ·E 2 [41,46]. In\\nthis paper, we study the scaling properties of transformers\\nwhen used as the backbone of diffusion models of images.\\n2', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 1}),\n",
       " Document(page_content='Multi-Head Self-AttentionLayer NormScale, ShiftMLPPointwise FeedforwardLayer NormScale, ShiftScale++\\nScale𝛾!,𝛽!𝛼!𝛾\",𝛽\"𝛼\"\\nInput TokensConditioningDiTBlock with adaLN-ZeroLatent Diffusion TransformerTimestep 𝑡Label 𝑦DiTBlockN xPatchifyLayer NormLinear and ReshapeEmbedNoiseΣ32 x 32 x 432 x 32 x 4\\nNoised Latent32 x 32 x 4Multi-Head Self-AttentionLayer NormPointwise Feedforward\\nLayer Norm++\\nInput TokensConditioningDiTBlock with Cross-AttentionMulti-Head Cross-AttentionLayer Norm+Multi-Head Self-AttentionLayer NormPointwise Feedforward++\\nInput TokensConditioningDiTBlock with In-Context ConditioningLayer Norm\\nConcatenate on Sequence DimensionFigure 3. The Diffusion Transformer (DiT) architecture. Left: We train conditional latent DiT models. The input latent is decomposed\\ninto patches and processed by several DiT blocks. Right: Details of our DiT blocks. We experiment with variants of standard transformer\\nblocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Adaptive layer norm works best.\\nDenoising diffusion probabilistic models (DDPMs).\\nDiffusion [19, 54] and score-based generative models [22,\\n56] have been particularly successful as generative models\\nof images [35,46,48,50], in many cases outperforming gen-\\nerative adversarial networks (GANs) [12] which had previ-\\nously been state-of-the-art. Improvements in DDPMs over\\nthe past two years have largely been driven by improved\\nsampling techniques [19, 27, 55], most notably classiﬁer-\\nfree guidance [21], reformulating diffusion models to pre-\\ndict noise instead of pixels [19] and using cascaded DDPM\\npipelines where low-resolution base diffusion models are\\ntrained in parallel with upsamplers [9, 20]. For all the dif-\\nfusion models listed above, convolutional U-Nets [49] are\\nthe de-facto choice of backbone architecture. Concurrent\\nwork [24] introduced a novel, efﬁcient architecture based\\non attention for DDPMs; we explore pure transformers.\\nArchitecture complexity. When evaluating architecture\\ncomplexity in the image generation literature, it is fairly\\ncommon practice to use parameter counts. In general, pa-\\nrameter counts can be poor proxies for the complexity of\\nimage models since they do not account for, e.g., image res-\\nolution which signiﬁcantly impacts performance [44, 45].\\nInstead, much of the model complexity analysis in this pa-\\nper is through the lens of theoretical Gﬂops. This brings us\\nin-line with the architecture design literature where Gﬂops\\nare widely-used to gauge complexity. In practice, the\\ngolden complexity metric is still up for debate as it fre-\\nquently depends on particular application scenarios. Nichol\\nand Dhariwal’s seminal work improving diffusion mod-\\nels [9, 36] is most related to us—there, they analyzed the\\nscalability and Gﬂop properties of the U-Net architecture\\nclass. In this paper, we focus on the transformer class.3. Diffusion Transformers\\n3.1. Preliminaries\\nDiffusion formulation. Before introducing our architec-\\nture, we brieﬂy review some basic concepts needed to\\nunderstand diffusion models (DDPMs) [19, 54]. Gaus-\\nsian diffusion models assume a forward noising process\\nwhich gradually applies noise to real data x0:q(xt|x0) =\\nN(xt;√¯αtx0,(1−¯αt)I), where constants ¯αtare hyperpa-\\nrameters. By applying the reparameterization trick, we can\\nsamplext=√¯αtx0+√1−¯αtϵt, whereϵt∼N(0,I).\\nDiffusion models are trained to learn the reverse process\\nthat inverts forward process corruptions: pθ(xt−1|xt) =\\nN(µθ(xt),Σθ(xt)), where neural networks are used to pre-\\ndict the statistics of pθ. The reverse process model is\\ntrained with the variational lower bound [30] of the log-\\nlikelihood of x0, which reduces to L(θ) =−p(x0|x1) +∑\\ntDKL(q∗(xt−1|xt,x0)||pθ(xt−1|xt)), excluding an ad-\\nditional term irrelevant for training. Since both q∗andpθ\\nare Gaussian,DKLcan be evaluated with the mean and co-\\nvariance of the two distributions. By reparameterizing µθas\\na noise prediction network ϵθ, the model can be trained us-', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 2}),\n",
       " Document(page_content='are Gaussian,DKLcan be evaluated with the mean and co-\\nvariance of the two distributions. By reparameterizing µθas\\na noise prediction network ϵθ, the model can be trained us-\\ning simple mean-squared error between the predicted noise\\nϵθ(xt)and the ground truth sampled Gaussian noise ϵt:\\nLsimple (θ) =||ϵθ(xt)−ϵt||2\\n2. But, in order to train diffu-\\nsion models with a learned reverse process covariance Σθ,\\nthe fullDKLterm needs to be optimized. We follow Nichol\\nand Dhariwal’s approach [36]: train ϵθwithLsimple , and\\ntrainΣθwith the fullL. Oncepθis trained, new images can\\nbe sampled by initializing xtmax∼ N (0,I)and sampling\\nxt−1∼pθ(xt−1|xt)via the reparameterization trick.\\n3', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 2}),\n",
       " Document(page_content='Classiﬁer-free guidance. Conditional diffusion models\\ntake extra information as input, such as a class label c.\\nIn this case, the reverse process becomes pθ(xt−1|xt,c),\\nwhereϵθandΣθare conditioned on c. In this setting,\\nclassiﬁer-free guidance can be used to encourage the sam-\\npling procedure to ﬁnd xsuch that logp(c|x)is high [21].\\nBy Bayes Rule, logp(c|x)∝logp(x|c)−logp(x), and\\nhence∇xlogp(c|x)∝∇xlogp(x|c)−∇xlogp(x). By in-\\nterpreting the output of diffusion models as the score func-\\ntion, the DDPM sampling procedure can be guided to sam-\\nplexwith highp(x|c)by:ˆϵθ(xt,c) =ϵθ(xt,∅) +s·\\n∇xlogp(x|c)∝ϵθ(xt,∅)+s·(ϵθ(xt,c)−ϵθ(xt,∅)), where\\ns>1indicates the scale of the guidance (note that s= 1re-\\ncovers standard sampling). Evaluating the diffusion model\\nwithc=∅is done by randomly dropping out cduring\\ntraining and replacing it with a learned “null” embedding\\n∅. Classiﬁer-free guidance is widely-known to yield sig-\\nniﬁcantly improved samples over generic sampling tech-\\nniques [21, 35, 46], and the trend holds for our DiT models.\\nLatent diffusion models. Training diffusion models di-\\nrectly in high-resolution pixel space can be computationally\\nprohibitive. Latent diffusion models (LDMs) [48] tackle this\\nissue with a two-stage approach: (1) learn an autoencoder\\nthat compresses images into smaller spatial representations\\nwith a learned encoder E; (2) train a diffusion model of\\nrepresentations z=E(x)instead of a diffusion model of\\nimagesx(Eis frozen). New images can then be generated\\nby sampling a representation zfrom the diffusion model\\nand subsequently decoding it to an image with the learned\\ndecoderx=D(z).\\nAs shown in Figure 2, LDMs achieve good performance\\nwhile using a fraction of the Gﬂops of pixel space diffusion\\nmodels like ADM. Since we are concerned with compute\\nefﬁciency, this makes them an appealing starting point for\\narchitecture exploration. In this paper, we apply DiTs to\\nlatent space, although they could be applied to pixel space\\nwithout modiﬁcation as well. This makes our image genera-\\ntion pipeline a hybrid-based approach; we use off-the-shelf\\nconvolutional V AEs and transformer-based DDPMs.\\n3.2. Diffusion Transformer Design Space\\nWe introduce Diffusion Transformers (DiTs), a new ar-\\nchitecture for diffusion models. We aim to be as faithful to\\nthe standard transformer architecture as possible to retain\\nits scaling properties. Since our focus is training DDPMs of\\nimages (speciﬁcally, spatial representations of images), DiT\\nis based on the Vision Transformer (ViT) architecture which\\noperates on sequences of patches [10]. DiT retains many of\\nthe best practices of ViTs. Figure 3 shows an overview of\\nthe complete DiT architecture. In this section, we describe\\nthe forward pass of DiT, as well as the components of the\\ndesign space of the DiT class.\\n𝑝𝐼𝑝𝐼𝑇=𝐼/𝑝!Noised LatentI×I×CInput Tokens T×dDiTBlockFigure 4. Input speciﬁcations for DiT. Given patch size p×p,\\na spatial representation (the noised latent from the V AE) of shape\\nI×I×Cis “patchiﬁed” into a sequence of length T= (I/p)2\\nwith hidden dimension d. A smaller patch size presults in a longer\\nsequence length and thus more Gﬂops.\\nPatchify. The input to DiT is a spatial representation z\\n(for256×256×3images,zhas shape 32×32×4). The\\nﬁrst layer of DiT is “patchify,” which converts the spatial\\ninput into a sequence of Ttokens, each of dimension d,\\nby linearly embedding each patch in the input. Following\\npatchify, we apply standard ViT frequency-based positional\\nembeddings (the sine-cosine version) to all input tokens.\\nThe number of tokens Tcreated by patchify is determined\\nby the patch size hyperparameter p. As shown in Figure 4,\\nhalvingpwill quadruple T, and thus at least quadruple total\\ntransformer Gﬂops. Although it has a signiﬁcant impact on\\nGﬂops, note that changing phas no meaningful impact on\\ndownstream parameter counts.\\nWe addp= 2,4,8to the DiT design space.\\nDiT block design. Following patchify, the input tokens\\nare processed by a sequence of transformer blocks. In ad-', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 3}),\n",
       " Document(page_content='downstream parameter counts.\\nWe addp= 2,4,8to the DiT design space.\\nDiT block design. Following patchify, the input tokens\\nare processed by a sequence of transformer blocks. In ad-\\ndition to noised image inputs, diffusion models sometimes\\nprocess additional conditional information such as noise\\ntimestepst, class labels c, natural language, etc. We explore\\nfour variants of transformer blocks that process conditional\\ninputs differently. The designs introduce small, but impor-\\ntant, modiﬁcations to the standard ViT block design. The\\ndesigns of all blocks are shown in Figure 3.\\n–In-context conditioning. We simply append the vec-\\ntor embeddings of tandcas two additional tokens in\\nthe input sequence, treating them no differently from\\nthe image tokens. This is similar to cls tokens in\\nViTs, and it allows us to use standard ViT blocks with-\\nout modiﬁcation. After the ﬁnal block, we remove the\\nconditioning tokens from the sequence. This approach\\nintroduces negligible new Gﬂops to the model.\\n4', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 3}),\n",
       " Document(page_content='100K 200K 300K 400K\\nTraining Steps20406080100FID-50K\\nXL/2 In-Context\\nXL/2 Cross-Attention\\nXL/2 adaLN\\nXL/2 adaLN-ZeroFigure 5. Comparing different conditioning strategies. adaLN-\\nZero outperforms cross-attention and in-context conditioning at all\\nstages of training.\\n–Cross-attention block. We concatenate the embeddings\\noftandcinto a length-two sequence, separate from\\nthe image token sequence. The transformer block is\\nmodiﬁed to include an additional multi-head cross-\\nattention layer following the multi-head self-attention\\nblock, similar to the original design from Vaswani et\\nal.[60], and also similar to the one used by LDM for\\nconditioning on class labels. Cross-attention adds the\\nmost Gﬂops to the model, roughly a 15% overhead.\\n–Adaptive layer norm (adaLN) block. Following\\nthe widespread usage of adaptive normalization lay-\\ners [40] in GANs [2, 28] and diffusion models with U-\\nNet backbones [9], we explore replacing standard layer\\nnorm layers in transformer blocks with adaptive layer\\nnorm (adaLN). Rather than directly learn dimension-\\nwise scale and shift parameters γandβ, we regress\\nthem from the sum of the embedding vectors of tand\\nc. Of the three block designs we explore, adaLN adds\\nthe least Gﬂops and is thus the most compute-efﬁcient.\\nIt is also the only conditioning mechanism that is re-\\nstricted to apply the same function to all tokens.\\n–adaLN-Zero block. Prior work on ResNets has found\\nthat initializing each residual block as the identity\\nfunction is beneﬁcial. For example, Goyal et al. found\\nthat zero-initializing the ﬁnal batch norm scale factor γ\\nin each block accelerates large-scale training in the su-\\npervised learning setting [13]. Diffusion U-Net mod-\\nels use a similar initialization strategy, zero-initializing\\nthe ﬁnal convolutional layer in each block prior to any\\nresidual connections. We explore a modiﬁcation of\\nthe adaLN DiT block which does the same. In addi-\\ntion to regressing γandβ, we also regress dimension-\\nwise scaling parameters αthat are applied immediately\\nprior to any residual connections within the DiT block.Model Layers N Hidden size dHeads Gﬂops (I=32,p=4)\\nDiT-S 12 384 6 1.4\\nDiT-B 12 768 12 5.6\\nDiT-L 24 1024 16 19.7\\nDiT-XL 28 1152 16 29.1\\nTable 1. Details of DiT models. We follow ViT [10] model con-\\nﬁgurations for the Small (S), Base (B) and Large (L) variants; we\\nalso introduce an XLarge (XL) conﬁg as our largest model.\\nWe initialize the MLP to output the zero-vector for all\\nα; this initializes the full DiT block as the identity\\nfunction. As with the vanilla adaLN block, adaLN-\\nZero adds negligible Gﬂops to the model.\\nWe include the in-context, cross-attention, adaptive layer\\nnorm and adaLN-Zero blocks in the DiT design space.\\nModel size. We apply a sequence of NDiT blocks, each\\noperating at the hidden dimension size d. Following ViT,\\nwe use standard transformer conﬁgs that jointly scale N,\\ndand attention heads [10, 63]. Speciﬁcally, we use four\\nconﬁgs: DiT-S, DiT-B, DiT-L and DiT-XL. They cover a\\nwide range of model sizes and ﬂop allocations, from 0.3\\nto 118.6 Gﬂops, allowing us to gauge scaling performance.\\nTable 1 gives details of the conﬁgs.\\nWe add B, S, L and XL conﬁgs to the DiT design space.\\nTransformer decoder. After the ﬁnal DiT block, we need\\nto decode our sequence of image tokens into an output noise\\nprediction and an output diagonal covariance prediction.\\nBoth of these outputs have shape equal to the original spa-\\ntial input. We use a standard linear decoder to do this; we\\napply the ﬁnal layer norm (adaptive if using adaLN) and lin-\\nearly decode each token into a p×p×2Ctensor, where Cis\\nthe number of channels in the spatial input to DiT. Finally,\\nwe rearrange the decoded tokens into their original spatial\\nlayout to get the predicted noise and covariance.\\nThe complete DiT design space we explore is patch size,\\ntransformer block architecture and model size.\\n4. Experimental Setup\\nWe explore the DiT design space and study the scaling', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 4}),\n",
       " Document(page_content='The complete DiT design space we explore is patch size,\\ntransformer block architecture and model size.\\n4. Experimental Setup\\nWe explore the DiT design space and study the scaling\\nproperties of our model class. Our models are named ac-\\ncording to their conﬁgs and latent patch sizes p; for exam-\\nple, DiT-XL/2 refers to the XLarge conﬁg and p= 2.\\nTraining. We train class-conditional latent DiT models at\\n256×256 and512×512 image resolution on the Ima-\\ngeNet dataset [31], a highly-competitive generative mod-\\neling benchmark. We initialize the ﬁnal linear layer with\\nzeros and otherwise use standard weight initialization tech-\\nniques from ViT. We train all models with AdamW [29,33].\\n5', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 4}),\n",
       " Document(page_content='Figure 6. Scaling the DiT model improves FID at all stages of training. We show FID-50K over training iterations for 12 of our DiT\\nmodels. Top row: We compare FID holding patch size constant. Bottom row: We compare FID holding model size constant. Scaling the\\ntransformer backbone yields better generative models across all model sizes and patch sizes.\\nWe use a constant learning rate of 1×10−4, no weight de-\\ncay and a batch size of 256. The only data augmentation\\nwe use is horizontal ﬂips. Unlike much prior work with\\nViTs [57, 61], we did not ﬁnd learning rate warmup nor\\nregularization necessary to train DiTs to high performance.\\nEven without these techniques, training was highly stable\\nacross all model conﬁgs and we did not observe any loss\\nspikes commonly seen when training transformers. Follow-\\ning common practice in the generative modeling literature,\\nwe maintain an exponential moving average (EMA) of DiT\\nweights over training with a decay of 0.9999. All results\\nreported use the EMA model. We use identical training hy-\\nperparameters across all DiT model sizes and patch sizes.\\nOur training hyperparameters are almost entirely retained\\nfrom ADM. We did not tune learning rates, decay/warm-up\\nschedules, Adam β1/β2or weight decays.\\nDiffusion. We use an off-the-shelf pre-trained variational\\nautoencoder (V AE) model [30] from Stable Diffusion [48].\\nThe V AE encoder has a downsample factor of 8—given an\\nRGB image xwith shape 256×256×3,z=E(x)has\\nshape 32×32×4. Across all experiments in this section,\\nour diffusion models operate in this Z-space. After sam-\\npling a new latent from our diffusion model, we decode it\\nto pixels using the V AE decoder x=D(z). We retain diffu-\\nsion hyperparameters from ADM [9]; speciﬁcally, we use a\\ntmax= 1000 linear variance schedule ranging from 1×10−4\\nto2×10−2, ADM’s parameterization of the covariance Σθ\\nand their method for embedding input timesteps and labels.\\nEvaluation metrics. We measure scaling performance\\nwith Fr ´echet Inception Distance (FID) [18], the standard\\nmetric for evaluating generative models of images.We follow convention when comparing against prior works\\nand report FID-50K using 250 DDPM sampling steps.\\nFID is known to be sensitive to small implementation de-\\ntails [37]; to ensure accurate comparisons, all values re-\\nported in this paper are obtained by exporting samples and\\nusing ADM’s TensorFlow evaluation suite [9]. FID num-\\nbers reported in this section do notuse classiﬁer-free guid-\\nance except where otherwise stated. We additionally report\\nInception Score [51], sFID [34] and Precision/Recall [32]\\nas secondary metrics.\\nCompute. We implement all models in JAX [1] and train\\nthem using TPU-v3 pods. DiT-XL/2, our most compute-\\nintensive model, trains at roughly 5.7 iterations/second on a\\nTPU v3-256 pod with a global batch size of 256.\\n5. Experiments\\nDiT block design. We train four of our highest Gﬂop\\nDiT-XL/2 models, each using a different block design—\\nin-context (119.4 Gﬂops), cross-attention (137.6 Gﬂops),\\nadaptive layer norm (adaLN, 118.6 Gﬂops) or adaLN-zero\\n(118.6 Gﬂops). We measure FID over the course of training.\\nFigure 5 shows the results. The adaLN-Zero block yields\\nlower FID than both cross-attention and in-context condi-\\ntioning while being the most compute-efﬁcient. At 400K\\ntraining iterations, the FID achieved with the adaLN-Zero\\nmodel is nearly half that of the in-context model, demon-\\nstrating that the conditioning mechanism critically affects\\nmodel quality. Initialization is also important—adaLN-\\nZero, which initializes each DiT block as the identity func-\\ntion, signiﬁcantly outperforms vanilla adaLN. For the rest\\nof the paper, all models will use adaLN-Zero DiT blocks.\\n6', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 5}),\n",
       " Document(page_content='Increasing transformer size \\nDecreasing patch size\\nFigure 7. Increasing transformer forward pass Gﬂops increases sample quality. Best viewed zoomed-in. We sample from all 12 of\\nour DiT models after 400K training steps using the same input latent noise and class label. Increasing the Gﬂops in the model—either by\\nincreasing transformer depth/width or increasing the number of input tokens—yields signiﬁcant improvements in visual ﬁdelity.\\n7', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 6}),\n",
       " Document(page_content='100101102\\nTransformer Gflops20406080100120140160FID-50K\\nCorrelation: -0.93S/8\\nS/4\\nS/2B/8\\nB/4\\nB/2L/8\\nL/4\\nL/2XL/8\\nXL/4\\nXL/2Figure 8. Transformer Gﬂops are strongly correlated with FID.\\nWe plot the Gﬂops of each of our DiT models and each model’s\\nFID-50K after 400K training steps.\\nScaling model size and patch size. We train 12 DiT mod-\\nels, sweeping over model conﬁgs (S, B, L, XL) and patch\\nsizes (8, 4, 2). Note that DiT-L and DiT-XL are signiﬁcantly\\ncloser to each other in terms of relative Gﬂops than other\\nconﬁgs. Figure 2 (left) gives an overview of the Gﬂops of\\neach model and their FID at 400K training iterations. In\\nall cases, we ﬁnd that increasing model size and decreasing\\npatch size yields considerably improved diffusion models.\\nFigure 6 (top) demonstrates how FID changes as model\\nsize is increased and patch size is held constant. Across all\\nfour conﬁgs, signiﬁcant improvements in FID are obtained\\nover all stages of training by making the transformer deeper\\nand wider. Similarly, Figure 6 (bottom) shows FID as patch\\nsize is decreased and model size is held constant. We again\\nobserve considerable FID improvements throughout train-\\ning by simply scaling the number of tokens processed by\\nDiT, holding parameters approximately ﬁxed.\\nDiT Gﬂops are critical to improving performance. The\\nresults of Figure 6 suggest that parameter counts do not\\nuniquely determine the quality of a DiT model. As model\\nsize is held constant and patch size is decreased, the trans-\\nformer’s total parameters are effectively unchanged (actu-\\nally, total parameters slightly decrease ), and only Gﬂops are\\nincreased. These results indicate that scaling model Gﬂops\\nis actually the key to improved performance. To investi-\\ngate this further, we plot the FID-50K at 400K training steps\\nagainst model Gﬂops in Figure 8. The results demonstrate\\nthat different DiT conﬁgs obtain similar FID values when\\ntheir total Gﬂops are similar (e.g., DiT-S/2 and DiT-B/4).\\nWe ﬁnd a strong negative correlation between model Gﬂops\\nand FID-50K, suggesting that additional model compute is\\nthe critical ingredient for improved DiT models. In Fig-\\nure 12 (appendix), we ﬁnd that this trend holds for other\\nmetrics such as Inception Score.\\n107108109101010111012\\nTraining Compute (Gflops)0255075100125150175200FID-50K\\n1015202530\\nS/8\\nS/4\\nS/2B/8\\nB/4\\nB/2L/8\\nL/4\\nL/2XL/8\\nXL/4\\nXL/2Figure 9. Larger DiT models use large compute more efﬁ-\\nciently. We plot FID as a function of total training compute.\\nLarger DiT models are more compute-efﬁcient. In\\nFigure 9, we plot FID as a function of total training compute\\nfor all DiT models. We estimate training compute as model\\nGﬂops·batch size·training steps·3, where the factor of\\n3 roughly approximates the backwards pass as being twice\\nas compute-heavy as the forward pass. We ﬁnd that small\\nDiT models, even when trained longer, eventually become\\ncompute-inefﬁcient relative to larger DiT models trained for\\nfewer steps. Similarly, we ﬁnd that models that are identi-\\ncal except for patch size have different performance proﬁles\\neven when controlling for training Gﬂops. For example,\\nXL/4 is outperformed by XL/2 after roughly 1010Gﬂops.\\nVisualizing scaling. We visualize the effect of scaling on\\nsample quality in Figure 7. At 400K training steps, we sam-\\nple an image from each of our 12 DiT models using iden-\\ntical starting noise xtmax, sampling noise and class labels.\\nThis lets us visually interpret how scaling affects DiT sam-\\nple quality. Indeed, scaling both model size and the number\\nof tokens yields notable improvements in visual quality.\\n5.1. State-of-the-Art Diffusion Models\\n256×256 ImageNet. Following our scaling analysis, we\\ncontinue training our highest Gﬂop model, DiT-XL/2, for\\n7M steps. We show samples from the model in Figures 1,\\nand we compare against state-of-the-art class-conditional\\ngenerative models. We report results in Table 2. When us-\\ning classiﬁer-free guidance, DiT-XL/2 outperforms all prior', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 7}),\n",
       " Document(page_content='and we compare against state-of-the-art class-conditional\\ngenerative models. We report results in Table 2. When us-\\ning classiﬁer-free guidance, DiT-XL/2 outperforms all prior\\ndiffusion models, decreasing the previous best FID-50K of\\n3.60 achieved by LDM to 2.27. Figure 2 (right) shows that\\nDiT-XL/2 (118.6 Gﬂops) is compute-efﬁcient relative to la-\\ntent space U-Net models like LDM-4 (103.6 Gﬂops) and\\nsubstantially more efﬁcient than pixel space U-Net mod-\\nels such as ADM (1120 Gﬂops) or ADM-U (742 Gﬂops).\\n8', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 7}),\n",
       " Document(page_content='Class-Conditional ImageNet 256 ×256\\nModel FID ↓ sFID↓ IS↑ Precision↑Recall↑\\nBigGAN-deep [2] 6.95 7.36 171.4 0.87 0.28\\nStyleGAN-XL [53] 2.30 4.02 265.12 0.78 0.53\\nADM [9] 10.94 6.02 100.98 0.69 0.63\\nADM-U 7.49 5.13 127.49 0.72 0.63\\nADM-G 4.59 5.25 186.70 0.82 0.52\\nADM-G, ADM-U 3.94 6.14 215.84 0.83 0.53\\nCDM [20] 4.88 - 158.71 - -\\nLDM-8 [48] 15.51 - 79.03 0.65 0.63\\nLDM-8-G 7.76 - 209.52 0.84 0.35\\nLDM-4 10.56 - 103.49 0.71 0.62\\nLDM-4-G (cfg=1.25) 3.95 - 178.22 0.81 0.55\\nLDM-4-G (cfg=1.50) 3.60 - 247.67 0.87 0.48\\nDiT-XL/2 9.62 6.85 121.50 0.67 0.67\\nDiT-XL/2-G (cfg=1.25) 3.22 5.28 201.77 0.76 0.62\\nDiT-XL/2-G (cfg=1.50) 2.27 4.60 278.24 0.83 0.57\\nTable 2. Benchmarking class-conditional image generation on\\nImageNet 256×256. DiT-XL/2 achieves state-of-the-art FID.\\nClass-Conditional ImageNet 512 ×512\\nModel FID ↓ sFID↓ IS↑ Precision↑Recall↑\\nBigGAN-deep [2] 8.43 8.13 177.90 0.88 0.29\\nStyleGAN-XL [53] 2.41 4.06 267.75 0.77 0.52\\nADM [9] 23.24 10.19 58.06 0.73 0.60\\nADM-U 9.96 5.62 121.78 0.75 0.64\\nADM-G 7.72 6.57 172.71 0.87 0.42\\nADM-G, ADM-U 3.85 5.86 221.72 0.84 0.53\\nDiT-XL/2 12.03 7.12 105.25 0.75 0.64\\nDiT-XL/2-G (cfg=1.25) 4.64 5.77 174.77 0.81 0.57\\nDiT-XL/2-G (cfg=1.50) 3.04 5.02 240.82 0.84 0.54\\nTable 3. Benchmarking class-conditional image generation on\\nImageNet 512×512. Note that prior work [9] measures Precision\\nand Recall using 1000 real samples for 512×512resolution; for\\nconsistency, we do the same.\\nOur method achieves the lowest FID of all prior generative\\nmodels, including the previous state-of-the-art StyleGAN-\\nXL [53]. Finally, we also observe that DiT-XL/2 achieves\\nhigher recall values at all tested classiﬁer-free guidance\\nscales compared to LDM-4 and LDM-8. When trained for\\nonly 2.35M steps (similar to ADM), XL/2 still outperforms\\nall prior diffusion models with an FID of 2.55.\\n512×512 ImageNet. We train a new DiT-XL/2 model on\\nImageNet at 512×512resolution for 3M iterations with\\nidentical hyperparameters as the 256×256model. With a\\npatch size of 2, this XL/2 model processes a total of 1024\\ntokens after patchifying the 64×64×4input latent (524.6\\nGﬂops). Table 3 shows comparisons against state-of-the-art\\nmethods. XL/2 again outperforms all prior diffusion models\\nat this resolution, improving the previous best FID of 3.85\\nachieved by ADM to 3.04. Even with the increased num-\\nber of tokens, XL/2 remains compute-efﬁcient. For exam-\\nple, ADM uses 1983 Gﬂops and ADM-U uses 2813 Gﬂops;\\nXL/2 uses 524.6 Gﬂops. We show samples from the high-\\nresolution XL/2 model in Figure 1 and the appendix.\\n101\\n102\\n103\\n104\\n105\\nSampling Compute (Gflops)20406080100120140160180FID-10K\\nS/8\\nS/4\\nS/2B/8\\nB/4\\nB/2L/8\\nL/4\\nL/2XL/8\\nXL/4\\nXL/2Figure 10. Scaling-up sampling compute does not compensate\\nfor a lack of model compute. For each of our DiT models trained\\nfor 400K iterations, we compute FID-10K using [16, 32, 64, 128,\\n256, 1000] sampling steps. For each number of steps, we plot the\\nFID as well as the Gﬂops used to sample each image. Small mod-\\nels cannot close the performance gap with our large models, even\\nif they sample with more test-time Gﬂops than the large models.\\n5.2. Scaling Model vs. Sampling Compute\\nDiffusion models are unique in that they can use addi-\\ntional compute after training by increasing the number of\\nsampling steps when generating an image. Given the im-\\npact of model Gﬂops on sample quality, in this section we\\nstudy if smaller- model compute DiTs can outperform larger\\nones by using more sampling compute . We compute FID\\nfor all 12 of our DiT models after 400K training steps, us-\\ning [16, 32, 64, 128, 256, 1000] sampling steps per-image.\\nThe main results are in Figure 10. Consider DiT-L/2 us-\\ning 1000 sampling steps versus DiT-XL/2 using 128 steps.\\nIn this case, L/2 uses 80.7Tﬂops to sample each image;\\nXL/2 uses 5×less compute— 15.2Tﬂops—to sample each\\nimage. Nonetheless, XL/2 has the better FID-10K (23.7\\nvs 25.9). In general, scaling-up sampling compute cannot\\ncompensate for a lack of model compute.\\n6. Conclusion', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 8}),\n",
       " Document(page_content='image. Nonetheless, XL/2 has the better FID-10K (23.7\\nvs 25.9). In general, scaling-up sampling compute cannot\\ncompensate for a lack of model compute.\\n6. Conclusion\\nWe introduce Diffusion Transformers (DiTs), a simple\\ntransformer-based backbone for diffusion models that out-\\nperforms prior U-Net models and inherits the excellent scal-\\ning properties of the transformer model class. Given the\\npromising scaling results in this paper, future work should\\ncontinue to scale DiTs to larger models and token counts.\\nDiT could also be explored as a drop-in backbone for text-\\nto-image models like DALL ·E 2 and Stable Diffusion.\\nAcknowledgements. We thank Kaiming He, Ronghang\\nHu, Alexander Berg, Shoubhik Debnath, Tim Brooks, Ilija\\nRadosavovic and Tete Xiao for helpful discussions. William\\nPeebles is supported by the NSF GRFP.\\n9', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 8}),\n",
       " Document(page_content='References\\n[1] James Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal Maclau-\\nrin, George Necula, Adam Paszke, Jake VanderPlas, Skye\\nWanderman-Milne, and Qiao Zhang. JAX: composable\\ntransformations of Python+NumPy programs, 2018. 6\\n[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\\nscale GAN training for high ﬁdelity natural image synthesis.\\nInICLR , 2019. 5, 9\\n[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\\nguage models are few-shot learners. In NeurIPS , 2020. 1\\n[4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\\nFreeman. Maskgit: Masked generative image transformer. In\\nCVPR , pages 11315–11325, 2022. 2\\n[5] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,\\nAditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srini-\\nvas, and Igor Mordatch. Decision transformer: Reinforce-\\nment learning via sequence modeling. In NeurIPS , 2021. 2\\n[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\\ntraining from pixels. In ICML , 2020. 1, 2\\n[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\\nGenerating long sequences with sparse transformers. arXiv\\npreprint arXiv:1904.10509 , 2019. 2\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\\nToutanova. Bert: Pre-training of deep bidirectional trans-\\nformers for language understanding. In NAACL-HCT , 2019.\\n1\\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models\\nbeat gans on image synthesis. In NeurIPS , 2021. 1, 2, 3, 5,\\n6, 9, 12\\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. In ICLR , 2020. 1, 2,\\n4, 5\\n[11] Patrick Esser, Robin Rombach, and Bj ¨orn Ommer. Taming\\ntransformers for high-resolution image synthesis, 2020. 2\\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\\nYoshua Bengio. Generative adversarial nets. In NIPS , 2014.\\n3\\n[13] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noord-\\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\\nYangqing Jia, and Kaiming He. Accurate, large minibatch\\nsgd: Training imagenet in 1 hour. arXiv:1706.02677 , 2017.\\n5\\n[14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\\ntor quantized diffusion model for text-to-image synthesis. In\\nCVPR , pages 10696–10706, 2022. 2\\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In CVPR ,\\n2016. 2[16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 12\\n[17] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\\nChristopher Hesse, Jacob Jackson, Heewoo Jun, Tom B\\nBrown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws\\nfor autoregressive generative modeling. arXiv preprint\\narXiv:2010.14701 , 2020. 2\\n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\\ntwo time-scale update rule converge to a local nash equilib-\\nrium. 2017. 6\\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\\nsion probabilistic models. In NeurIPS , 2020. 2, 3\\n[20] Jonathan Ho, Chitwan Saharia, William Chan, David J\\nFleet, Mohammad Norouzi, and Tim Salimans. Cas-\\ncaded diffusion models for high ﬁdelity image generation.\\narXiv:2106.15282 , 2021. 3, 9\\n[21] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion\\nguidance. In NeurIPS 2021 Workshop on Deep Generative\\nModels and Downstream Applications , 2021. 3, 4\\n[22] Aapo Hyv ¨arinen and Peter Dayan. Estimation of non-\\nnormalized statistical models by score matching. Journal\\nof Machine Learning Research , 6(4), 2005. 3', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 9}),\n",
       " Document(page_content='[22] Aapo Hyv ¨arinen and Peter Dayan. Estimation of non-\\nnormalized statistical models by score matching. Journal\\nof Machine Learning Research , 6(4), 2005. 3\\n[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\\nEfros. Image-to-image translation with conditional adver-\\nsarial networks. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 1125–1134,\\n2017. 2\\n[24] Allan Jabri, David Fleet, and Ting Chen. Scalable adap-\\ntive computation for iterative generation. arXiv preprint\\narXiv:2212.11972 , 2022. 3\\n[25] Michael Janner, Qiyang Li, and Sergey Levine. Ofﬂine rein-\\nforcement learning as one big sequence modeling problem.\\nInNeurIPS , 2021. 2\\n[26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\\nneural language models. arXiv:2001.08361 , 2020. 2, 13\\n[27] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\\nElucidating the design space of diffusion-based generative\\nmodels. In Proc. NeurIPS , 2022. 3\\n[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based\\ngenerator architecture for generative adversarial networks. In\\nCVPR , 2019. 5\\n[29] Diederik Kingma and Jimmy Ba. Adam: A method for\\nstochastic optimization. In ICLR , 2015. 5\\n[30] Diederik P Kingma and Max Welling. Auto-encoding varia-\\ntional bayes. arXiv preprint arXiv:1312.6114 , 2013. 3, 6\\n[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\\nImagenet classiﬁcation with deep convolutional neural net-\\nworks. In NeurIPS , 2012. 5\\n[32] Tuomas Kynk ¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko\\nLehtinen, and Timo Aila. Improved precision and recall met-\\nric for assessing generative models. In NeurIPS , 2019. 6\\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv:1711.05101 , 2017. 5\\n10', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 9}),\n",
       " Document(page_content='[34] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W\\nBattaglia. Generating images with sparse representations.\\narXiv preprint arXiv:2103.03841 , 2021. 6\\n[35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\\nand Mark Chen. Glide: Towards photorealistic image\\ngeneration and editing with text-guided diffusion models.\\narXiv:2112.10741 , 2021. 3, 4\\n[36] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\\ndenoising diffusion probabilistic models. In ICML , 2021. 3\\n[37] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On\\naliased resizing and surprising subtleties in gan evaluation.\\nInCVPR , 2022. 6\\n[38] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\\nage transformer. In International conference on machine\\nlearning , pages 4055–4064. PMLR, 2018. 2\\n[39] William Peebles, Ilija Radosavovic, Tim Brooks, Alexei\\nEfros, and Jitendra Malik. Learning to learn with genera-\\ntive models of neural network checkpoints. arXiv preprint\\narXiv:2209.12892 , 2022. 2\\n[40] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-\\nmoulin, and Aaron Courville. Film: Visual reasoning with a\\ngeneral conditioning layer. In AAAI , 2018. 2, 5\\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\\ning transferable visual models from natural language super-\\nvision. In ICML , 2021. 2\\n[42] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\\nSutskever. Improving language understanding by generative\\npre-training. 2018. 1\\n[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\\nAmodei, Ilya Sutskever, et al. Language models are unsu-\\npervised multitask learners. 2019. 1\\n[44] Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo,\\nand Piotr Doll ´ar. On network design spaces for visual recog-\\nnition. In ICCV , 2019. 3\\n[45] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\\nKaiming He, and Piotr Doll ´ar. Designing network design\\nspaces. In CVPR , 2020. 3\\n[46] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\\nand Mark Chen. Hierarchical text-conditional image gener-\\nation with clip latents. arXiv:2204.06125 , 2022. 1, 2, 3, 4\\n[47] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\\nChelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.\\nZero-shot text-to-image generation. In ICML , 2021. 1, 2\\n[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image syn-\\nthesis with latent diffusion models. In CVPR , 2022. 2, 3, 4,\\n6, 9\\n[49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\\nnet: Convolutional networks for biomedical image segmen-\\ntation. In International Conference on Medical image com-\\nputing and computer-assisted intervention , pages 234–241.\\nSpringer, 2015. 2, 3[50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\\nFleet, and Mohammad Norouzi. Photorealistic text-to-\\nimage diffusion models with deep language understanding.\\narXiv:2205.11487 , 2022. 3\\n[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\\nCheung, Alec Radford, Xi Chen, and Xi Chen. Improved\\ntechniques for training GANs. In NeurIPS , 2016. 6\\n[52] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P\\nKingma. PixelCNN++: Improving the pixelcnn with dis-\\ncretized logistic mixture likelihood and other modiﬁcations.\\narXiv preprint arXiv:1701.05517 , 2017. 2\\n[53] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-\\nxl: Scaling stylegan to large diverse datasets. In SIGGRAPH ,\\n2022. 9\\n[54] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\\nand Surya Ganguli. Deep unsupervised learning using\\nnonequilibrium thermodynamics. In ICML , 2015. 3\\n[55] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 10}),\n",
       " Document(page_content='and Surya Ganguli. Deep unsupervised learning using\\nnonequilibrium thermodynamics. In ICML , 2015. 3\\n[55] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\\ning diffusion implicit models. arXiv:2010.02502 , 2020. 3\\n[56] Yang Song and Stefano Ermon. Generative modeling by es-\\ntimating gradients of the data distribution. In NeurIPS , 2019.\\n3\\n[57] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross\\nWightman, Jakob Uszkoreit, and Lucas Beyer. How to train\\nyour ViT? data, augmentation, and regularization in vision\\ntransformers. TMLR , 2022. 6\\n[58] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt,\\nOriol Vinyals, Alex Graves, et al. Conditional image genera-\\ntion with pixelcnn decoders. Advances in neural information\\nprocessing systems , 29, 2016. 2\\n[59] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\\nrepresentation learning. Advances in neural information pro-\\ncessing systems , 30, 2017. 2\\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 1,\\n2, 5\\n[61] Tete Xiao, Piotr Dollar, Mannat Singh, Eric Mintun, Trevor\\nDarrell, and Ross Girshick. Early convolutions help trans-\\nformers see better. In NeurIPS , 2021. 6\\n[62] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,\\nGunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku,\\nYinfei Yang, Burcu Karagol Ayan, et al. Scaling autore-\\ngressive models for content-rich text-to-image generation.\\narXiv:2206.10789 , 2022. 2\\n[63] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-\\ncas Beyer. Scaling vision transformers. In CVPR , 2022. 2,\\n5\\n11', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 10}),\n",
       " Document(page_content='Figure 11. Additional selected samples from our 512 ×512 and 256×256 resolution DiT-XL/2 models. We use a classiﬁer-free guidance\\nscale of 6.0 for the 512×512model and 4.0 for the 256×256model. Both models use the ft-EMA V AE decoder.\\nA. Additional Implementation Details\\nWe include detailed information about all of our DiT\\nmodels in Table 4, including both 256×256and512×512\\nmodels. In Figure 13, we report DiT training loss curves.\\nFinally, we also include Gﬂop counts for DDPM U-Net\\nmodels from ADM and LDM in Table 6.\\nDiT model details. To embed input timesteps, we use\\na 256-dimensional frequency embedding [9] followed by\\na two-layer MLP with dimensionality equal to the trans-\\nformer’s hidden size and SiLU activations. Each adaLN\\nlayer feeds the sum of the timestep and class embeddings\\ninto a SiLU nonlinearity and a linear layer with output neu-\\nrons equal to either 4×(adaLN) or 6×(adaLN-Zero) the\\ntransformer’s hidden size. We use GELU nonlinearities (ap-\\nproximated with tanh) in the core transformer [16].\\nClassiﬁer-free guidance on a subset of channels. In our\\nexperiments using classiﬁer-free guidance, we applied guid-\\nance only to the ﬁrst three channels of the latents instead of\\nall four channels. Upon investigating, we found that three-\\nchannel guidance and four-channel guidance give similarresults (in terms of FID) when simply adjusting the scale\\nfactor. Speciﬁcally, three-channel guidance with a scale\\nof(1 +x)appears reasonably well-approximated by four-\\nchannel guidance with a scale of (1 +3\\n4x)(e.g., three-\\nchannel guidance with a scale of 1.5gives an FID-50K of\\n2.27, and four-channel guidance with a scale of 1.375gives\\nan FID-50K of 2.20). It is somewhat interesting that ap-\\nplying guidance to a subset of elements can still yield good\\nperformance, and we leave it to future work to explore this\\nphenomenon further.\\nB. Model Samples\\nWe show samples from our two DiT-XL/2 models at\\n512×512and256×256resolution trained for 3M and 7M\\nsteps, respectively. Figures 1 and 11 show selected samples\\nfrom both models. Figures 14 through 33 show uncurated\\nsamples from the two models across a range of classiﬁer-\\nfree guidance scales and input class labels (generated with\\n250 DDPM sampling steps and the ft-EMA V AE decoder).\\nAs with prior work using guidance, we observe that larger\\nscales increase visual ﬁdelity and decrease sample diversity.\\n12', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 11}),\n",
       " Document(page_content='Model Image Resolution Flops (G) Params (M) Training Steps (K) Batch Size Learning Rate DiT Block FID-50K (no guidance)\\nDiT-S/8 256×256 0.36 33 400 256 1×10−4adaLN-Zero 153.60\\nDiT-S/4 256×256 1.41 33 400 256 1×10−4adaLN-Zero 100.41\\nDiT-S/2 256×256 6.06 33 400 256 1×10−4adaLN-Zero 68.40\\nDiT-B/8 256×256 1.42 131 400 256 1×10−4adaLN-Zero 122.74\\nDiT-B/4 256×256 5.56 130 400 256 1×10−4adaLN-Zero 68.38\\nDiT-B/2 256×256 23.01 130 400 256 1×10−4adaLN-Zero 43.47\\nDiT-L/8 256×256 5.01 459 400 256 1×10−4adaLN-Zero 118.87\\nDiT-L/4 256×256 19.70 458 400 256 1×10−4adaLN-Zero 45.64\\nDiT-L/2 256×256 80.71 458 400 256 1×10−4adaLN-Zero 23.33\\nDiT-XL/8 256×256 7.39 676 400 256 1×10−4adaLN-Zero 106.41\\nDiT-XL/4 256×256 29.05 675 400 256 1×10−4adaLN-Zero 43.01\\nDiT-XL/2 256×256 118.64 675 400 256 1×10−4adaLN-Zero 19.47\\nDiT-XL/2 256×256 119.37 449 400 256 1×10−4in-context 35.24\\nDiT-XL/2 256×256 137.62 598 400 256 1×10−4cross-attention 26.14\\nDiT-XL/2 256×256 118.56 600 400 256 1×10−4adaLN 25.21\\nDiT-XL/2 256×256 118.64 675 2352 256 1×10−4adaLN-Zero 10.67\\nDiT-XL/2 256×256 118.64 675 7000 256 1×10−4adaLN-Zero 9.62\\nDiT-XL/2 512×512 524.60 675 1301 256 1×10−4adaLN-Zero 13.78\\nDiT-XL/2 512×512 524.60 675 3000 256 1×10−4adaLN-Zero 11.93\\nTable 4. Details of all DiT models. We report detailed information about every DiT model in our paper. Note that FID-50K here is\\ncomputed without classiﬁer-free guidance. Parameter and ﬂop counts exclude the V AE model which contains 84M parameters across the\\nencoder and decoder. For both the 256×256and512×512DiT-XL/2 models, we never observed FID saturate and continued training\\nthem as long as possible. Numbers reported in this table use the ft-MSE V AE decoder.\\nC. Additional Scaling Results\\nImpact of scaling on metrics beyond FID. In Figure 12,\\nwe show the effects of DiT scale on a suite of evaluation\\nmetrics—FID, sFID, Inception Score, Precision and Recall.\\nWe ﬁnd that our FID-driven analysis in the main paper gen-\\neralizes to the other metrics—across every metric, scaled-up\\nDiT models are more compute-efﬁcient and model Gﬂops\\nare highly-correlated with performance. In particular, In-\\nception Score and Precision beneﬁt heavily from increased\\nmodel scale.\\nImpact of scaling on training loss. We also examine the\\nimpact of scale on training loss in Figure 13. Increasing\\nDiT model Gﬂops (via transformer size or number of input\\ntokens) causes the training loss to decrease more rapidly and\\nsaturate at a lower value. This phenomenon is consistent\\nwith trends observed with language models, where scaled-\\nup transformers demonstrate both improved loss curves as\\nwell as improved performance on downstream evaluation\\nsuites [26].\\nD. V AE Decoder Ablations\\nWe used off-the-shelf, pre-trained V AEs across our ex-\\nperiments. The V AE models (ft-MSE and ft-EMA) are ﬁne-\\ntuned versions of the original LDM “f8” model (only the\\ndecoder weights are ﬁne-tuned). We monitored metrics for\\nour scaling analysis in Section 5 using the ft-MSE decoder,\\nand we used the ft-EMA decoder for our ﬁnal metrics re-\\nported in Tables 2 and 3. In this section, we ablate threeClass-Conditional ImageNet 256 ×256, DiT-XL/2-G (cfg=1.5)\\nDecoder FID↓sFID↓ IS↑ Precision↑ Recall↑\\noriginal 2.46 5.18 271.56 0.82 0.57\\nft-MSE 2.30 4.73 276.09 0.83 0.57\\nft-EMA 2.27 4.60 278.24 0.83 0.57\\nTable 5. Decoder ablation. We tested different pre-trained V AE\\ndecoder weights available at https://huggingface.co/\\nstabilityai/sd-vae-ft-mse . Different pre-trained de-\\ncoder weights yield comparable results on ImageNet 256×256.\\nDiffusion U-Net Model Complexities\\nModel Image Resolution Base Flops (G) Upsampler Flops (G) Total Flops (G)\\nADM 128×128 307 - 307\\nADM 256×256 1120 - 1120\\nADM 512×512 1983 - 1983\\nADM-U 256×256 110 632 742\\nADM-U 512×512 307 2506 2813\\nLDM-4 256×256 104 - 104\\nLDM-8 256×256 57 - 57\\nTable 6. Gﬂop counts for baseline diffusion models that use U-\\nNet backbones. Note that we only count Flops for DDPM com-\\nponents.\\ndifferent choices of the V AE decoder; the original one used', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 12}),\n",
       " Document(page_content='Table 6. Gﬂop counts for baseline diffusion models that use U-\\nNet backbones. Note that we only count Flops for DDPM com-\\nponents.\\ndifferent choices of the V AE decoder; the original one used\\nby LDM and the two ﬁne-tuned decoders used by Stable\\nDiffusion. Because the encoders are identical across mod-\\nels, the decoders can be swapped-in without retraining the\\ndiffusion model. Table 5 shows results; XL/2 continues to\\noutperform all prior diffusion models when using the LDM\\ndecoder.\\n13', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 12}),\n",
       " Document(page_content='Figure 12. DiT scaling behavior on several generative modeling metrics. Left: We plot model performance as a function of total training\\ncompute for FID, sFID, Inception Score, Precision and Recall. Right: We plot model performance at 400K training steps for all 12 DiT\\nvariants against transformer Gﬂops, ﬁnding strong correlations across metrics. All values were computed using the ft-MSE V AE decoder.\\n14', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 13}),\n",
       " Document(page_content='0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M\\nTraining Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20S/8 S/4 S/2\\n0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M\\nTraining Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20B/8 B/4 B/2\\n0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M\\nTraining Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20L/8 L/4 L/2\\n0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M\\nTraining Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20XL/8 XL/4 XL/2\\n0 0.25M 0.50M 0.75M 1.00M 1.25M 1.50M 1.75M 2.00M 2.25M 2.50M 2.75M 3.00M 3.25M 3.50M 3.75M 4.00M 4.25M 4.50M 4.75M 5.00M 5.25M 5.50M 5.75M 6.00M 6.25M 6.50M 6.75M 7.00M\\nTraining Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20XL/2 (256x256) XL/2 (512x512)Figure 13. Training loss curves for all DiT models. We plot the loss over training for all DiT models (the sum of the noise prediction\\nmean-squared error and DKL). We also highlight early training behavior. Note that scaled-up DiT models exhibit lower training losses.\\n15', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 14}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 4.0\\nFigure 14. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “arctic wolf” (270)\\nFigure 15. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “volcano” (980)\\n16', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 15}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 4.0\\nFigure 16. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “husky” (250)\\nFigure 17. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “sulphur-crested cockatoo” (89)\\n17', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 16}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 4.0\\nFigure 18. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “cliff drop-off” (972)\\nFigure 19. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “balloon” (417)\\n18', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 17}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 4.0\\nFigure 20. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “lion” (291)\\nFigure 21. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “otter” (360)\\n19', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 18}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 2.0\\nFigure 22. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 2.0\\nClass label = “red panda” (387)\\nFigure 23. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 2.0\\nClass label = “panda” (388)\\n20', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 19}),\n",
       " Document(page_content='DiT-XL/2 512×512samples, classiﬁer-free guidance scale = 1.5\\nFigure 24. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 1.5\\nClass label = “coral reef” (973)\\nFigure 25. Uncurated 512×512DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 1.5\\nClass label = “macaw” (88)\\n21', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 20}),\n",
       " Document(page_content='DiT-XL/2 256×256samples, classiﬁer-free guidance scale = 4.0\\nFigure 26. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “macaw” (88)\\nFigure 27. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “dog sled” (537)\\n22', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 21}),\n",
       " Document(page_content='DiT-XL/2 256×256samples, classiﬁer-free guidance scale = 4.0\\nFigure 28. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “arctic fox” (279)\\nFigure 29. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 4.0\\nClass label = “loggerhead sea turtle” (33)\\n23', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 22}),\n",
       " Document(page_content='DiT-XL/2 256×256samples, classiﬁer-free guidance scale = 2.0\\nFigure 30. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 2.0\\nClass label = “golden retriever” (207)\\nFigure 31. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 2.0\\nClass label = “lake shore” (975)\\n24', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 23}),\n",
       " Document(page_content='DiT-XL/2 256×256samples, classiﬁer-free guidance scale = 1.5\\nFigure 32. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 1.5\\nClass label = “space shuttle” (812)\\nFigure 33. Uncurated 256×256DiT-XL/2 samples.\\nClassiﬁer-free guidance scale = 1.5\\nClass label = “ice cream” (928)\\n25', metadata={'source': 'https://arxiv.org/pdf/2212.09748v2.pdf', 'page': 24})]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SME_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
